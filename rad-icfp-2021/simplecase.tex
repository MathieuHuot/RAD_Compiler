\section{Simple case}

\subsection{Language}

We first consider a very simple first-order language. The types and terms are as follows:
% \input{simpletypesandterms}

The type system is given in Figure \ref{fig:types1}.

% \begin{figure}[b]
%     \framebox{\scalebox{0.82}{\begin{minipage}{1.2\linewidth}\noindent\input{simpletypesystemsource}\end{minipage}}}
%     \caption{Typing rules for the simple language.\label{fig:types1}}
% \end{figure}

Reverse-mode introduces pairs and continuations, so we need to enrich the target language of differentiation with these constructs.
The target language is given in Figure \ref{fig:types2}.

% \begin{figure}[b]
%     \framebox{\scalebox{0.82}{\begin{minipage}{1.2\linewidth}\noindent\input{simpletypesystemtarget}\end{minipage}}}
%     \caption{Typing rules for the simple language.\label{fig:types2}}
% \end{figure}

\subsection{Naive reverse-mode}

We define the macro $\Dsynrevsymbol[][]{}$ by induction on the syntax of the source language as follows:

% \begin{figure}[b]
%     \framebox{\scalebox{0.82}{\begin{minipage}{1.2\linewidth}\noindent\input{simplenaivediffmacro}\end{minipage}}}
%     \caption{Typing rules for the simple language.\label{fig:simplediff}}
% \end{figure}

We show that our macro $\Dsynrev[]{t}$ is well-typed on the restricted language above. Given $x:\RR^n\vdash t:\RR$, we define the type $\TyD{t}\defeq\TyDinter{x:\RR^n}{t}$ where $\TyDinter{L}{t}$ is defined by induction on the structure of $t$ as follows:
\begin{itemize}
	\item $\TyDinter{x:\RR^n,y_1:\RR,\ldots,y_k:\RR}{\letin{y}{s}{t}}\defeq \TyDinter{x:\RR^n,y_1:\RR,\ldots,y_k:\RR,y:\RR}{t}$
	\item $\TyDinter{x:\RR^n,y_1:\RR,\ldots,y_k:\RR}{\cnst}\defeq \RR\times (\RR^{n+k+1}\to \RR^{n})$
	\item $\TyDinter{x:\RR^n,y_1:\RR,\ldots,y_k:\RR}{x_i}\defeq\RR\times (\RR^{n+k+1}\to \RR^{n})$
	\item $\TyDinter{x:\RR^n,y_1:\RR,\ldots,y_k:\RR}{y_i}\defeq\RR\times (\RR^{n+k+1}\to \RR^{n})$
	\item $\TyDinter{x:\RR^n,y_1:\RR,\ldots,y_k:\RR}{\Op_1(y_i)}\defeq \RR\times (\RR^{n+k+1}\to \RR^{n})$ and the same with $\Op_1(x_i)$
	\item $\TyDinter{x:\RR^n,y_1:\RR,\ldots,y_k:\RR}{\Op_2(y_i,y_j)}\defeq \RR\times (\RR^{n+k+1}\to \RR^{n})$
\end{itemize}

In other words, most items can be summarised as $\TyDinter{x:\RR^n,y_1:\RR,\ldots,y_k:\RR}{W}\defeq \RR\times (\RR^{n+k+1}\to \RR^{n+k})$ where $W$ is an almost immediate expression.

\begin{lemma}
	The macro $\Dsynrev[]{t}$ is well-typed. More precisely, if $x:\RR^n\vdash t:\RR$, then $(x,x'):\RR^n\times (\RR^n\to\RR^n)\vdash \Dsynrev[x]{t}:\TyD{t}$.
\end{lemma}


\subsection{Semantics} 
\label{sub:Semantics}

We give a standard denotational semantics $\sem{-}$ to our language in the category of Euclidean spaces and smooth maps.

\subsection{Optimisations}

With the current reverse mode transformation, the produced term is much bigger than the original term, by about a quadratic factor. 
That is, if the original term has roughly $n$ reductions, the produced term will have $O(n^2)$ reductions. 
For efficient reverse mode implementations, this should be $O(n)$ and the constant (usually at most 4) matters as well.
By applying some well chosen partial evaluation, we can recover an efficient term.

The simple insight in each function in the continuation, at most $2$ out of the $m$ current variables are not simply returned, so the continuation as each step looks like
$\lambda x1,\ldots,xm.X(x1,\ldots,xm)$ except for at most 2 positions.
Unrolling X one step further, we see that when beta-reducing, most substitutions will be of the form $[x/y]$ where $x,y$ are both variables, 
and this gives a big opportunity for partial evaluation.

Slighly more formally, the basic partial evaluation consist of:
\begin{enumerate}
	\item Remplacing the variables binding the $\inllambda$s by the actual function they represent (CBN evaluation)
	\item Replace all $(\inllambda x.e_1)e2$s by $\letin{x}{e_2}{e_1}$
	\item Use the following reduction $\letin{\var}{\var[2]}{\trm}\to\trm{}[\var[2]/\var{}]$ when $\var,\var[2]$ are variables
\end{enumerate}

Note that we didn't use anything about the tuple $(0,0,0,0,1)$ used to compute the reverse-derivative of the whole term. 
At this point we can optimise further with partial evaluation. We could also use another tuple to compute the partial derivative of a subterm, 
i.e. with the tuple $(0,0,0,1,0)$, we would compute the derivative of the subterm ending where $w_1$ is introduced (and returned). 
The standard imperative form for reverse-mode is not as flexible and one would need to write a separate algorithm 
or use a few tricks to compute the derivative of a subterm. 

Each inlined lambda $\inllambda$ is used exactly once and we are in a pure typed setting, so termination is easy to prove. These transformations are semantics preserving.

\begin{proposition}
	Steps 1)-3) terminate and are semantics preserving.
\end{proposition}

\subsection{Correctness}

Now we can phrase that our sequence of transformations is correct.

\begin{proposition}
    TODO
\end{proposition}

\subsection{Cheap gradient principle}

TODO: recall what it is and that it's what we want. introduce simple cost model.

\begin{proposition}
    If $\Gamma\vdash e:\RR$ then $C(rad(e))\leq k C(e)$ for a constant $k$ independant of $e$, and $k\leq4$.
\end{proposition}

\subsubsection*{Key insight:}

As is well known (see e.g. \cite{pearlmutter2008reverse}), duplication of a variable $\Delta$ is turned into $+$ by reverse mode.
TODO: explain this is the hard part to make pure and efficient, and shows up in the case of non-unary operators, and things like let binding.
In the inefficient representation, every operator was seen as a unary operator using all the available variables and returning them in addition to the extra return value. 
In essence, we are putting boxes locally around places where the compute-flow might be non-linear and turning everything into something linear.
This linearisation of the compute-flow is the key to efficient purely functional reverse-mode. We extend this idea formally in the next sections by introducing a unary form, 
on which it is easier to do naive reverse-mode that can be optimized. 