\section{Complexity}
\label{sec:complexity}

\subsection{Cost model}
\label{sub:costModel}

We follow a simple model similar to the one in \cite{griewank2008evaluating}.
We assume the cost is divided into 4 elementary measures, being the number of MOVES, ADDS, MULTS, and NLOPS.
MOVES assumes a flat memory and represents moving a fixed size information  (e.g. 64 bits). 
ADDS represents the number of additions, 
MULTS the number of multiplications, 
and NLOPS the number of elementary non linear operations like !cos!, !exp!.

The gives a complexity function $\cost$ valued in $\RR^4$. 
For instance, we have 

\begin{itemize}
    \item $\cost(*)=(3,0,1,0)$
    \item $\cost(+)=(3,1,0,0)$
    \item $\cost(c)=(1,0,0,0)$
    \item $\cost($!sin!$)=(2,0,0,1)$
\end{itemize}

TODO: extend cost array
TODO: extend cost compositional way

\subsection{Cheap gradient principle}

The cheap gradient principle (see e.g. \cite{griewank2008evaluating}) for reverse-mode
asserts that evaluating the gradient of a function $e:\RR^n\to\RR$ 
should be the same order of cost as evaluating $e$. 
More precisely, there should be a constant $K$ such that for each program !$\Gamma \vdash$ e: $\reals$! in the context $\Gamma=\{x_1:\reals,\ldots,x_n:\reals\}$,
 !$\cost$($\grad$e) $\leq$ $K$*$\cost$(e)!.

As $\directD{\rho}{\Gamma}{Y}$ is defined by induction on programs, it suffices to show locally 
that $\directD{\rho}{\Gamma}{Y}$ verifies the cheap gradient principle. 
However, this is not quite the case with $\directD{\rho}{\Gamma}{Y}$ 
before the optimization (inlining and forward substitution) step from Section~\ref{sub:Partial evaluation and optimization}.

\begin{proposition}
    TODO: cost $\directD{\rho}{\Gamma}{Y}$. 
\end{proposition}

As we can see, the only problem is that too many variables are being moved, 
due to the continuation carrying and copying $n$ variables at each step.
This is exactly the overhead that our inlining and forward substitution removes.

\begin{proposition}
    TODO: complexity gain using our optim.
\end{proposition}

Combining the two previous points, we conclude that our reverse mode transformation satisfies the cheap gradient principle.

\begin{theorem}
    TODO: The reverse mode transformation satisfies the cheap gradient principle.
\end{theorem}

\subsection{Optimizations} % (fold)
\label{sub:Optimizations}

In Figure~\ref{fig:optim} we present a of optimizations. 
As our language is purely functional and effect free, 
we can use these optimizations aggressively. 
A lot of simplifications come from the ring structure of the reals, lifted to tuples and arrays.
After these optimizations, the output program is essentially a sequence of let bindings 
and resembles SSA-code.

\input{figures/optims}