\section{Complexity}
\label{sec:complexity}

\subsection{Cost model}
\label{sub:costModel}

We follow a simple model similar to the one in \cite{griewank2008evaluating}.
We assume the cost is divided into $4$ elementary measures, being the number of MOVES, ADDS, MULTS, and NLOPS.
MOVES assumes a flat memory and represents moving a fixed size information  (e.g. 64 bits). 
ADDS represents the number of additions, 
MULTS the number of multiplications, 
and NLOPS the number of elementary non-linear operations like !cos! or !exp!.

The gives a complexity function $\cost$ valued in $\RR^4$. 
For instance, we have 

\begin{tabular}{ll}
    $\cost(*)=(3,0,1,0)$ & $\cost(+)=(3,1,0,0)$\\
    $\cost(c)=(1,0,0,0)$ & $\cost($!sin!$)=(2,0,0,1)$
\end{tabular}

For simplicity, we will not address any parallelism in our cost model. 
So the cost of !map (x.e)! on an array of size $n$ will be $n*\cost$(!e!).
The cost function extends compositionally to the whole language in the obvious way.
For instance we have !$\cost$(let x=e$_1$ in e$_2$)!= !$\cost$(e$_1$)+$\cost$(e$_2$)!.
TODO: for let binding and same in map, do I need to add some MOVES?

\subsection{Cheap gradient principle}

The cheap gradient principle (see e.g. \cite{griewank2008evaluating}) for reverse-mode
asserts that evaluating the gradient of a function $e:\RR^n\to\RR$ 
should be the same order of cost as evaluating $e$. 
More precisely, there should be a constant $K$ such that for each program !$\Gamma \vdash$ e: $\reals$! in the context $\Gamma=\{x_1:\reals,\ldots,x_n:\reals\}$,
 !$\cost$($\grad$e) $\leq$ $K$*$\cost$(e)!.

As $\directD{\rho}{\Gamma}{Y}$ is defined by induction on programs, it suffices to show locally 
that $\directD{\rho}{\Gamma}{Y}$ verifies the cheap gradient principle. 

There is a first restriction that prevents our transformation from satisfying the cheap gradient principle.
If we try to show that the cheap gradient principle holds by induction on the term, it fails on the !map2!.
When differentiating !map2!, there are two series of calls to (sub)gradients of !e$_1$!. 
The induction hypothesis is then too weak to conclude. 
The problem comes from the fact that !e$_1$! could itself use !map2!. 
So the constant $K$ can be independant of $n$ and of 
the size of the term if we allow it to be dependent on the level of nestings of !map2!.
A similar phenomenon happens with !reduce!.

Another problem is that the continuation part of $\directD{\rho}{\Gamma}{Y}$ adds $O(n)$ MOVES at each step.
This overhead is precisely what that our inlining and forward substitution removes, 
as was exemplified in Section~\ref{sub:Partial evaluation and optimization}.

Combining the two previous points, we conclude that our reverse mode transformation satisfies a version of the cheap gradient principle.

\begin{theorem}
    Given a term !$\Gamma \vdash$ e: $\reals$! in Source with at most $p$ nested !map2! or !reduce!. 
    Denote by $G$ the term $\grad_\Gamma$!e! after inlining and forward substitution.
    Then !$\cost$(G) $\leq$ $4*2^{p}*\cost$(e)!.
\end{theorem}

\begin{proof}[notes]
    The proof is by routine induction on !e!. 
    First, one computes the cost of $\directD{\rho}{\Gamma}{Y}$(!e!).
    It has $N$ too many MOVES which are removed by inlining and forward substitution using the invariant that at each step, 
    at most 2 !x!$_i$ in the continuation are not variables.
\end{proof}

\subsection{Optimizations} % (fold)
\label{sub:Optimizations}

In Figure~\ref{fig:optim} we present a of optimizations. 
As our language is purely functional and effect free, 
we can use these optimizations aggressively. 
A lot of simplifications come from the ring structure of the reals, lifted to tuples and arrays.
After these optimizations, the output program is essentially a sequence of let bindings 
and resembles SSA-code.

\input{figures/optims}