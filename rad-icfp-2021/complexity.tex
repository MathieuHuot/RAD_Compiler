\section{Complexity analysis}
\label{sec:complexity}

In this section, we introduce a simple cost model for our language and show 
that, after partial evaluation, our reverse-mode transformation satisfies a version of the cheap gradient principle (Theorem \ref{thm:complexity}).

\subsection{Partial evaluation and optimization} % (fold)
\label{sub:Partial evaluation and optimization}

As can be seen from the examples, $\directD{\rho}{\Gamma}{Y}$ introduces a lot of functions of several arguments.
Following the insight from Section~\ref{subsec:insights}, we do not want to keep all these costly lambda abstractions.
The transformation is designed in such a way that all the lambda abstractions are given arguments,  
and we can use partial evaluation to beta-reduce all these lambda abstractions. 
By inspecting each rule in Figure~\ref{fig:direct_diff_macro}, this allows us to prove by induction on the judgment of !e!:

\begin{lemma}
    Let $\Gamma \vdash$!e: A! be a term in Source. 
    Every variable !Y! in $\directD{\rho}{\Gamma}{Y}$(!e!) which is not of ground type has exactly one occurrence in the term.
\end{lemma}

From the previous lemma, we see that there is a linear usage of each continuation variable $Y$. 
This is one key property ensured by $\UNFSymbol$.
We also note that, apart from the !map2! case, this continuation variable is always applied to almost the identity. 
More precisely, we have applications of the form !fun (x1,$\ldots$,xn) -> Y(e1,$\ldots$,en)! 
where the !ei! are !xi! except for at most $k$ (independent of $n$) terms.
In fact we have $k=2$, except for the !map2! and !reduce!.

We can first use inlining, the first optimization rule given in Figure~\ref{fig:optim}.
Using the invariant above, most of the !e!$_{i}$ below are variables. 
Without loss of generality, assume the only terms !e!$_{i}$ which are not variables are !e!$_{n-1}$ and !e!$_{n}$.
We can then use forward substitution, the third optimization rule in Figure~\ref{fig:optim}, on the other !e!$_{i}$.
In brief:

\begin{tabular}{l}
!fun (x$_{1}$,$\ldots$,x$_n$) -> (fun (y$_{1}$,$\ldots$,y$_n$) -> (f$_{1}$,$\ldots$,f$_n$))(e$_{1}$,$\ldots$,e$_n$)! 
$\transto$ \\
!fun (x$_{1}$,$\ldots$,x$_n$) -> let y$_{1}$,$\ldots$,y$_n$ = e$_1$,$\ldots$,e$_n$ in (f$_{1}$,$\ldots$,f$_n$)! $\transto$ \\
!fun (x$_{1}$,$\ldots$,x$_n$) -> let y$_{n-1}$,y$_n$ = e$_{n-1},$e$_1$ in!\\
\hspace{3cm}!(f$_{1}$[x$_{1}$/y$_{1}$],$\ldots$,f$_{n-2}[$x$_{n-2}$/y$_{n-2}]$,f$_{n-1}$,f$_{n}$)!
\end{tabular}

This rewriting does not change the evaluation cost of the !f!$_{i}$ for $1\leq i \leq n-2$.
The new evaluation cost is reduced to the sum of the cost of evaluating 
the !f!$_{i}$ in addition to the cost of evaluating !e!$_{n-1}$ and !e!$_{n}$, gaining O($n$) movement of variables.

\begin{example}
    After forward-substitution and inlining the inner !Y!$_{i}$, the gradient of the terms from the introduction reduces to

    \begin{tabular}{c l}
        & $\grad_\Gamma$(!let w$_{1}$ = x$_{1}$ * x$_{2}$ in let w$_{2}$ = w$_{1}$ * x$_{1}$ in w$_{2}$!) \\
        =& !let w$_1$,Y$_1$= <x$_1$ * x$_{2}$, fun (y$_{1}$,y$_{2}$,y$_{3}$,z) -> Y(y$_{1}$+x$_2$*z,y$_{2}$+x$_1$*z,y$_{3}$) > in! \\
        & !let w$_{2}$,Y$_{2}$= <w$_{1}$ * x$_{1}$, fun (y$_{1}$,y$_{2}$,y$_{3}$,y$_{4}$,z) -> Y$_{1}$(y$_{1}$+w$_1$*z,y$_{2}$,y$_{3}$,y$_{4}$+x$_{1}$*z) > in! \\
        & !let y,Y$_{3}$= <w$_{2}$, fun (y$_{1}$,y$_{2}$,y$_{3}$,y$_{4}$,z) -> Y$_{2}$(y$_{1}$,y$_{2}$,y$_{3}$,y$_{4}$+z)> in! \\
        & !<y, fun (y$_{1}$,y$_{2}$,y$_{3}$,z) -> Y$_{3}$(y$_{1}$,y$_{2}$,y$_{3}$,0,z) >!
    \end{tabular}

After another simplification step, we obtain 

\begin{tabular}{c l}
    & $\grad_\Gamma$(!let w$_{1}$ = x$_{1}$ * x$_{2}$ in let w$_{2}$ = w$_{1}$ * x$_{1}$ in w$_{2}$!) \\
    =& !let w$_1$= x$_1$ * x$_{2}$ in let w$_{2}$= w$_{1}$ * x$_{1}$ in! \\
        & !<w$_{2}$, fun (y$_{1}$,y$_{2}$,y$_{3}$,z) -> let y'$_{1}$=y$_{1}$+w$_1$*z in !\\
        & \quad\quad\quad!let z'=y$_4$+x$_1$*z in Y(y'+x$_2$*z',y$_{2}$+x$_1$*z',y$_{3}$) >!
\end{tabular}

Similarly, for the gradient of !prod(A)! we obtain

\begin{tabular}{c l}
        & $\grad_\Gamma$(!prod(A)!) \\
        & !let A$_{0}$= shift1R (scanl * 1 A) in!\\
        & !let A$_{1}$= shift1L (map2 (a,b.b) A$_{0}$ A) in!\\
        & !let A$_{2}$= map2 (a,b.a) A$_{0}$ A in!\\
        & !let A$_{3}$= scanr * 1 A$_{1}$ in!\\
        & !<prod(A), fun (X,z) -> Y(X+map2 (a,b. a*z*b) A$_{2}$ A$_{3}$)>! 
    \end{tabular}
\end{example}

We call this optimization step partial evaluation in the rest of the complexity section. 
We have the following result.

\begin{lemma}
    \label{lem:noHO}
    Let $\Gamma \vdash$ !e:A! be a term in Source. 
    After the partial evaluation step, $\directD{\rho}{\Gamma}{Y}$(!e!) does not have lambda abstractions.
    Its only variable (bound and free) which does not have a ground type is !Y!.
\end{lemma}

\begin{proof}
    By induction on the judgment of !e!. 
    We inspect each rule in Figure~\ref{fig:direct_diff_macro} 
    and note that the partial evaluation step precisely allows us to conclude the inductive step.
\end{proof}

\subsection{Cost model}
\label{sub:costModel}

We follow a simple model similar to the one in \cite{griewank2008evaluating}.
We assume the cost is divided into $4$ elementary measures, being the number of MOVES, ADDS, MULTS, and NLOPS.
MOVES assumes a flat memory and represents moving fixed-size information (e.g. 64 bits). 
ADDS represents the number of additions, 
MULTS the number of multiplications, 
and NLOPS the number of elementary non-linear operations like !cos! or !exp!.

The gives a complexity function $\cost$ valued in $\RR^4$. 
For primitive operations, we have for instance

\begin{tabular}{ll}
    $\cost$(!*!)=$(3,0,1,0)$ & $\cost($!+!$)=(3,1,0,0)$\\
    $\cost$(!c!)=$(1,0,0,0)$ & $\cost($!sin!$)=(2,0,0,1)$
\end{tabular}

More generally $\cost($!op1!$)=(2,0,0,1)$ for the other unary operations.
For simplicity, we will not address any parallelism in our cost model. 
So the cost of !map (x.e)! on an array of size $n$ will be $n*\cost$(!e!).
Following Lemma \ref{lem:noHO}, we do not need our cost model to deal with higher-order variables and lambda abstractions.
We can thus restrict our attention to the subset of the Target language which does not contain lambdas, applications or variables which are not of ground type.
The cost function extends compositionally to the restricted Target language. 
It is given in Figure~\ref{fig:costmodel}.

\input{figures/costmodel}

\subsection{Cheap gradient principle}

We define the Nesting of Array Operations $\NestA$ of a term !e! of Source by induction on !e! as follows.

\begin{center}
\begin{tabular}{r c l}
    $\NestA$(!c!), $\NestA$(!x!) &=& 0 \\
    $\NestA$($\pi_i$(!e!)), $\NestA$(!op1 e!) &=& $\NestA$(!e!) \\
    $\NestA$(!let x:A = e$_1$ in e$_2$:B!) &=& max($\NestA$(!e$_1$!), $\NestA$(!e$_2$!))  \\ 
    $\NestA$(!< e$_1$, e$_2$ >:AxB!) &=& max($\NestA$(!e$_1$!), $\NestA$(!e$_2$!)) \\ 
    $\NestA$(!e$_1$ op2 e$_2$!) &=& max($\NestA$(!e$_1$!), $\NestA$(!e$_2$!))\\
    $\NestA$(!map2 (x,y.e$_1$) e$_2$ e$_3$!) &=& max(1+$\NestA$(!e$_1$!), $\NestA$(!e$_2$!), $\NestA$(!e$_3$!)) \\
    $\NestA$(!reduce (x,y.e$_1$) e$_2$ e$_3$!) &=& max(1+$\NestA$(!e$_1$!), $\NestA$(!e$_2$!), $\NestA$(!e$_3$!))
\end{tabular}
\end{center}

We can now phrase our main complexity theorem.

\begin{theorem}
    \label{thm:complexity}
    Given a term !$\Gamma \vdash$ e: $\reals$! such that $\NestA$(!e!)$\leq$ $p$.
    Denote by $G$ the term $\grad_\Gamma$!e! after the partial evaluation step from Section~\ref{sub:Partial evaluation and optimization}.
    Then !$\cost$(G) $\leq$ $4*3^{p}*\cost$(e)!.
\end{theorem}

The cheap gradient principle (see e.g. \cite{griewank2008evaluating}) for reverse-mode
asserts that evaluating the gradient of a function $e:\RR^n\to\RR$ 
should be the same order of cost as evaluating $e$. 
More precisely, there should be a constant $K$ such that for each program !$\Gamma \vdash$ e: $\reals$! in the context $\Gamma=\{x_1:\reals,\ldots,x_n:\reals\}$,
!$\cost$($\grad$e) $\leq$ $K$*$\cost$(e)!.

\begin{proof}[Proof Sketch]
    As $\directD{\rho}{\Gamma}{Y}$ is defined by induction on programs, it suffices to show locally 
that $\directD{\rho}{\Gamma}{Y}$ verifies the cheap gradient principle. 

There is a first restriction that prevents our transformation from satisfying the cheap gradient principle.
If we try to show that the cheap gradient principle holds by induction on the term, it fails on !map2!.
When differentiating !map2!, there are three series of calls to (sub)gradients of !e$_1$!. 
The induction hypothesis is then too weak to conclude. 
The problem comes from the fact that !e$_1$! could itself use !map2!. 
So the constant $K$ can be independant of $n$ and of 
the size of the term if we allow it to be dependent on the level of nestings of !map2!.
A similar phenomenon happens with !reduce!.

Another problem is that the continuation part of $\directD{\rho}{\Gamma}{Y}$ adds $O(n)$ MOVES at each step.
This overhead is precisely what that our inlining and forward substitution removes, 
as was exemplified in Section~\ref{sub:Partial evaluation and optimization}. 

After that, the proof is by routine induction on !e!. 
First, one computes the cost of $\directD{\rho}{\Gamma}{Y}$(!e!).
It has $n$ too many MOVES which are removed by inlining and forward substitution using the invariant that at each step, 
at most 2 !x!$_i$ in the continuation are not variables.
\end{proof}

\input{figures/optims}

\subsection{Optimizations} % (fold)
\label{sub:Optimizations}

In Figure~\ref{fig:optim} we present a list of optimizations. 
These optimizations are all obviously valid according to the semantic model. 
As our language is purely functional, we can use these extra optimizations aggressively. 
A lot of simplifications come from the ring structure of the reals, lifted to tuples and arrays.
After these optimizations, the output program is essentially a sequence of let-bindings 
and resembles SSA-code \cite{cytron1989efficient}.

Even though our reverse-mode transformation has the right complexity after the partial evaluation step, 
the constant factor is of huge importance in practice. 
The purity of our transformation allows us to make the most out of generic optimizations.  
In addition, hand-crafted efficient derivatives and more optimizations can easily be added to our language.

\begin{example}
    As shown in the supplementary material,  
    %appendix \ref{sub:gradintro}, 
    the optimizations from Figure~\ref{fig:optim} 
    are sufficient to show that the gradients of the terms of the introduction reduce to the following.
    
    \begin{tabular}{{r c l}}
        $\nabla_A$!prod(A)! &=& !map2 * (scanr * 1 (shift1L A)) (shift1R (scanl * 1 A))!\\
        $\nabla_A$!sum(A)! &=& !map (x -> 1) A!\\
        $\nabla_A$!dot(A,B)! &=& !B! 
    \end{tabular}
\end{example}