\section{Introduction}

% % (Source-to-source) Automatic Differentiation (AD) is a technique for transforming code that implements 
% % a function $f$ into code that computes $f$'s derivative, essentially by using the chain rule for derivatives. 
% % Due to its efficiency and numerical stability, AD is the technique of choice whenever derivatives need to be computed 
% % of functions that are implemented as programs, particularly in high dimentional settings. 
% % Optimisations and Monte Carlo integration algorithms, such as Gradient Descent and Hamiltonian Monte-Carlo methods, 
% % rely crucially on the calculation of derivatives. These algorithms are used in virtually everywhere in machine learning 
% % and computational statistics, and the calculation of derivatives is usually the computational bottle-neck. 
% % AD, roughly speaking, comes in two modes; forward-mode and reverse-mode. 
% % When differentiating a function $\RR^n\to\RR^m$, forward-mode tends to be more efficient if $m \gg n$, while reverse-mode 
% % is generally more performant if $n \gg m$. As most applications reduce to optimization of Monte-Carlo 
% % integration of an objective function $\RR^n\to\RR$ with $n$ very large ($10^4-10^7$), reverse-mode AD is 
% % in many ways the more interesting algorithm.

% % %define diff prog beyond AD, be specific with references/different define-then-run vs Pytorch/Swift etc.
% % %Diff Curry has OK intro

\subsection{Motivation}

In 2008, Pearlmutter and Siskind \cite{pearlmutter2008reverse} introduced Automatic Differentiation (AD) in a functional setting. 
They introduced reverse-mode in their setting. 
Reverse-mode can compute in one pass the whole gradient of a function $\RR^n\to\RR$, and this is very fast.
By comparison, the easier forward-mode transformation would need $n$ passes. 
These functions $\RR^n\to\RR$ actually appear a lot in practice, for instance as loss functions in machine learning.
Reverse-mode is thus important as it computes a whole gradient very quickly for these functions. 
To be as efficient as the usual imperative version of reverse-mode, the transformation in \cite{pearlmutter2008reverse} introduced references.

The lack of purity of reverse-mode is known to make it significantly harder to optimise and find parrallelism. 
None of the current implementations of reverse-mode in a functional setting \cite{lantern_icfp,pearlmutter2008reverse,baydin2016diffsharp} are pure, and often complicated heuristics with no guarantees are used, e.g. \cite{xla}.
It means that for every important non trivial operation, a hand-crafted reverse-derivative must be given to be efficient.
In other words functional implementations seem to stuggle to abstract away from imperative code.

For example, \MH{TODO: show how $\prod_i v_i$ for a vector $v$ is done in some functional AD systems}.
For real world AD systems, modularity is key. They need to be expressive but have a lot of optimised derivatives for operators that are used a lot.
This is easily seen on numpy \MH{TODO: explain a bit better why torch is good by using optimised grad for numpy.}
An efficient purely functional reverse-mode transformation would be more easily explainable, optimisable, and potentially reusable.

\subsection{Problem}

Following \cite{pearlmutter2008reverse}, it is relatively easy to define a purely functional reverse-mode on a first-order language using continuations. 
As remarked by the authors, this is highly inefficient. 
The problem we tackle in this paper is how to go from a simple and general setting where we can relatively easily prove correctness of reverse-mode, 
to a representation that matches more efficient implementations.

The problems that this raises and that our paper deals with are the following. First, there is a tradeoff between a general expressive language 
as opposed to a domain specific language. The latter usually has more static information and a specific representation to perform optimisations.
Then, a lot of optimisations performed on AD implementations consist of a lot of hand-crafted derivatives for a lot of useful operations like matrix-matrix multiplication, dot-product, etc.
They don't seem to come with more theoretical justification, are error prone, and can hardly fit more general optimisations.
\MH{TODO: maybe the example and ref that ML systems spend 90percent of the time outside of numpy.}
This makes these systems harder to prove correct. The problem is thus to stay provably correct and pure, and yet efficient. What's more is we would like something easily provably efficient. 
A real implementation on top of our work would of course optimise further, potentially using hand-crafted operations as well, but it would be based on more solid grounds.

\subsection{Contribution}

Our work provides a sort of compilation scheme from a source functional language to a target functional language for reverse-mode differentiation.
To emphasize some key ideas and for presentation purposes, we present our work on a simple yet expressive language in Section~\ref{sec:simplediff}, 
and show some useful generalisation in Section~\ref{sec:generalisation}. 

The key ideas of our work can be summarised by saying that reverse-mode differentiation
\begin{itemize}
   \item can be made efficient way more easily on unary operators
   \item can be done efficiently and purely using second-order array operators for second-order array operators (e.g. !map,foldl!) 
   \item can be made efficient via standard functional optimisation techniques
\end{itemize}

Our contribution tries to make these claims precise. Our detailed  contribution consist of
\begin{itemize}
    \item introducing an expressive source language with array operations, yet restrictive enough for efficient differentiation
    \item a method for extending our work and modular method for adding and prove correct new primitives as needed in practice 
    \item a new intermediate representation (I.R.) which we call unary form(UNF), essentially turning every operation as a unary one
    \item a simple reverse-mode transformation on the I.R.
    \item an expressive purely functional target language and efficient transformation from the I.R. to this language
    \item several useful generalisations in Section~\ref{sec:generalisation}. 
    \item correctness of the differentiation \ref{sec:correctness}, and complexity guarantees \ref{sec:complexity}
    \item showing how efficiency  is recovered via standard optimisation techniques
    \item a detailed comparison with other work and some limitations to our work in Section~\ref{sec:conclusion}
\end{itemize}

\subsection{Examples}

We introduce a general idea of how to do reverse-mode in a functional setting that is efficient and pure.
We recall some basics of reverse-mode differentiation in Section~\ref{sec:background}.
Let's have a look at what we obtain on a few examples.

\begin{example}[First-order term]
On simple first-order terms, the output code looks like SSA code \cite{cytron1989efficient}.
Let's consider  the term !let w1 = x1 * x2 in let w2 = w1 * x1 in w2! in the context $\Gamma:=\{x1,x2,x3:\RR\}$.\\
After our (inefficient) reverse-mode transformation, we obtain
\begin{center}
    \begin{tabular}{l}
        !let w1,w1'! = !< x1*x2, fun (y1,...,y4) -> (y1+x2*y4, y2+x1*y4, y3)> in!\\
        !let w2,w2'! = !< w1*x1, fun (y1,...,y5) -> w1'!!(y1+w1*y5, y2, y3, y4+x1*y5)> in!\\
        !w_2'(0,0,0,0,1)!
    \end{tabular}
\end{center}
The part !(0,0,0,0,1)! comes as an equivalent to initialing the tangent variables in the imperative reverse-mode algorithm. 

After some general partial evaluation that will be detailed further in the paper, we obtain     
        \begin{center}
            \begin{tabular}{l}
                !let w1 = x1 * x2 in!\\ 
                !let w2 = w1 * x1 in!\\
                !let y1 = 0 in!\\
                !let y2 = 0 in!\\
                !let y3 = 0 in!\\
                !let y4 = 0 in!\\
                !let y5 = 1 in!\\
                !let y1'! != y1+w1*y5 in!\\
                !let y4'! != y4+x1*y5 in!\\
                !(y1'+x2*y4'!!, y2+x1*y4'!!, y3)!
            \end{tabular}
        \end{center}  
For the expert viewer, this is equivalent to the SSA form \cite{cytron1989efficient} of what the imperative reverse-mode differentiation of our initial term would be.\\
Of course, this term can be further optimised to give
        \begin{center}
            \begin{tabular}{{c}}
                !let w1 = x1 * x2 in!\\ 
                !let w2 = w1 * x1 in!\\
                !(w1+x2*x1, x1*x1, 0)!
            \end{tabular}
        \end{center}
    \end{example}

 \begin{example}[Simple operations on arrays]
    On arrays, three simple operations of interest are the dot-product of two vectors, and the product or sum of the elements of a vector.
    In a functional setting, these can be defined as follows:
\begin{center}
    \begin{tabular}{{r c l}}
        !prod(A)! &:=& !reduce * 1 A! \\
        !sum(A)! &:=& !reduce + 0 A! \\
        !dot(A,B)! &:=& !reduce + 0 (map2 * A B)!     
    \end{tabular}
\end{center}
where !reduce! is a known fold left operator for which the function argument is associative and is faster than a fold left.

The gradient of each of these construct w.r.t. !A! will be:
\begin{center}
    \begin{tabular}{{r c l}}
        $\nabla_A$!prod(A)! &:=& !map2 * (scanr x 1 A) (shift1(scanl * 1 A))!\\
        $\nabla_A$!sum(A)! &:=& !map (x,y -> 1) A!\\
        $\nabla_A$!dot(A,B)! &:=& !map2 * B (map (x,y -> 1) A! 
    \end{tabular}
\end{center}
where !shift1 [v1,...,vn] = [1,v1,...,v(n-1)]! is a simple operator yet perhaps strange at first naturally showing up in our constructions.
!scanl! and !scanr! are respectively scan left and scan right. These are like fold but store all intermediate results in an array.
We obtain these gradients, which are efficient and usually derived by hand, as special cases of our work.
They are obtained automatically in our case.
\end{example}   

\subsection{Outline of the paper}

Our work looks like the following compilation scheme
\input{figures/compilation_scheme}

In Section~\ref{sec:background} we recall reduminents of automatic differentiation, and in particular the idea of reverse-mode and the cheap gradient principle.
Next, in Section~\ref{sec:simplediff} we introduce the source language we want to differentiate. We also introduce our new intermediate representation UNF, and a simple reverse-mode macro on this representation.
In Section~\ref{sec:efficientrad} we translate back from UNF to a purely functional target language and show several general optimisations to make our reverse-mode transformation efficient.
In the next Section~\ref{sec:complexity} we show that the previous optimisations are sufficient to ensure our transformation has the right complexity.
Section~\ref{sec:correctness} shows that our construction is correct and we give standard semantics to our source and target languages. 
In Section~\ref{sec:generalisation} we show how to generalise the previous sections for a richer source language. 
Finally, related work and some limitations to our approach are presentend in Section~\ref{sec:conclusion}.