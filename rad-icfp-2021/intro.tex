\section{Introduction}

% % (Source-to-source) Automatic Differentiation (AD) is a technique for transforming code that implements 
% % a function $f$ into code that computes $f$'s derivative, essentially by using the chain rule for derivatives. 
% % Due to its efficiency and numerical stability, AD is the technique of choice whenever derivatives need to be computed 
% % of functions that are implemented as programs, particularly in high dimentional settings. 
% % Optimisations and Monte Carlo integration algorithms, such as Gradient Descent and Hamiltonian Monte-Carlo methods, 
% % rely crucially on the calculation of derivatives. These algorithms are used in virtually everywhere in machine learning 
% % and computational statistics, and the calculation of derivatives is usually the computational bottle-neck. 
% % AD, roughly speaking, comes in two modes; forward-mode and reverse-mode. 
% % When differentiating a function $\RR^n\to\RR^m$, forward-mode tends to be more efficient if $m \gg n$, while reverse-mode 
% % is generally more performant if $n \gg m$. As most applications reduce to optimization of Monte-Carlo 
% % integration of an objective function $\RR^n\to\RR$ with $n$ very large ($10^4-10^7$), reverse-mode AD is 
% % in many ways the more interesting algorithm.

% % %define diff prog beyond AD, be specific with references/different define-then-run vs Pytorch/Swift etc.
% % %Diff Curry has OK intro

% \begin{example}
% 	Let 
% \begin{align*}
% 	\var_1,\var_2,\var_3:\RR\vdash \trm:=\big(\lambda f:\RR\to\RR. (f(\var_1))*\var_1\big)(\lambda \var[2]. \var[2]* \var_2)
% \end{align*}
% After $\partial_1$ we obtain 
% \begin{align*}
% 	(\letin{\var[2]}{\var_1}{\var[2]*\var_2})*\var_1
% \end{align*}
% After $ANF$ we obtain
% \begin{align*}
% 	&\letin{w_1}{\var_1 * \var_2}{\\&\letin{w_2}{w_1 * \var_1}{\\&w_2}}
% \end{align*}
% After $\Dsynrev[\rho]{-}^*$ we obtain
% 	\begin{align*}
% 		&\letin{w_1,w_1'}{\tPair{\var_1 * \var_2}{\inllambda \var[2]_1...\var[2]_4.Y\big(J^1(\var_1,\var_2,\var_3)^T(\var[2]_1,...,\var[2]_4)\big)} }{
% 		\\
% 		&\letin{w_2,w_2'}{\tPair{w_1 * \var_1}{\inllambda \var[2]_1...\var[2]_5.w_1'\big(J^2(\var_1,\var_2,\var_3,w_1)^T(\var[2]_1,...,\var[2]_5)\big)}}{\\ 
% 		& w_2'}}
% 	\end{align*}
% where $Y$ is a continuation variable of type $\RR^3\to\RR^3$, $J^1,J^2$ are Jacobian matrices for $*$ operators which also return their context, $\inllambda$ is just a normal $\lambda$ but inlined for future optimisation.

% After applying $Jacob$, we obtain
% 	\begin{align*}
% 		&\letin{w_1,w_1'}{\tPair{\var_1 * \var_2}{\inllambda \var[2]_1...\var[2]_4.Y(\var[2]_1+\var_2*\var[2]_4,\var[2]_2+\var_1*\var[2]_4,\var[2]_3)} }{
% 		\\
% 		&\letin{w_2,w_2'}{\tPair{w_1 * \var_1}{\inllambda \var[2]_1...\var[2]_5.w_1'(\var[2]_1+w_1 * \var[2]_5,\var[2]_2,\var[2]_3,\var[2]_4+ \var_1 * \var[2]_5)}}{\\ 
% 		& w_2'}}
% 	\end{align*}
% To actually compute the reverse-derivative of the full term, we substitute the identity to $Y$ and apply the whole term to $(0,0,0,0,1)$, the functional equivalent to initialing the sensitivities in the imperative reverse-mode algorithm. We obtain

% \begin{align*}
% 		&\letin{w_1,w_1'}{\tPair{\var_1 * \var_2}{\inllambda \var[2]_1...\var[2]_4.(\var[2]_1+\var_2*\var[2]_4,\var[2]_2+\var_1*\var[2]_4,\var[2]_3)} }{
% 		\\
% 		&\letin{w_2,w_2'}{\tPair{w_1 * \var_1}{\inllambda \var[2]_1...\var[2]_5.w_1'(\var[2]_1+w_1 * \var[2]_5,\var[2]_2,\var[2]_3,\var[2]_4+ \var_1 * \var[2]_5)}}{\\ 
% 		& w_2'(0,0,0,0,1)}}
% 	\end{align*}

% After $\partial_2$, we get
% 	\begin{align*}
% 		&\letin{w_1}{\var_1 * \var_2}{\\
% 		&\letin{w_2}{w_1 * \var_1}{\\
% 		&\bigg[\inllambda \var[2]_1...\var[2]_5.\Big(\inllambda \var[2]_1'...\var[2]_4'.(\var[2]_1+\var_2*\var[2]_4,\var[2]_2+\var_1*\var[2]_4,\var[2]_3)\Big)(\var[2]_1+w_1 * \var[2]_5,\var[2]_2,\var[2]_3,\var[2]_4+ \var_1 * \var[2]_5)\bigg](0,0,0,0,1)
% 		}
% 		}
% 	\end{align*}

% 	Now let's apply the first part of $OPT$:
% 	\begin{align*}
% 		&\letin{w_1}{\var_1 * \var_2}{\\
% 		&\letin{w_2}{w_1 * \var_1}{\\
% 		&\letin{(\var[2]_1\ldots\var[2]_5)}{(0,0,0,0,1)}{\\
% 		& \letin{(\var[2]_1'\ldots\var[2]_4')}{(\var[2]_1+w_1 * \var[2]_5,\var[2]_2,\var[2]_3,\var[2]_4+ \var_1 * \var[2]_5)}{\\
% 		&(\var[2]_1'+\var_2*\var[2]_4',\var[2]_2'+\var_1*\var[2]_4',\var[2]_3')
% 		}}}}
% 	\end{align*}
% Let's unsugar the lets:
% 	\begin{align*}
% 		&\letin{w_1}{\var_1 * \var_2}{\\
% 		&\letin{w_2}{w_1 * \var_1}{\\
% 		&\letin{\var[2]_1}{0}{\\
% 		&\letin{\var[2]_2}{0}{\\
% 		&\letin{\var[2]_3}{0}{\\
% 		&\letin{\var[2]_4}{0}{\\
% 		&\letin{\var[2]_5}{1}{\\
% 		& \letin{\var[2]_1'}{\var[2]_1+w_1 * \var[2]_5}{\\
% 		&\letin{ {\color{red} \var[2]_2'} }{  {\color{red} \var[2]_2} }{\\
% 		&\letin{ {\color{red} \var[2]_3'} }{ {\color{red} \var[2]_3}  }{\\
% 		&\letin{\var[2]_4'}{\var[2]_4+ \var_1 * \var[2]_5}{\\
% 		&(\var[2]_1'+\var_2*\var[2]_4',\var[2]_2'+\var_1*\var[2]_4',\var[2]_3')
% 		}}}}}}}}}}}
% 	\end{align*}
% 	We apply the second part of $OPT$ to remove the variables in red:
% 	\begin{align*}
% 		&\letin{w_1}{\var_1 * \var_2}{\\
% 		&\letin{w_2}{w_1 * \var_1}{\\
% 		&\letin{\var[2]_1}{0}{\\
% 		&\letin{\var[2]_2}{0}{\\
% 		&\letin{\var[2]_3}{0}{\\
% 		&\letin{\var[2]_4}{0}{\\
% 		&\letin{\var[2]_5}{1}{\\
% 		& \letin{\var[2]_1'}{\var[2]_1+w_1 * \var[2]_5}{\\
% 		&\letin{\var[2]_4'}{\var[2]_4+ \var_1 * \var[2]_5}{\\
% 		&(\var[2]_1'+\var_2*\var[2]_4',\var[2]_2+\var_1*\var[2]_4',\var[2]_3)
% 		}}}}}}}}}
% 	\end{align*}
% 	We apply some algebraic simplifications in $\partial_3$
% 	\begin{align*}
% 		&\letin{w_1}{\var_1 * \var_2}{\\
% 		&\letin{w_2}{w_1 * \var_1}{\\
% 		& \letin{\var[2]_1'}{w_1}{\\
% 		&\letin{\var[2]_4'}{\var_1}{\\
% 		&(\var[2]_1'+\var_2*\var[2]_4',\var_1*\var[2]_4',0)
% 		}}}}
% 	\end{align*}
% 	We can optimise some more with $\partial_3$, and we finally obtain
% 	\begin{align*}
% 		&\letin{w_1}{\var_1 * \var_2}{\\
% 		&\letin{w_2}{w_1 * \var_1}{\\
% 		&(w_1+\var_2*\var_1,\var_1*\var_1,0)
% 		}}
% 	\end{align*}
% \end{example}

\subsection{Motivation}

\begin{itemize}
    \item rewrite Pearlmutter's paper
    \item purely functional yet efficient, where P's paper used references
    \item no purity means harder to optimise, lack of real FP, parallelism, modularity
    \item show how $\prod_i v_i$ for a vector $v$ is done in some functional AD systems
    \item modularity, claritity and understanding are key.
\end{itemize} 

\subsection{Problem}

How to go from nice theory to efficient practice?
\begin{itemize}
    \item tradeoff expressivity versus static knowledge to do Optimisations
    \item general optimisation versus ad-hoc hand-writen choice
    \item how to stay provably correct and pure
    \item how to be probably efficient?
\end{itemize}

\subsection{Examples}

\begin{itemize}
    \item $\Dsynrevsymbol$ of !let! and optimise
    \item $\Dsynrevsymbol$ of $\prod_i v_i, \sum_i v_i$ and/or dot-product.
\end{itemize}

\subsection{Contribution}

\begin{itemize}
    \item expressive source language with arrays but restrictive enough for efficient $\Dsynrevsymbol$
    \item method for extending our work and modular method for adding and prove correct new primitives as needed in practice 
    \item new I.R.: UNF
    \item a very simple $\Dsynrevsymbol$ macro
    \item expressive purely functional target language
    \item several generalisation at the end
    \item correctness and complexity theorems
    \item recover efficiency via standard optimisation techniques as opposed to ad-hoc ones
    \item detailed comparison with other work and limitations to our work, some ideas around limitations
\end{itemize}

\subsection{Outline of the paper}

Our work looks like a compilation scheme, which is the following:

\input{figures/compilation_scheme}