\section{Introduction}
\label{sec:intro}

% % (Source-to-source) Automatic Differentiation (AD) is a technique for transforming code that implements 
% % a function $f$ into code that computes $f$'s derivative, essentially by using the chain rule for derivatives. 
% % Due to its efficiency and numerical stability, AD is the technique of choice whenever derivatives need to be computed 
% % of functions that are implemented as programs, particularly in high dimensional settings. 
% % Optimizations and Monte Carlo integration algorithms, such as Gradient Descent and Hamiltonian Monte-Carlo methods, 
% % rely crucially on the calculation of derivatives. These algorithms are used in virtually everywhere in machine learning 
% % and computational statistics, and the calculation of derivatives is usually the computational bottle-neck. 
% % AD, roughly speaking, comes in two modes; forward-mode and reverse-mode. 
% % When differentiating a function $\RR^n\to\RR^m$, forward-mode tends to be more efficient if $m \gg n$, while reverse-mode 
% % is generally more performant if $n \gg m$. As most applications reduce to optimization of Monte-Carlo 
% % integration of an objective function $\RR^n\to\RR$ with $n$ very large ($10^4-10^7$), reverse-mode AD is 
% % in many ways the more interesting algorithm.

% % %define diff prog beyond AD, be specific with references/different define-then-run vs Pytorch/Swift etc.
% % %Diff Curry has OK intro

% \subsection{Motivation}
Deep learning is moving towards increasingly sophisticated optimization objectives that employ tensors and operations on tensors.
Reverse-mode Automatic Differentiation (AD) is a technique to automatically compute the gradient of objective functions of the form $\RR^n\to\RR$.
Such functions appear a lot in practice: for instance, as loss functions in machine learning.

In order to reach the efficiency of the usual imperative version of reverse-mode, the transformations, even in functional languages~\cite{pearlmutter2008reverse}, resort to references.
The lack of purity in reverse-mode is known to make it significantly harder to optimize and parallelize. 
None of the current implementations of reverse-mode in a functional setting~\cite{lantern_icfp,pearlmutter2008reverse,baydin2016diffsharp} are pure, and often complicated heuristics with no guarantees are used, e.g.~\cite{xla}.
As a result, to optimize for efficiency, a hand-crafted reverse-derivative must be given, for every important non-trivial operation.
In other words, abstracting away from imperative code is still a hurdle that functional implementations need to overcome.

% An efficient purely functional reverse-mode transformation would be more easily explainable, optimizable, and reusable.

% \subsection{Problem}

% Following \cite{pearlmutter2008reverse}, it is relatively easy to define a purely functional reverse-mode on a first-order language using continuations. 
% Yet, as remarked by the authors, this is highly inefficient. 
% The problem we tackle in this paper is how to go from a simple and general setting, where we can relatively easily prove correctness of reverse-mode, 
% to a representation that leads to more efficient implementations.

In this paper, we define a purely functional (without references or control mechanisms such as state monads), denotationally correct, and provably efficient reverse-mode AD. 
To do so, we define the Unary Normal Form (UNF) representation inspired by PROPs~\cite{maclane1965categorical} %\cite{hackney2015category} 
and compilation to categories~\cite{elliott2017compiling}.
We can easily define and prove correctness of reverse-mode on this representation. 
The whole reverse-mode transformation is obtained by compiling the language to this Intermediate Representation (IR), applying the simpler reverse-mode transformation, and compiling again to the original language.
After standard optimizations, the output program looks like SSA~\cite{cytron1989efficient} or ANF~\cite{sabry1993reasoning}, which leads to more efficient implementations.

\input{figures/related_work_table}

\subsection{Examples}

We introduce the general idea of efficient reverse-mode in a functional setting through the following examples.
% We recall some basics of reverse-mode differentiation in Section~\ref{sec:background}.

\begin{example}[First-order term]
% On simple first-order terms, the output code is reminiscent of SSA code~\cite{cytron1989efficient}.
Let us consider the term !let w$_1$ = x$_1$ * x$_2$ in let w$_2$ = w$_1$ * x$_1$ in w$_2$! in the context $\Gamma:=\{x_1,x_2, x_3:\RR\}$.\\
After an (inefficient) reverse-mode transformation, we obtain:
\begin{center}
    \begin{tabular}{l}
        !let w$_1$,w$_1'$! = !< x$_1$ * x$_2$, fun (y$_1$,$\ldots$, y$_4$) -> (y$_1$+x$_2$*y$_4$, y$_2$+x$_1$*y$_4$, y$_3$)> in!\\
        !let w$_2$,w$_2'$! = !< w$_1$*x$_1$, fun (y$_1$,$\ldots$, y$_5$) -> w$_1'$!!(y$_1$+w$_1$*y$_5$, y$_2$, y$_3$, y$_4$+x$_1$*y$_5$)> in!\\
        !w$_2'$(0,0,0,0,1)!
    \end{tabular}
\end{center}
The part !(0,0,0,0,1)! corresponds to initializing the tangent variables in the imperative reverse-mode algorithm. 
After some general partial evaluation techniques that will be detailed further in the paper, we obtain:    

    \begin{center}
            \begin{tabular}{l}
                !let w$_1$ = x$_1$ * x$_2$ in!\\ 
                !let w$_2$ = w$_1$ * x$_1$ in!\\
                !let y$_1$,y$_2$,y$_3$,y$_4$,y$_5$ = 0,0,0,0,1 in!\\
                !let y$_1'$! != y$_1$+w$_1$*y$_5$ in!\\
                !let y$_4'$! != y$_4$+x$_1$*y$_5$ in!\\
                !(y$_1'$+x$_2$*y$_4'$!!, y$_2$+x$_1$*y$_4'$!!, y$_3$)!
            \end{tabular}
    \end{center}   
This is very close to the SSA form \cite{cytron1989efficient} of what the imperative reverse-mode differentiation of our initial term would be.

\begin{absolutelynopagebreak}
This term can be further optimized via constant propagation and algebraic simplifications to give
    \begin{center}
            \begin{tabular}{{c}}
                !let w$_1$ = x$_1$ * x$_2$ in!\\ 
                !let w$_2$ = w$_1$ * x$_1$ in!\\
                !(w$_1$+x$_2$*x$_1$, x$_1$*x$_1$, 0)!
            \end{tabular}
    \end{center}
\end{absolutelynopagebreak}

\end{example}


\begin{example}[Simple operations on arrays]
    On arrays, three simple operations of interest are the dot-product of two vectors, and the product or sum of the elements of a vector.
    In a functional setting, these can be defined as follows:
\begin{center}
    \begin{tabular}{{r c l}}
        !prod(A)! &:=& !reduce * 1 A! \\
        !sum(A)! &:=& !reduce + 0 A! \\
        !dot(A,B)! &:=& !reduce + 0 (map2 * A B)!     
    \end{tabular}
\end{center}
where !reduce! is a known fold-left operator for which the function argument is associative. 
It is notably faster to execute than a fold-left as it is parallel-friendly.

The gradient of each of these expressions with respect to !A! is:
\begin{center}
    \begin{tabular}{{r c l}}
        $\nabla_A$!prod(A)! &:=& !map2 * (scanr * 1 (shift1L A)) (shift1R (scanl * 1 A))! \\
        $\nabla_A$!sum(A)! &:=& !map (x -> 1) A!\\
        $\nabla_A$!dot(A,B)! &:=& !B! 
    \end{tabular}
\end{center}
where
\begin{itemize}
\item !scanl! is the scan-left operator that returns all the intermediate results of fold-left,
\item !scanr! is the scan-right operator that returns all the intermediate results of fold-right,
\item !shift1L [v$_1$,$\ldots$,v$_n$]! is the shift-left operator and returns ![v$_2$,$\ldots$,v$_{n}$]!, and 
\item !shift1R [v$_1$,$\ldots$,v$_n$]! is the shift-right operator and returns ![v$_1$,$\ldots$,v$_{n-1}$]!.
\end{itemize}
These gradients are a few examples among numerous ones which are usually derived by hand, and are here obtained automatically as special cases of our work.
\end{example}   

\input{figures/compilation_scheme}

\subsection{Contributions}
We propose a source-code transformation on a simple purely functional language for purely functional reverse-mode differentiation.
Our transformation is comprised of a compilation scheme that is outlined in Figure~\ref{fig:outline}.
We make the following contributions:

\begin{itemize}[leftmargin=*]
\item We present our work with a simple yet expressive array-based language (with constructs such as !map2! and !reduce!) in Section~\ref{sec:simplediff}. 
We show how to directly compute an efficient reverse-mode AD for the expressions of this program (top of Figure~\ref{fig:outline}).  
Furthermore, we show how to extend our work to a richer language in Section~\ref{sec:generalization}.
\item One of the key insights behind efficient reverse-mode AD is to only consider unary operators. 
Inspired by this insight and following IRs such as SSA and ANF, we introduce a novel IR, UNF (Section~\ref{sec:unf}).
We introduce an alternative and easier-to-follow compilation pipeline for efficient reverse-mode AD (bottom of Figure~\ref{fig:outline}).
\item We prove complexity guarantees for the reverse-ADed programs. Furthermore, we show a list of optimizations that can further improve the constant factors (Section~\ref{sec:complexity}).
\item We prove the correctness of our transformations (top/bottom parts of Figure~\ref{fig:outline}) by defining a denotational semantics of our languages using multicategories and concategories (Section~\ref{sec:correctness}).
\end{itemize}

Next, we recall rudiments of automatic differentiation, forward and reverse-mode differentiation.