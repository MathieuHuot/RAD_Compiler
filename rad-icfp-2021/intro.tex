\section{Introduction}

% % (Source-to-source) Automatic Differentiation (AD) is a technique for transforming code that implements 
% % a function $f$ into code that computes $f$'s derivative, essentially by using the chain rule for derivatives. 
% % Due to its efficiency and numerical stability, AD is the technique of choice whenever derivatives need to be computed 
% % of functions that are implemented as programs, particularly in high dimentional settings. 
% % Optimisations and Monte Carlo integration algorithms, such as Gradient Descent and Hamiltonian Monte-Carlo methods, 
% % rely crucially on the calculation of derivatives. These algorithms are used in virtually everywhere in machine learning 
% % and computational statistics, and the calculation of derivatives is usually the computational bottle-neck. 
% % AD, roughly speaking, comes in two modes; forward-mode and reverse-mode. 
% % When differentiating a function $\RR^n\to\RR^m$, forward-mode tends to be more efficient if $m \gg n$, while reverse-mode 
% % is generally more performant if $n \gg m$. As most applications reduce to optimization of Monte-Carlo 
% % integration of an objective function $\RR^n\to\RR$ with $n$ very large ($10^4-10^7$), reverse-mode AD is 
% % in many ways the more interesting algorithm.

% % %define diff prog beyond AD, be specific with references/different define-then-run vs Pytorch/Swift etc.
% % %Diff Curry has OK intro

% \begin{example}
% 	Let 
% \begin{align*}
% 	\var_1,\var_2,\var_3:\RR\vdash \trm:=\big(\lambda f:\RR\to\RR. (f(\var_1))*\var_1\big)(\lambda \var[2]. \var[2]* \var_2)
% \end{align*}
% After $\partial_1$ we obtain 
% \begin{align*}
% 	(\letin{\var[2]}{\var_1}{\var[2]*\var_2})*\var_1
% \end{align*}
% After $ANF$ we obtain
% \begin{align*}
% 	&\letin{w_1}{\var_1 * \var_2}{\\&\letin{w_2}{w_1 * \var_1}{\\&w_2}}
% \end{align*}
% After $\Dsynrev[\rho]{-}^*$ we obtain
% 	\begin{align*}
% 		&\letin{w_1,w_1'}{\tPair{\var_1 * \var_2}{\inllambda \var[2]_1...\var[2]_4.Y\big(J^1(\var_1,\var_2,\var_3)^T(\var[2]_1,...,\var[2]_4)\big)} }{
% 		\\
% 		&\letin{w_2,w_2'}{\tPair{w_1 * \var_1}{\inllambda \var[2]_1...\var[2]_5.w_1'\big(J^2(\var_1,\var_2,\var_3,w_1)^T(\var[2]_1,...,\var[2]_5)\big)}}{\\ 
% 		& w_2'}}
% 	\end{align*}
% where $Y$ is a continuation variable of type $\RR^3\to\RR^3$, $J^1,J^2$ are Jacobian matrices for $*$ operators which also return their context, $\inllambda$ is just a normal $\lambda$ but inlined for future optimisation.

% After applying $Jacob$, we obtain
% 	\begin{align*}
% 		&\letin{w_1,w_1'}{\tPair{\var_1 * \var_2}{\inllambda \var[2]_1...\var[2]_4.Y(\var[2]_1+\var_2*\var[2]_4,\var[2]_2+\var_1*\var[2]_4,\var[2]_3)} }{
% 		\\
% 		&\letin{w_2,w_2'}{\tPair{w_1 * \var_1}{\inllambda \var[2]_1...\var[2]_5.w_1'(\var[2]_1+w_1 * \var[2]_5,\var[2]_2,\var[2]_3,\var[2]_4+ \var_1 * \var[2]_5)}}{\\ 
% 		& w_2'}}
% 	\end{align*}
% To actually compute the reverse-derivative of the full term, we substitute the identity to $Y$ and apply the whole term to $(0,0,0,0,1)$, the functional equivalent to initialing the sensitivities in the imperative reverse-mode algorithm. We obtain

% \begin{align*}
% 		&\letin{w_1,w_1'}{\tPair{\var_1 * \var_2}{\inllambda \var[2]_1...\var[2]_4.(\var[2]_1+\var_2*\var[2]_4,\var[2]_2+\var_1*\var[2]_4,\var[2]_3)} }{
% 		\\
% 		&\letin{w_2,w_2'}{\tPair{w_1 * \var_1}{\inllambda \var[2]_1...\var[2]_5.w_1'(\var[2]_1+w_1 * \var[2]_5,\var[2]_2,\var[2]_3,\var[2]_4+ \var_1 * \var[2]_5)}}{\\ 
% 		& w_2'(0,0,0,0,1)}}
% 	\end{align*}

% After $\partial_2$, we get
% 	\begin{align*}
% 		&\letin{w_1}{\var_1 * \var_2}{\\
% 		&\letin{w_2}{w_1 * \var_1}{\\
% 		&\bigg[\inllambda \var[2]_1...\var[2]_5.\Big(\inllambda \var[2]_1'...\var[2]_4'.(\var[2]_1+\var_2*\var[2]_4,\var[2]_2+\var_1*\var[2]_4,\var[2]_3)\Big)(\var[2]_1+w_1 * \var[2]_5,\var[2]_2,\var[2]_3,\var[2]_4+ \var_1 * \var[2]_5)\bigg](0,0,0,0,1)
% 		}
% 		}
% 	\end{align*}

% 	Now let's apply the first part of $OPT$:
% 	\begin{align*}
% 		&\letin{w_1}{\var_1 * \var_2}{\\
% 		&\letin{w_2}{w_1 * \var_1}{\\
% 		&\letin{(\var[2]_1\ldots\var[2]_5)}{(0,0,0,0,1)}{\\
% 		& \letin{(\var[2]_1'\ldots\var[2]_4')}{(\var[2]_1+w_1 * \var[2]_5,\var[2]_2,\var[2]_3,\var[2]_4+ \var_1 * \var[2]_5)}{\\
% 		&(\var[2]_1'+\var_2*\var[2]_4',\var[2]_2'+\var_1*\var[2]_4',\var[2]_3')
% 		}}}}
% 	\end{align*}
% Let's unsugar the lets:
% 	\begin{align*}
% 		&\letin{w_1}{\var_1 * \var_2}{\\
% 		&\letin{w_2}{w_1 * \var_1}{\\
% 		&\letin{\var[2]_1}{0}{\\
% 		&\letin{\var[2]_2}{0}{\\
% 		&\letin{\var[2]_3}{0}{\\
% 		&\letin{\var[2]_4}{0}{\\
% 		&\letin{\var[2]_5}{1}{\\
% 		& \letin{\var[2]_1'}{\var[2]_1+w_1 * \var[2]_5}{\\
% 		&\letin{ {\color{red} \var[2]_2'} }{  {\color{red} \var[2]_2} }{\\
% 		&\letin{ {\color{red} \var[2]_3'} }{ {\color{red} \var[2]_3}  }{\\
% 		&\letin{\var[2]_4'}{\var[2]_4+ \var_1 * \var[2]_5}{\\
% 		&(\var[2]_1'+\var_2*\var[2]_4',\var[2]_2'+\var_1*\var[2]_4',\var[2]_3')
% 		}}}}}}}}}}}
% 	\end{align*}
% 	We apply the second part of $OPT$ to remove the variables in red:
% 	\begin{align*}
% 		&\letin{w_1}{\var_1 * \var_2}{\\
% 		&\letin{w_2}{w_1 * \var_1}{\\
% 		&\letin{\var[2]_1}{0}{\\
% 		&\letin{\var[2]_2}{0}{\\
% 		&\letin{\var[2]_3}{0}{\\
% 		&\letin{\var[2]_4}{0}{\\
% 		&\letin{\var[2]_5}{1}{\\
% 		& \letin{\var[2]_1'}{\var[2]_1+w_1 * \var[2]_5}{\\
% 		&\letin{\var[2]_4'}{\var[2]_4+ \var_1 * \var[2]_5}{\\
% 		&(\var[2]_1'+\var_2*\var[2]_4',\var[2]_2+\var_1*\var[2]_4',\var[2]_3)
% 		}}}}}}}}}
% 	\end{align*}
% 	We apply some algebraic simplifications in $\partial_3$
% 	\begin{align*}
% 		&\letin{w_1}{\var_1 * \var_2}{\\
% 		&\letin{w_2}{w_1 * \var_1}{\\
% 		& \letin{\var[2]_1'}{w_1}{\\
% 		&\letin{\var[2]_4'}{\var_1}{\\
% 		&(\var[2]_1'+\var_2*\var[2]_4',\var_1*\var[2]_4',0)
% 		}}}}
% 	\end{align*}
% 	We can optimise some more with $\partial_3$, and we finally obtain
% 	\begin{align*}
% 		&\letin{w_1}{\var_1 * \var_2}{\\
% 		&\letin{w_2}{w_1 * \var_1}{\\
% 		&(w_1+\var_2*\var_1,\var_1*\var_1,0)
% 		}}
% 	\end{align*}
% \end{example}

\subsection{Motivation}

In 2008, Pearlmutter and Siskind \cite{pearlmutter2008reverse} introduced AD in a functional setting. 
They introduced reverse-mode in their setting. 
Reverse-mode can compute in one pass the whole gradient of a function $\RR^n\to\RR$, and this is very fast.
By comparison, the easier forward-mode transformation would need $n$ passes. 
These functions $\RR^n\to\RR$ actually appear a lot in practice, for instance as loss functions in machine learning.
Reverse-mode is thus important as it computes a whole gradient very quickly for these functions. 
To be as efficient as the usual imperative version of reverse-mode, the transformation in \cite{pearlmutter2008reverse} introduced references.

The lack of purity of reverse-mode is known to make it significantly harder to optimise and find parrallelism. 
None of the current implementations of reverse-mode in a functional setting are pure \cite{}, and often complicated heuristics with no guarantees are used \cite{}.
It means that for every important non trivial operation, a hand-crafted reverse-derivative must be given to be efficient.
In other words functional implementations seem to stuggle to abstract away from imperative code.

For example, \MH{TODO: show how $\prod_i v_i$ for a vector $v$ is done in some functional AD systems}.
For real world AD systems, modularity is key. They need to be expressive but have a lot of optimised derivatives for operators that are used a lot.
This is easily seen on numpy \MH{TODO: explain a bit better why torch is good by using optimised grad for numpy.}
An efficient purely functional reverse-mode transformation would be more easily explainable, optimisable, and potentially reusable.

\subsection{Problem}

Following \cite{pearlmutter2008reverse}, it is relatively easy to define a purely functional reverse-mode on a first-order language using continuations. 
As remarked by the authors, this is highly inefficient. 
The problem we tackle in this paper is how to go from a simple and general setting where we can relatively easily prove correctness of reverse-mode, 
to a representation that matches more efficient implementations.

The problems that this raises and that our paper deals with are the following. First, there is a tradeoff between a general expressive language 
as opposed to a domain specific language. The latter usually has more static information and a specific representation to perform optimisations.
Then, a lot of optimisations performed on AD implementations consist of a lot of hand-crafted derivatives for a lot of useful operations like matrix-matrix multiplication, dot-product, etc.
They don't seem to come with more theoretical justification, are error prone, and can hardly fit more general optimisations.
\MH{TODO: maybe the example and ref that ML systems ake 90percent of the time outside of numpy.}
This makes these systems harder to prove correct. The problem is thus to stay provably correct and pure, and yet efficient. What's more is we would like something easily provably efficient. 
A real implementation on top of our work would of course optimise further, potentially using hand-crafted operations as well, but it would be based on more solid grounds.

\subsection{Examples}

We introduce a general idea of how to do reverse-mode in a functional setting that is efficient and pure. 
On simple first-order terms, the output code looks like SSA code\cite{}.

\MH{TODO: example !let!.}

On arrays, two simple operations of interest are the dot-product of two vectors and the product of the elements of a vector.
\MH{TODO: examples $\prod_i v_i, \sum_i v_i$ and/or dot-product.}

\subsection{Contribution}

\begin{itemize}
    \item expressive source language with arrays but restrictive enough for efficient $\Dsynrevsymbol$
    \item method for extending our work and modular method for adding and prove correct new primitives as needed in practice 
    \item new I.R.: UNF
    \item a very simple $\Dsynrevsymbol$ macro
    \item expressive purely functional target language
    \item several generalisation at the end
    \item correctness and complexity theorems
    \item recover efficiency via standard optimisation techniques as opposed to ad-hoc ones
    \item detailed comparison with other work and limitations to our work, some ideas around limitations
\end{itemize}

\subsection{Outline of the paper}

Our work looks like a compilation scheme, which is the following:

\input{figures/compilation_scheme}