\section{Introduction}
\label{sec:intro}

% % (Source-to-source) Automatic Differentiation (AD) is a technique for transforming code that implements 
% % a function $f$ into code that computes $f$'s derivative, essentially by using the chain rule for derivatives. 
% % Due to its efficiency and numerical stability, AD is the technique of choice whenever derivatives need to be computed 
% % of functions that are implemented as programs, particularly in high dimensional settings. 
% % Optimizations and Monte Carlo integration algorithms, such as Gradient Descent and Hamiltonian Monte-Carlo methods, 
% % rely crucially on the calculation of derivatives. These algorithms are used in virtually everywhere in machine learning 
% % and computational statistics, and the calculation of derivatives is usually the computational bottle-neck. 
% % AD, roughly speaking, comes in two modes; forward-mode and reverse-mode. 
% % When differentiating a function $\RR^n\to\RR^m$, forward-mode tends to be more efficient if $m \gg n$, while reverse-mode 
% % is generally more performant if $n \gg m$. As most applications reduce to optimization of Monte-Carlo 
% % integration of an objective function $\RR^n\to\RR$ with $n$ very large ($10^4-10^7$), reverse-mode AD is 
% % in many ways the more interesting algorithm.

% % %define diff prog beyond AD, be specific with references/different define-then-run vs Pytorch/Swift etc.
% % %Diff Curry has OK intro

\subsection{Motivation}

In 2008, Pearlmutter and Siskind~\cite{pearlmutter2008reverse} introduced Automatic Differentiation (AD) in a functional setting, where they defined reverse-mode. 
Reverse-mode computes in a single fast pass the whole gradient of a function $\RR^n\to\RR$.
By comparison, the easier forward-mode transformation would require $n$ passes. 
These functions $\RR^n\to\RR$ appear a lot in practice: for instance, as loss functions in machine learning.
Reverse-mode is thus important, as it computes the whole gradient of such functions very quickly. 
However, in order to reach the efficiency of the usual imperative version of reverse-mode, the transformation in~\cite{pearlmutter2008reverse} resorts to references.

The lack of purity in reverse-mode is known to make it significantly harder to optimize and parallelize. 
None of the current implementations of reverse-mode in a functional setting~\cite{lantern_icfp,pearlmutter2008reverse,baydin2016diffsharp} are pure, and often complicated heuristics with no guarantees are used, e.g.~\cite{xla}.

As a result, to optimize for efficiency, a hand-crafted reverse-derivative must be given, for every important non trivial operation.
In other words, abstracting away from imperative code is still a hurdle that functional implementations need to overcome.

An efficient purely functional reverse-mode transformation would be more easily explainable, optimizable, and reusable.

\MH{TODO: rewrite for more theory people. some ideas: for canonical than Plotkin's thing because normal semantics, complexity proof compared to all other recent papers, 
all papers that have implementations, their interface in the paper is too high level and misses a lot of things (e.g. in \cite{vytiniotis2019differentiable,lantern_icfp,sherman2021}) compared to implementation, hinders progress. 
For instance, in \cite{lantern_icfp} it's a lot of manually optimized and hand-derived primitives to achieve good performance.}

\subsection{Problem}

Following \cite{pearlmutter2008reverse}, it is relatively easy to define a purely functional reverse-mode on a first-order language using continuations. 
Yet, as remarked by the authors, this is highly inefficient. 
The problem we tackle in this paper is how to go from a simple and general setting, where we can relatively easily prove correctness of reverse-mode, 
to a representation that leads to more efficient implementations.

This raises several issues that we deal with throughout the paper. First, there is a tradeoff to reach between a general expressive language and a domain specific one. The latter usually has more static information and a specific representation that lends itself to better optimizations.
Then, many optimizations performed on AD implementations consist in hand-crafted derivatives for useful operations like matrix-matrix multiplication, dot-product, etc.
They don't seem to arise from theoretical justifications, are error prone, and can hardly fit with more general optimizations.
\MH{TODO: maybe the example and ref that ML systems spend 90percent of the time outside of numpy.}
This makes these systems harder to prove correct. The problem is thus to ensure provable correctness and pureness, while not compromising on efficiency. In addition, we would like something easily provably efficient. 
A real-world implementation based on our work would of course optimize further, potentially using hand-crafted operations as well, but it would be based on solid grounds.

\MH{TODO: same as above, maybe rewrite for more theory audience.}

\subsection{Summary of contributions}

We propose a source-code transformation on a simple purely functional language for purely functional reverse-mode differentiation.
Our transformation is comprised of a compilation scheme through a novel intermediate representation (IR): unary form (UNF).
To emphasize the key ideas and for expository purposes, we present our work with a simple yet expressive language in Section~\ref{sec:simplediff}, 
and show how to extend our work to a richer language in Section~\ref{sec:generalization}.

We summarize the key ideas of our work. Reverse-mode can be made efficient
\begin{itemize}
   \item easily if there are only unary operators
   \item in a purely functional way
   \item on expressive constructs such as second-order array operators (e.g. !map2, reduce!) 
   \item via standard functional optimization techniques
\end{itemize}

Our contribution makes these claims precise. In more details, we introduce
\begin{itemize}
    \item an expressive source language with array operations, yet restrictive enough for efficient differentiation (\S\ref{sub:sourcelang})
    \item a method for extending our work and modular approaches to add and prove correct new primitives as needed in practice (\S\ref{sec:generalization})
    \item a new intermediate representation (IR), UNF, essentially turning every operation into a unary one which inputs and outputs a tuple (\S\ref{sec:unf})
    \item a transformation between the IR and the main language (\S\ref{sub:transformations to and from UNF})
    \item a simple reverse-mode transformation on the IR (\S\ref{sub:Simple reverse mode transformation})
    \item the direct reverse-mode transformation obtained from the source to the target language (\S\ref{sub:Macro for pure reverse mode transformation})
    \item denotational semantics of our languages and transformations using multicategories and concategories (\S\ref{sec:correctness})
    \item correctness of the transformations (\S\ref{sec:correctness})
    \item complexity guarantees (\S\ref{sec:complexity})
    \item how efficiency is recovered via standard optimization techniques (\S\ref{sub:Optimizations})
    \item several additions to the source language (\S\ref{sec:generalization}) \MH{should emphasize that it should be easier for us to add primitives compared to several recent work. give recipe.}
    \item a detailed comparison with other work and some limitations to our work (\S\ref{sec:conclusion})
\end{itemize}

\subsection{Examples}

We introduce the general idea of how we do efficient reverse-mode in a functional setting on some examples.
We recall some basics of reverse-mode differentiation in Section~\ref{sec:background}.

\begin{example}[First-order term]
On simple first-order terms, the output code looks like SSA code \cite{cytron1989efficient}.
Let us consider  the term !let w1 = x1 * x2 in let w2 = w1 * x1 in w2! in the context $\Gamma:=\{x1,x2,x3:\RR\}$.\\
After our (inefficient) reverse-mode transformation, we obtain
\begin{center}
    \begin{tabular}{l}
        !let w1,w1'! = !< x1*x2, fun (y1,...,y4) -> (y1+x2*y4, y2+x1*y4, y3)> in!\\
        !let w2,w2'! = !< w1*x1, fun (y1,...,y5) -> w1'!!(y1+w1*y5, y2, y3, y4+x1*y5)> in!\\
        !w_2'(0,0,0,0,1)!
    \end{tabular}
\end{center}
The part !(0,0,0,0,1)! comes as an equivalent to initialing the tangent variables in the imperative reverse-mode algorithm. 

After some general partial evaluation techniques that will be detailed further in the paper, we obtain     

    \begin{center}
            \begin{tabular}{l}
                !let w1 = x1 * x2 in!\\ 
                !let w2 = w1 * x1 in!\\
                !let y1,y2,y3,y4,y5 = 0,0,0,0,1 in!\\
                !let y1'! != y1+w1*y5 in!\\
                !let y4'! != y4+x1*y5 in!\\
                !(y1'+x2*y4'!!, y2+x1*y4'!!, y3)!
            \end{tabular}
    \end{center}   
For the expert viewer, this is equivalent to the SSA form \cite{cytron1989efficient} of what the imperative reverse-mode differentiation of our initial term would be.\\
This term can be further optimised via constant propagation and algebraic simplifications to give
        \begin{center}
            \begin{tabular}{{c}}
                !let w1 = x1 * x2 in!\\ 
                !let w2 = w1 * x1 in!\\
                !(w1+x2*x1, x1*x1, 0)!
            \end{tabular}
        \end{center}
    \end{example}

 \begin{example}[Simple operations on arrays]
    On arrays, three simple operations of interest are the dot-product of two vectors, and the product or sum of the elements of a vector.
    In a functional setting, these can be defined as follows:
\begin{center}
    \begin{tabular}{{r c l}}
        !prod(A)! &:=& !reduce * 1 A! \\
        !sum(A)! &:=& !reduce + 0 A! \\
        !dot(A,B)! &:=& !reduce + 0 (map2 * A B)!     
    \end{tabular}
\end{center}
where !reduce! is a known fold left operator for which the function argument is associative. 
It is notably faster to execute than a fold left as it parallel friendly.

The gradient of each of these construct w.r.t. !A! will be:
\begin{center}
    \begin{tabular}{{r c l}}
        $\nabla_A$!prod(A)! &:=& !map2 * (scanr * 1 A) (shift1(scanl * 1 A))!\\
        $\nabla_A$!sum(A)! &:=& !map (x -> 1) A!\\
        $\nabla_A$!dot(A,B)! &:=& !B! 
    \end{tabular}
\end{center}
where !shift1 [v1,...,vn] = [1,v1,...,v(n-1)]! is a simple operator yet perhaps strange at first naturally showing up in our constructions.
!scanl! and !scanr! are respectively scan left and scan right. These return all the intermediate result of a fold in an array.
These gradients are some examples among numerous ones which are usually derived by hand, and are here obtained automatically as special cases of our work.
\end{example}   

\subsection{Outline of the paper}

Our main transformation is present on the top of the diagram and presented in Section~\ref{sec:simplediff}. 
It it decomposed 3 steps which are presented in Section~\ref{sec:unf}.
\input{figures/compilation_scheme}

In Section~\ref{sec:background} we recall rudiments of automatic differentiation, forward and reverse-mode differentiation.
Next, in Section~\ref{sec:simplediff} we introduce the source and target languages for differentiation, and our reverse mode transformation. 
In Section~\ref{sec:unf} we introduce a new intermediate representation UNF, and a simple reverse-mode macro on this representation.
We introduce transformations from the source language to UNF and from UNF to the target language.
Next, in Section~\ref{sec:correctness} we give denotational semantics to our languages and show that our decomposition respects the diagram above and that our reverse mode transformation is correct.
In Section~\ref{sec:complexity}, we show that our transformation verifies the cheap gradient principle and extra general optimizations.
In Section~\ref{sec:generalization} we show how to adapt our work for a richer source language. 
Finally, related work, limitations to our approach and future work are presented in Section~\ref{sec:conclusion}.