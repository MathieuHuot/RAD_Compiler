\section{Beyond the Source language}
\label{sec:generalization}
In this section, we generalize our language by relaxing the imposed restrictions.
First, we allow free variables for the function argument of !reduce! in Section~\ref{sub:lift_reduce}. 
Then we show how to support conditionals in Section~\ref{sub:lift_conditional}.
% Then we show how to support more array operations (Section~\ref{sub:lift_more_arr_op}), conditionals (Section~\ref{sub:lift_conditional}), and non-smooth scalar operators (Section~\ref{sub:lift_non_smooth}).
% Afterwards, we extend the language to support general arrays instead of supporting only $\Array{\reals}{n}$ in Section~\ref{sub:lift_gen_arr}. 
Finally, we provide a recipe for adding more constructs to the language in Section~\ref{sub:lift_recipe}. Further generalizations for more array operations, non-smooth scalar operators, and general array support can be found in the supplementary materials.

\subsection{Lifting the restriction on reduce}
\label{sub:lift_reduce}

Assume we allow $\Gamma$, !x!: $\reals$, !y!: $\reals$ $\vdash$ !e!: $\reals$ to be the function argument in
!reduce (x,y.e) v A!. We need to add a term depending on $\grad_{\{xi\}}e$ to !yi! in the continuation.
As writing the derivative becomes quite cumbersome, we use a more compact notation using arrays of tuples. 
Similarly to the monoid $\widehat{+},0_\Gamma$ defined in Section~\ref{sub:Macro for pure reverse mode transformation},
we use $\widehat{\times},1_\Gamma$ for obvious extension of the monoid $(\reals,\times,1)$. The modified reverse-mode transformation is shown in Figure~\ref{fig:lift_reduce}.

We can now compute the reverse-mode transformation as follows.

\begin{figure}

\begin{small}
\begin{tabular}{|r c l|}
\hline
$\directD{\rho}{\Gamma}{Y}$(!reduce (x,y.e1) e$_2$ e$_3$!) 
&=& $\ldots$ same as Figure~\ref{fig:direct_diff_macro} up until !A_$3$! \\
&& !let B$_0$=map2 (a,b ->(!$\grad_\Gamma$!e)[a/x,b/y] A$_0$ A) in!\\
&& !let B$_1$=scanr $1_\Gamma$ $\widehat{\times}$ B$_0$ in! \\
&& !let B$_2$=reduce $\widehat{+}$ $0_\Gamma$ B$_1$ in! \\
&& !<reduce (x,y.e1) y$_1$ A, fun (x$_1$,$\ldots$,x$_n$,z) ->! \\
&& !Y((x$_1$,$\ldots$,x$_n$)$\widehat{+}(z*$B$_2$),0,map2 (a,b. a*b*z) A$_2$ A$_3$)! \\ \hline
\end{tabular}
\end{small}
\caption{The reverse-mode AD transformation of \codekw{reduce} without restrictions on its function argument.}
\label{fig:lift_reduce}
\end{figure}

\subsection{Conditionals} % (fold)
\label{sub:lift_conditional}

\smartpara{Non-Solution 1}
Adding conditionals to the source language can be done easily, for instance via defining

\begin{tabular}{r c l}
$\directD{\rho}{\Gamma}{Y}$(!if e$_1$ then e$_2$ else e$_3$!) &=& !if e$_1$ then $\directD{\rho}{\Gamma}{Y}$(e$_2$) else $\directD{\rho}{\Gamma}{Y}$(e$_3$)! 
\end{tabular}

The problem is that this could break the complexity of reverse mode because of the non-linear usage of $Y$, and makes everything harder to optimize.

\smartpara{Non-Solution 2}
A slightly better option would be to define 

\begin{tabular}{r c l}
    $\directD{\rho}{\Gamma}{Y}$(!if e$_1$ then e$_2$ else e$_3$!) 
    &=& !let b=e$_1$ in!   \\
    && !<if b then e$_2$ else e$_3$, fun (x$_1$,$\ldots$,x$_n$,z) ->! \\
    && !Y(b*!$\directD{\rho}{\Gamma}{Y}$!(e$_2$)(x$_1$,$\ldots$,x$_n$,z)+!\\
    && \quad!(1-b)*!$\directD{\rho}{\Gamma}{Y}$!(e$_3$)(x$_1$,$\ldots$,x$_n$,z))>!
\end{tabular}

It is slightly better because both derivatives of !e2! and !e3! are put together, and this might unlock some optimizations, 
but there is still a non-linear usage of the continuation variable !Y!.

\smartpara{Solution}
If we know we want to compute the whole gradient of the expression, we can define the translation as follows:

\begin{tabular}{r c l}
    $\directD{\rho}{\Gamma}{Y}$(!if e$_1$ then e$_2$ else e$_3$!) 
    &=& !let b=e$_1$ in!   \\
    && !<if b then e$_2$ else e$_3$, fun (x$_1$,$\ldots$,x$_n$,z) ->! \\
    && !Y((x$_1$,$\ldots$,x$_n$)!$\widehat{+}$($\grad_\Gamma$!(e$_2$)*b!$\widehat{+}\grad_\Gamma$!(e$_3$)*(1-b))*z)>!\\
\end{tabular}

This time, there is a linear usage of the continuation variable !Y!. We note that adding conditionals does not break differentiability as long as there are no non-trivial primitives $\reals\to\BB$.
Non smooth functions such as the Rectified Linear Unit (ReLU) in machine learning which can be defined as $ReLU(x)\defeq max(0,x)$ 
requires a non smooth primitive such as $>0: \reals\to\BB$. In several AD systems such as TensorFlow, 
the condition is evaluated before differentiation is applied, and so conditionals are never directly differentiated.

\subsection{Recipe for Adding More Constructs}
\label{sub:lift_recipe}
To add more constructs to our language, 
there is no need to go through UNF; most of the backend of our work can be used as a black box.
The non-smooth variables of the operator (typically booleans or integers) should be considered as external to the language, similarly to the way we treat the index $n$ of arrays.

\smartpara{Reverse AD}
If the operator has type $A\to B$, then one should provide its transpose Jacobian, a term $B\to A$. 
Such operators can often be unrolled to first-order programs. 

\smartpara{Correctness}
To check correctness of the given Jacobian, it suffices to 
check that the semantics of the transpose Jacobian matches the Jacobian of the unrolled program. 

\smartpara{Complexity}
To ensure the complexity guarantee of the whole transformation, one needs to check that there a linear usage of the continuation variable $Y$ and that the cost of the 
proposed Jacobian is at most $k$ times the complexity of the operator. 
