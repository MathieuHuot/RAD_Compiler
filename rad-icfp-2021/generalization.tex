\section{Beyond the Source language}
\label{sec:generalization}
In this section, we generalize our language by relaxing the imposed restrictions.
First, we allow free variables for the function argument of !reduce! in Section~\ref{sub:lift_reduce}. 
Then we show how to support more array operations (Section~\ref{sub:lift_more_arr_op}), conditionals (Section~\ref{sub:lift_conditional}), and non-smooth scalar operators (Section~\ref{sub:lift_non_smooth}).
Afterwards, we extend the language to support general arrays instead of supporting only $\Array{\reals}{n}$ in Section~\ref{sub:lift_gen_arr}. Finally, we provide a recipe for adding more constructs to the language in Section~\ref{sub:lift_recipe}.

\subsection{Lifting the restriction on reduce}
\label{sub:lift_reduce}

Assume we allow $\Gamma$, !x!: $\reals$, !y!: $\reals$ $\vdash$ !e!: $\reals$ to be the function argument in
!reduce (x,y.e) v A!. We need to add a term depending on $\grad_{\{xi\}}e$ to !yi! in the continuation.
As writing the derivative becomes quite cumbersome, we use a more compact notation using arrays of tuples. 
Similarly to the monoid $\widehat{+},0_\Gamma$ defined in Section~\ref{sub:Macro for pure reverse mode transformation},
we use $\widehat{\times},1_\Gamma$ for obvious extension of the monoid $(\reals,\times,1)$. The modified reverse-mode transformation is shown in Figure~\ref{fig:lift_reduce}.

We can now compute the reverse-mode transformation as follows.

\begin{figure}

\begin{small}
\begin{tabular}{|r c l|}
\hline
$\directD{\rho}{\Gamma}{Y}$(!reduce (x,y.e1) e$_2$ e$_3$!) 
&=& $\ldots$ same as Figure~\ref{fig:direct_diff_macro} up until !A_$3$! \\
&& !let B$_0$=map2 (a,b ->(!$\grad_\Gamma$!e)[a/x,b/y] A$_0$ A) in!\\
&& !let B$_1$=scanr $1_\Gamma$ $\widehat{\times}$ B$_0$ in! \\
&& !let B$_2$=reduce $\widehat{+}$ $0_\Gamma$ B$_1$ in! \\
&& !<reduce (x,y.e1) y$_1$ A, fun (x$_1$,$\ldots$,x$_n$,z) ->! \\
&& !Y((x$_1$,$\ldots$,x$_n$)$\widehat{+}$B$_2$,0,map2 (a,b. a*b*z) A$_2$ A$_3$)! \\ \hline
\end{tabular}
\end{small}
\caption{The reverse-mode AD transformation of \codekw{reduce} without restrictions on its function argument.}
\label{fig:lift_reduce}
\end{figure}

\subsection{Conditionals} % (fold)
\label{sub:lift_conditional}

\smartpara{Non-Solution 1}
Adding conditionals to the source language can be done easily, for instance via defining

\begin{tabular}{r c l}
$\directD{\rho}{\Gamma}{Y}$(!if e$_1$ then e$_2$ else e$_3$!) &=& !if e$_1$ then $\directD{\rho}{\Gamma}{Y}$(e$_2$) else $\directD{\rho}{\Gamma}{Y}$(e$_3$)! 
\end{tabular}

The problem is that this could break the complexity of reverse mode because of the non-linear usage of $Y$, and makes everything harder to optimize.

\smartpara{Non-Solution 2}
A slightly better option would be to define 

\begin{tabular}{r c l}
    $\directD{\rho}{\Gamma}{Y}$(!if e$_1$ then e$_2$ else e$_3$!) 
    &=& !let b=e$_1$ in!   \\
    && !<if b then e$_2$ else e$_3$, fun (x$_1$,$\ldots$,x$_n$,z) ->! \\
    && !Y(b*!$\directD{\rho}{\Gamma}{Y}$!(e$_2$)(x$_1$,$\ldots$,x$_n$,z)+!\\
    && \quad!(1-b)*!$\directD{\rho}{\Gamma}{Y}$!(e$_3$)(x$_1$,$\ldots$,x$_n$,z))>!
\end{tabular}

It is slightly better because both derivatives of !e2! and !e3! are put together, and this might unlock some optimizations, 
but there is still a non-linear usage of the continuation variable !Y!.

\smartpara{Solution}
If we know we want to compute the whole gradient of the expression, we can define the translation as follows:

\begin{tabular}{r c l}
    $\directD{\rho}{\Gamma}{Y}$(!if e$_1$ then e$_2$ else e$_3$!) 
    &=& !let b=e$_1$ in!   \\
    && !<if b then e$_2$ else e$_3$, fun (x$_1$,$\ldots$,x$_n$,z) ->! \\
    && !Y((x$_1$,$\ldots$,x$_n$)!$\widehat{+}$($\grad_\Gamma$!(e$_2$)*b!$\widehat{+}\grad_\Gamma$!(e$_3$)*(1-b))*z)>!\\
\end{tabular}

This time, there is a linear usage of the continuation variable !Y!. We note that adding conditionals does not break differentiability as long as there are no non-trivial primitives $\reals\to\BB$.
Non smooth functions such as the Rectified Linear Unit (ReLU) in machine learning which can be defined as $ReLU(x)\defeq max(0,x)$ 
requires a non smooth primitive such as $>0: \reals\to\BB$. In several AD systems such as TensorFlow, 
the condition is evaluated before differentiation is applied, and so conditionals are never directly differentiated.

\subsection{General arrays} % (fold)
\label{sub:lift_gen_arr}

We now show how to generalize our reverse-mode transformation to be defined on arrays over any ground type !G!.
That is, we need to adapt the reverse derivatives of !map2! and !reduce! when they have more general function arguments.

A ground type !G! is interpreted as an Euclidean space $A$. 
It is in particular a real vector space.
Similarly, a ground context !$\Gamma$=x$_1$:G$_1$,$\ldots$,x$_n$:G$_n$! is interpreted as $\bigoplus_{1\leq i\leq n}A_i$, where $A_i\defeq\seml$(!G$_i$!)$\semr$.
The denotation of the gradient of a term !$\Gamma \vdash$ e: G! at a point is then a matrix, more precisely an element of $(\bigoplus_{1\leq i\leq n}A_i)\otimes A$
where $\otimes$ is the tensor product of real vector spaces. This space is isomorphic to $\bigoplus_{1\leq i\leq n}A_i\otimes A$.

We can define $\otimes$ on the types of our language inductively by

\begin{tabular}{r c l}
    $\reals \otimes A$ & $\defeq$ & $A$ \\
    $(A_1 \times \ldots \times A_n)\otimes A$ & $\defeq$ & $(A_1\otimes A) \times \ldots \times (A_n \otimes A)$ \\
    $\Array{A_1}{n} \otimes A$ & $\defeq$ & $\Array{A_1\otimes A}{n}$
\end{tabular}

With this definition, we recover that the gradient of $\Gamma \vdash$!e: !$\reals$ is a tuple of type $A_1\times\ldots\times A_n$ as expected.
For !map2!, we need a generalization of $*:\reals\to\reals$. 
If !e$_1$: A!, then $\grad_{\{x\}}$!e$_1$: $A\otimes A$! and we need a new primitive $\widehat{*}:(A\otimes A) \times A \to A$.
If we represent $A\otimes A$ as a matrix, then $\widehat{*}$ is matrix-vector multiplication.
Similarly, for !A$_{3}$! in !reduce! we need a new primitive $\widetilde{*}:(A\otimes A)\times(A\otimes A) \to (A\otimes A)$.
$\widetilde{*}$ corresponds to matrix-matrix multiplication.
Then, using this notation, there are only minimal changes to the reverse derivatives of !map2! and !reduce!, as can be seen in Figure~\ref{fig:lift_gen_arr}.

\begin{figure}
\begin{center}
\begin{tabular}{|r c l|}
\hline
    $\directD{\rho}{\Gamma}{Y}$(!map2 (x,y.e$_1$: G) e$_2$ e$_3$!) &=&  
    !let A,Y$_1$ = !$\directD{\rho}{\Gamma}{Y}$!(e2) in! \\
    && !let B,Y$_2$ = !$\directD{\rho}{\Gamma,A}{Y1}$!(e$_3$) in! \\
    && !let C=!$\grad_{\Gamma}$!e$_1$ in!\\
    && !<map2 (x,y.e$_1$) A B, fun (x$_1$,$\ldots$,x$_n$,Z) -> !\\
    && !Y$_2$( (x$_1$,$\ldots$,x$_n$)!$\widehat{+}$!(C!$\widehat{*}$!(reduce + 0 Z)),!\\
    && \quad\quad! map2 (a,b.(!$\grad_{\{x\}}$!e$_1$)[a/x]!$\widehat{*}$!b) A Z,!\\
    && \quad\quad! map2 (a,b.(!$\grad_{\{y\}}$!e$_1$)[a/x]!$\widehat{*}$!b) B Z )>!\\
    $\directD{\rho}{\Gamma}{Y}$(!reduce (x,y.e$_{1}$) e$_{2}$ e$_{3}$!) &=&
    !let y$_{1}$,Y$_{1}$ = !$\directD{\rho}{\Gamma}{Y}$!(e$_{2}$) in! \\
    && !let A,Y$_{2}$ = !$\directD{\rho}{\Gamma,y_1}{Y_1}$!(e$_{3}$) in! \\
    && !let A$_{0}$ = shift1R (scanl (x,y.e$_{1}$) y$_{1}$ A) in! \\
    && !let A$_{1}$ = shift1L (map2! \\ 
    && !      (a,b.(!$\grad_{\{x\}}$!e$_{1}$)[a/x,b/y]) A$_{0}$ A) in! \\
    && !let A$_{2}$ = map2! \\
    && !      (a,b.(!$\grad_{\{y\}}$!)e$_{1}$[a/x,b/y]) A$_{0}$ A in! \\
    && !let A$_{3}$ = scanr $\widetilde{*}$ 1 A$_{1}$ in! \\
    && !<reduce (x,y.e$_{1}$) y$_{1}$ A, fun (x$_{1}$,$\ldots$,x$_n$,z) ->! \\
    && !Y$_{2}$(x$_{1}$,$\ldots$,x$_n$, map2 (x,y. x$\widehat{*}$(y$\widehat{*}$z)) A$_{2}$ A$_{3}$>! \\ \hline
\end{tabular}
\end{center}
\caption{The reverse-mode AD transformation of \codekw{map2} and \codekw{reduce} for general arrays.}
\label{fig:lift_gen_arr}
\end{figure}

Evidently, one can combine all the generalizations from the previous subsections.
Even though this transformation has the correct complexity, it is open for future research to find
even better representations to allow for more optimizations. 
In particular, representations looking like Einsum \cite{van2011numpy} could be of interest 
and has been recently studied in the context of AD \cite{laue2018computing,laue2020simple}.
More generally, there is growing interest in tensor calculus \cite{liao2019differentiable,bernstein2020differentiating}.

\subsection{Recipe for Adding More Constructs}
\label{sub:lift_recipe}
To add more constructs to our language, 
there is no need to go through UNF; most of the backend of our work can be used as a black box.
The non-smooth variables of the operator (typically booleans or integers) should be considered as external to the language, similarly to the way we treat the index $n$ of arrays.

\smartpara{Reverse AD}
If the operator has type $A\to B$, then one should provide its transpose Jacobian, a term $B\to A$. 
Such operators can often be unrolled to first-order programs. 

\smartpara{Correctness}
To check correctness of the given Jacobian, it suffices to 
check that the semantics of the transpose Jacobian matches the Jacobian of the unrolled program. 

\smartpara{Complexity}
To ensure the complexity guarantee of the whole transformation, one needs to check that there a linear usage of the continuation variable $Y$ and that the cost of the 
proposed Jacobian is at most $k$ times the complexity of the operator. 
