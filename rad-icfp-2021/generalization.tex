\section{Beyond the Source language}
\label{sec:generalization}

\subsection{Lifting the restriction on reduce}
\label{sub:Lifting the restriction on reduce}

Assume we allow $\Gamma$, !x!: $\reals$, !y!: $\reals$ $\vdash$ !e!: $\reals$ to be the function argument in
!reduce (x,y.e) v A!. We need to add a term depending on $\grad_{\{xi\}}e$ to !yi! in the continuation.
Writing the derivative becomes quite cumbersome, the notation is more compact using arrays of tuples. 
Similarly to he monoid $\widehat{+},0_\Gamma$ defined in Section~\ref{sub:Macro for pure reverse mode transformation},
we use $\widehat{\times},1_\Gamma$ for obvious extension of the monoid $(\reals,\times,1)$.

We can now compute the reverse-mode transformation as follows.
\begin{center}
\begin{tabular}{r c l}
$\directD{\rho}{\Gamma}{Y}$($\Gamma\vdash $ !reduce (x,y.e1) e2 e3!) 
&=& $\ldots$ as before up until !A3! \\
&& !let B0 = map2 (a,b ->(!$\grad_\Gamma$!e)[a/x,b/y] A0 A) in!\\
&& !let B1 = scanr !$1_\Gamma$ $\widehat{\times}$ !B0 in! \\
&& !let B2 = reduce !$\widehat{+}$ $0_\Gamma$! B1 in! \\
&& !<reduce (x,y.e1) y1 A, fun (x1,$\ldots$,xn,z) ->! \\
&& !Y((x$_1$,$\ldots$,x$_n$)!$\widehat{+}$!B2,0,map2 (a,b. a*b*z) A2 A3)!
\end{tabular}
\end{center}

\subsection{More array operators}
\label{sub:Adding more array operators}

There are two main differences with fold left !foldl! compared to !reduce!. 
First, in !foldl (x,y.e) v A! the starting accumulation element !v! is not a unit for !(x,y.e)!,
so it will have non-trivial derivatives in general and we need to account for that.
Second, we will need a more general !scanl! which allows !(x,y.e)! as a function argument. 
In other words, we need the general scan left computing the intermediate values of a fold left.
This should be a different primitive but we will still call it !scanl! in this section.

Finally, the former point implies we need a few more array manipulations.
These can be elegantly dealt with by changing the semantics of !scanl! and !scanr!. 
Let's assume that they now return a pair of an array of size $n$ 
of the intermediate computations and the final result.
The reverse derivative of fold left is then

\begin{center}
\begin{tabular}{r c l}
    $\directD{\rho}{\Gamma}{Y}$($\Gamma\vdash $ !foldl (x,y.e$_1$) e$_2$ e$_3$!) &=&
            !let v,Y$_1$ = $\directD{\rho}{\Gamma}{Y}$(e$_2$) in! \\
            && !let A,Y$_2$ = $\directD{\rho}{\Gamma,v}{Y_1}$(e$_3$) in! \\
            && !let A$_0$,r$_1$ = (scanl (x,y.e$_1$) v A) in! \\
            && !let A$_1$ = map2 (a,b.(!$\grad_{\{x\}}$!e$_1$)[a/x,b/y]) A$_0$ A! \\
            && !let A$_2$ = map2 (a,b.(!$\grad_{\{y\}}$!)e$_1$[a/x,b/y]) A$_0$ A! \\
            && !let r$_2$, A$_3$ = scanr * 1 A$_1$! \\
            && !<r$_1$, fun (x$_1$,$\ldots$,x$_n$,z) ->! \\
            && !let y$_1$,B = r$_2$*z ,map2 (x,y. x*y*z) A$_2$ A$_3$ in! \\
            && !Y$_2$(x$_1$,$\ldots$,x$_n$,y$_1$,B)>! \\
\end{tabular}
\end{center}

Another useful example is !map!, which is very similar to !map2!.

\begin{center}
    \begin{tabular}{r c l}
        $\directD{\rho}{\Gamma}{Y}$(!map (x.e$_{1}$) e$_{2}$!) &=&  
            !let A,Y$_{1}$ = $\directD{\rho}{\Gamma}{Y}$(e$_{2}$) in! \\
            && !let G=$\grad_{\Gamma}$ e$_{1}$ in!\\
            && !<map (x.e$_{1}$) A, fun (x$_{1}$,$\ldots$,x$_n$,Z) -> !\\
            && !Y$_{1}$( (x$_{1}$,$\ldots$,x$_n$)$\widehat{+}$(G*(reduce + 0 Z)),!\\
            && \quad\quad !map2 (a,b.($\grad_{\{x\}}$e$_{1}$)[a/x]*b) A Z )>!\\
    \end{tabular}
\end{center}

\subsection{Non-smooth scalar operators} % (fold)
\label{sub:Adding more scalar operators}

We assumed the unary and binary operators were denoted by smooth functions $\RR^n\to\RR$. 
There is no additional difficulty in considering operators which are partial functions 
like division or operators which are not smooth at a point like square root.

These functions are then given intentional derivatives which provide valid derivatives 
on the domain of definition and differentiability of the operator. 
These functions are well known to be the bete noire of AD \cite{griewank2008evaluating} 
and we do not provide novel solutions to these.  
Several recent work have shown how to give semantics to such operators in the context of AD \cite{vakar2020denotational,mazza2021automatic,sherman2021,lee2020correctness}.

\subsection{Conditionals} % (fold)
\label{sub:Adding conditionals}

Adding conditionals to the source language can be done easily, for instance via defining

\begin{tabular}{r c l}
$\directD{\rho}{\Gamma}{Y}(\Gamma \vdash$ !if e1 then e2 else e3!) &=& !if e1 then !$\directD{\rho}{\Gamma}{Y}$!(e2) else !$\directD{\rho}{\Gamma}{Y}$!(e3)! 
\end{tabular}

The problem is that this could break the complexity of reverse mode because of the non-linear usage of $Y$, and makes everything harder to optimize.

A slightly better option would be to define 

\begin{tabular}{r c l}
    $\directD{\rho}{\Gamma}{Y}(\Gamma \vdash$ !if e1 then e2 else e3!) 
    &=& !let b=e1 in!   \\
    && !<if b then e2 else e3, fun (x1,$\ldots$,xn,z) ->! \\
    && !Y(b*!$\directD{\rho}{\Gamma}{Y}$!(e2)(x1,$\ldots$,xn,z)+!\\
    && \quad!(1-b)*!$\directD{\rho}{\Gamma}{Y}$!(e3)(x1,$\ldots$,xn,z))>!
\end{tabular}

It is slightly better because both derivatives of !e2! and !e3! are put together, and this might unlock some optimizations, 
but there is still a non-linear usage of the continuation variable !Y!.
One solution, if we know we want to compute the whole gradient of the expression, is to define instead

\begin{tabular}{r c l}
    $\directD{\rho}{\Gamma}{Y}(\Gamma \vdash$ !if e1 then e2 else e3!) 
    &=& !let b=e1 in!   \\
    && !<if b then e2 else e3, fun (x1,$\ldots$,xn,z) ->! \\
    && !Y((x1,$\ldots$,xn)!$\widehat{+}$($\grad_\Gamma$!(e2)*b!$\widehat{+}\grad_\Gamma$!(e3)*(1-b))*z)>!\\
\end{tabular}

This time, there is a linear usage of the continuation variable !Y! (and of the variables !x1,$\ldots$xn! too).

We note that adding conditionals does not break differentiability as long as there are no non-trivial primitives $\reals\to\BB$.
Non smooth functions such as the Rectified Linear Unit (ReLU) in machine learning which can be defined as $reLU(x)\defeq max(0,x)$ 
requires a non smooth primitive such as $>0: \reals\to\BB$. In several AD systems such as TensorFlow, 
the condition is evaluated before differentiation is applied, and so conditionals are never directly differentiated.

\subsection{General arrays} % (fold)
\label{sub:Adding general arrays}

We now show how to generalize our reverse-mode transformation to be defined on arrays over any ground type $G$.
That is, we need to adapt the reverse derivatives of !map2! and !reduce! when they have more general function arguments.

A ground type $G$ is interpreted as an Euclidean space $A$. 
It is in particular a real vector space.
Similarly, a ground context $\Gamma=x1:G1,\ldots,xn:Gn$ is interpreted as $\bigoplus_{1\leq i\leq n}Ai$, where $Ai\defeq\sem{Gi}$.
The denotation of the gradient of a term $\Gamma \vdash$!e: !$G$ at a point is then a matrix, more precisely an element of $(\bigoplus_{1\leq i\leq n}Ai)\otimes A$
where $\otimes$ is the tensor product of real vector spaces. This space is isomorphic to $\bigoplus_{1\leq i\leq n}Ai\otimes A$.

We can define $\otimes$ on the types of our language inductively by

\begin{tabular}{r c l}
    $\reals \otimes A$ & $\defeq$ & $A$ \\
    $(A1 \times \ldots \times An)\otimes A$ & $\defeq$ & $(A1\otimes A) \times \ldots \times (An \otimes A)$ \\
    $\Array{A1}{n} \otimes A$ & $\defeq$ & $\Array{A1\otimes A}{n}$
\end{tabular}

With this definition, we recover that the gradient of $\Gamma \vdash$!e: !$\reals$ is a tuple of type $A1\times\ldots\times An$ as expected.
For !map2!, we need a generalization of $*:\reals\to\reals$. 
If !e1: A!, then $\grad_{\{x\}}$!e1: $A\otimes A$! and we need a new primitive $\widehat{*}:(A\otimes A) \times A \to A$.
If we represent $A\otimes A$ as a matrix, then $\widehat{*}$ is matrix-vector multiplication.
Similarly, for !A$_{3}$! in !reduce! we need a new primitive $\widetilde{*}:(A\otimes A)\times(A\otimes A) \to (A\otimes A)$.
$\widetilde{*}$ corresponds to matrix-matrix multiplication.
Then, using this notation, there are only minimal changes to the reverse derivatives of !map2! and !reduce!.

\begin{center}
\begin{tabular}{r c l}
    $\directD{\rho}{\Gamma}{Y}$($\Gamma\vdash $ !map2 (x,y.e1: G) e2 e3!) &=&  
    !let A,Y1 = !$\directD{\rho}{\Gamma}{Y}$!(e2) in! \\
    && !let B,Y2 = !$\directD{\rho}{\Gamma,A}{Y1}$!(e3) in! \\
    && !let C=!$\grad_{\Gamma}$!e1 in!\\
    && !<map2 (x,y.e1) A B, fun (x1,$\ldots$,xn,Z) -> !\\
    && !Y2( (x1,$\ldots$,xn)!$\widehat{+}$!(C!$\widehat{*}$!(reduce + 0 Z)),!\\
    && \quad\quad! map2 (a,b.(!$\grad_{\{x\}}$!e1)[a/x]!$\widehat{*}$!b) A Z,!\\
    && \quad\quad! map2 (a,b.(!$\grad_{\{y\}}$!e1)[a/x]!$\widehat{*}$!b) B Z )>!\\
    \end{tabular}
    \end{center}

\begin{center}
\begin{tabular}{r c l}
    $\directD{\rho}{\Gamma}{Y}$(!reduce (x,y.e$_{1}$) e$_{2}$ e$_{3}$!) &=&
    !let y$_{1}$,Y$_{1}$ = !$\directD{\rho}{\Gamma}{Y}$!(e$_{2}$) in! \\
    && !let A,Y$_{2}$ = !$\directD{\rho}{\Gamma,y_1}{Y_1}$!(e$_{3}$) in! \\
    && !let A$_{0}$ = shift1R (scanl (x,y.e$_{1}$) y$_{1}$ A) in! \\
    && !let A$_{1}$ = shift1L (map2 (a,b.(!$\grad_{\{x\}}$!e$_{1}$)[a/x,b/y]) A$_{0}$ A) in! \\
    && !let A$_{2}$ = map2 (a,b.(!$\grad_{\{y\}}$!)e$_{1}$[a/x,b/y]) A$_{0}$ A in! \\
    && !let A$_{3}$ = scanr $\widetilde{*}$ 1 A$_{1}$ in! \\
    && !<reduce (x,y.e$_{1}$) y$_{1}$ A, fun (x$_{1}$,$\ldots$,x$_n$,z) ->! \\
    && !Y$_{2}$(x$_{1}$,$\ldots$,x$_n$, map2 (x,y. x$\widehat{*}$(y$\widehat{*}$z)) A$_{2}$ A$_{3}$>! \\
\end{tabular}
\end{center}

Evidently, one can combine all the generalizations from the previous subsections.
Even though this transformation has the correct complexity, it is open for future research to find
even better representations to allow for more optimizations. 
In particular, representations looking like Einsum \cite{van2011numpy} could be of interest 
and has been recently studied in the context of AD \cite{laue2018computing,laue2020simple}.
More generally, there is growing interest in tensor calculus \cite{liao2019differentiable,bernstein2020differentiating}.
