\section{Conclusion}
\label{sec:conclusion}

\subsection{Summary} % (fold)
\label{sub:summary}

We have introduced a transformation on programs to compute provably efficient (\ref{sec:complexity}) gradients via reverse derivatives in a purely functional way (\ref{sub:Macro for pure reverse mode transformation})
on a simple yet expressive language with functions on arrays (\ref{sub:sourcelang}), combined with standard functional optimisations (\ref{fig:optim}).  
We introduced a novel intermediate representation, unary form (\ref{sec:unf}) 
to decompose our transformation into simpler and more insightful transformations.
We showed (\ref{sec:generalisation}) how to lift some of the restrictions that
we introduced on arrays and how to extend our approach.

\subsection{Related Work} % (fold)
\label{sub:related_work}

Relatively recent work in AD (2008-2020):
\cite{mak2020differential,elliotthigher,vytiniotis2019differentiable,innes2018don,baydin2017automatic,huot2020correctness,gallagher-sdg,manzyuk2012confusion,wang2018demystifying,beck1994if,wang2018backpropagation,betancourt2018geometric,elliott2018simple,carpenter2015stan,paszke2017automatic,shaikhha2019efficient,innes2019zygote,griewank2008evaluating,kucukelbir2017automatic,brunel2019backpropagation,barthe2020versatility,abadi2019simple,cockett2019reverse,van2018automatic,hascoet2013tapenade,abadi2016tensorflow,pearlmutter2008reverse,bergstra2010theano,fong2019backprop,ehrhard2003differential,agrawal2019tensorflow,bettencourt2019taylor,cruttwell2017cartesian,manzyuk2012simply,laue2018computing,bernstein2020differentiating}

Some work in ML where some kind of iteration/recursion is used.
Dynamic neural networks: \cite{jin2017manipulability,wu2016deep}
Recursive NN: \cite{socher2011parsing,biancofiore2017recursive}
Reccurent NN: \cite{bahdanau2014neural,luong2015effective}
Tree LSTM: \cite{tai2015improved,chen2016enhanced}
Dynamic Recursive NN: \cite{guo2019dynamic}. It's recent with very few citations, but their point is close to ours. 
Top-down Tree LSTM: \cite{zhang2015top}
Recursion in DNN: \cite{jeong2018improving}

\noindent \textbf{Automatic Differentiation.} 

\noindent \textbf{Array Languages and Fusion.}

\noindent \textbf{Numerical DSLs.} 

\noindent \textbf{Correctness of AD in functional languages.}

\noindent \textbf{Comparison to other recent papers.}

\subsection{Discussion and future work} % (fold)
\label{sub:discussion_and_future_work}

\noindent \textbf{More dynamic language}
%TF is losing ground to Pytorch, Swift tries to renew with Pytorch-like dynamic features. 

\noindent \textbf{More optimisations.}
%maybe?

\noindent \textbf{Limitations.}
\begin{itemize}
    \item memory access for cost function 
    \item parallelism
    \item dedicated tensors for ML, and BLaS considerations
    \item implementation and benchmark
    \item conditionals
    \item recursion
    \item general HO function
    \item more 2nd order functions, though can be derived by hand once and then it's ok
    \item more dynamic arrays
    \item more dynamic control flow
    \item all store vs all recompute reverse-mode, and advanced checkpointing
    \item numerical accuracy
    \item fancier types and datatypes (eg dependent, adt) 
    \item iterated ad and higher derivatives
    \item use in Probabilistic programming
    \item non smooth and partial functions, although by simply following the ref book it's as usual in AD
    \item sparsity and data manipulation
    \item other optimisations like implicit function theorem
\end{itemize}

\MH{useful links (Swift is close to our project, and the doc is very readable):}
\begin{verbatim}https://docs.google.com/document/d/1_BirmTqdotglwNTOcYAW-ib6mx_jl-gH9Dbg4WmHZh0/edit# \end{verbatim}
\begin{verbatim}
	https://github.com/apple/swift/blob/master/docs/DifferentiableProgramming.md#approaches-to-automatic-differentiation
\end{verbatim}

\MH{understand related work sentence from Diff. Curry paper: The idea of using closures as back-propagators is receiving recent attention. For example Julia Zy- gote [15] and Swift AD adopts this design. Other recent work follows similar ideas [27, 26] but is using meta-programming as an implementation technique.}

\clearpage