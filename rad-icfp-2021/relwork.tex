\section{Discussion and future work} % (fold)
\label{sec:discussion_and_future_work}

\noindent \textbf{More dynamic language}
JAX \cite{bradbury2020jax,frostig2018compiling} is a modern framework that provides state-of-the-art automatic differentiation. 
One feature that makes it popular is that it uses JIT compilation, 
which seems to be the best avenue to help best ML practitioners as it allows them 
to quickly experiment but also benefit from the full power of static optimizations.
We are interested in seeing how our work could sit in a JIT compiler. 

\noindent \textbf{More optimizations.}
As pointed in \ref{sub:Adding general arrays} it would be interesting to have a more principled representation 
of tensors which could lead to more optimizations. 
There is always a tension between dynamicity and the possibility for static optimizations 
and we believe our language offers a good tradeoff, but it will be interesting to tailor it 
further for specific applications.

\noindent \textbf{Cost function and memory accesses}
Our cost model \ref{sub:costModel} is simple but has limits. 
In particular, it does not capture well cost in term of
parallel computing, and memory management (cache optimization is one of best way to optimize such linear algebra intensive computations).
For instance, in the earlier days of automatic differentiation, 
advanced checkpointing techniques were used to avoid storing all the intermediate values just to save memory 
and try avoiding saturation of RAM. 
These problems are harder to analyze and, eventually, it seems to come down to a lot of experimenting and benchmarking.

\noindent \textbf{Parallelism and GPU}
We tried to locally preserve parallelism. Having a pure transformation should help with found parallelism. 
It would be interesting to implement on transformations and see how they perform on GPU.
It is well-known that the addition of references provoked by the standard reverse-mode transformation
restricts a lot of optimizations and it is not hard to find examples in C where the compiler does not detect simple optimizations 
because of the references introduced by reverse-mode.

% \noindent \textbf{implementation, BLaS, benchmark}

\noindent \textbf{Higher-order functions}
Matthijs'work, use of defunctionalization, question efficiency and usefulness, 
interested in functions like root of function or minimal of a function. Implicit function theorem
Some recent work \cite{vakar2020reverse,sherman2021} present reverse-mode in a higher-order language.
\cite{vakar2020reverse} uses categorical semantics to show correctness of the reverse-mode transformation 
and \cite{sherman2021} uses sophisticated higher-order primitives such as root finding, max, argmax or integral. 
Their work focuses on computable reals which is hard to compare in terms of efficiency 
with our more standard approach of AD.

\noindent \textbf{More array primitives}
filter, flatten, gather, scanl, ...
can be derived by hand once and then it's ok

\noindent \textbf{Recursion}
TODO

\noindent \textbf{Higher derivatives}
TODO

\noindent \textbf{Sparse data and matrix representation}
TODO

\noindent \textbf{Further applications}
Reverse-mode differentiation is notoriously difficult to grasp, implement, and optimize.
We hope this work could help develop a new perspective on pure reverse-mode automatic differentiation.
Other applications include the use of fancier datatypes e.g. dependent types, 
in probabilistic programming, e.g. for HMC or Variational Inference. 
It is also of interest to see how to extend this work with other techniques which 
have had renewed interest such as implicit differentiation \cite{blondel2021efficient,lorraine2020optimizing}.

% 	https://github.com/apple/swift/blob/master/docs/DifferentiableProgramming.md#approaches-to-automatic-differentiation

TODO: understand related work sentence from Diff. Curry paper: The idea of using closures as back-propagators is receiving recent attention. 
    For example Julia Zygote \cite{innes2019zygote} and Swift AD \cite{wei2018first} chose this design. 
    Other recent work follows similar ideas [27, 26] but is using meta-programming as an implementation technique.

TODO: should talk a bit about the recent paper: reverse AD in Dex \cite{paszke2021getting}

\section{Related Work} % (fold)
\label{sec:related_work}

\noindent \textbf{Usage of iteration mechanics}
A immense effort in machine learning for the past decade has been in finding
good architectures, to limit computational costs, 
avoid vanishing and exploding gradients, 
and have better building blocks of large complicated systems than traditional layers of a neural network.
Different approaches 
(Dynamic neural networks \cite{jin2017manipulability,wu2016deep}, 
Recursive NN \cite{socher2011parsing,biancofiore2017recursive}, 
Reccurent NN \cite{bahdanau2014neural,luong2015effective}, 
Tree LSTM \cite{tai2015improved,chen2016enhanced}, 
Dynamic Recursive NN \cite{guo2019dynamic}, 
Top-down Tree LSTM \cite{zhang2015top}, 
Recursion in DNN \cite{jeong2018improving}) 
have found that recursive data structures such as trees are good candidates.
We have emphasized here on differentiating fold left on arrays for efficiency, 
but one should be able to adapt this on any algebraic data type. 
It will be interesting to see if and how we recover efficient backpropagation on the proposed architectures 
which is usually derived by hand and one main goal of these papers.

\noindent \textbf{Correctness of AD in functional languages.}
Several recent work \cite{huot2020correctness,vakar2020reverse,vakar2020denotational,brunel2019backpropagation,barthe2020versatility,mazza2021automatic,ee2020correctness} have focused on correctness of AD in a purely functional setting, 
often leaving efficiency on the side, especially for reverse-mode differentiation. 
We see our work as a complement and a first bridge between these works 
and more practical considerations of efficiency and implementations, 
which often require a lot more care than is acknowledegd in more theoretical works.

An exception is \cite{abadi-plotkin2020} which gives a reverse-mode transformation that is proved correct and with the right complexity.
This work is purely first-order, does not have arrays and uses a non standard semantics. 
Our work is more canonical in the sense that it can more directly be implemented in a standard call-by-value language.
They do allow conditionals and recursion but their semantics suggest that they don't have a canonical transformation on these constructs. 
Instead, they follow the principle of unrolling for recursive calls and evaluating the conditional before differentiating, 
which requires to recompute the whole gradient when taken at new values that can change the value of the conditional. 
This is a well-known limit in TensorFlow \cite{abadi2016tensorflow} which TeaserFlow EagerMode \cite{agrawal2019tensorflow} tries to bypass. 

\noindent \textbf{Array Languages and AD}

Given the enormous computation needs for state-of-the-art large scale machine learning applications, 
which require extremely efficient tensor computations and automatic differentiation for backpropagation, 
combining array languages and automatic differentiation (tensor calculus) in the best 
fitted intermediate representation for optimizations is of key interest and active research.
\cite{bernstein2020differentiating} 
TODO
\cite{laue2018computing,laue2020simple}

\noindent \textbf{Comparison to other recent papers.}

TODO
\cite{ee2020correctness}: non diff function
\cite{sherman2021}: non diff HO functions
\cite{pearlmutter2008reverse}: based on this
\cite{elliott2018simple}: bit similar because using CT as useful language, but we take impl and opti more seriously.
\cite{shaikhha2019efficient}: similar approach but here more complicated for reverse mode, would be interesting to investigate more loop fusions here.

Relatively recent work in AD (2008-2020):
\cite{mak2020differential,elliotthigher,vytiniotis2019differentiable,innes2018don,baydin2017automatic,huot2020correctness,gallagher-sdg,manzyuk2012confusion,wang2018demystifying,beck1994if,wang2018backpropagation,betancourt2018geometric,elliott2018simple,carpenter2015stan,paszke2017automatic,shaikhha2019efficient,innes2019zygote,griewank2008evaluating,kucukelbir2017automatic,brunel2019backpropagation,barthe2020versatility,abadi2019simple,cockett2019reverse,van2018automatic,hascoet2013tapenade,abadi2016tensorflow,pearlmutter2008reverse,bergstra2010theano,fong2019backprop,ehrhard2003differential,agrawal2019tensorflow,bettencourt2019taylor,cruttwell2017cartesian,manzyuk2012simply,laue2018computing,bernstein2020differentiating}

Good recent surveys: \cite{van2018automatic,baydin2017automatic}

\section{Conclusion}
\label{sec:conclusion}

We have introduced a transformation on programs to compute provably efficient (\ref{sec:complexity}) 
gradients via reverse derivatives in a purely functional way (\ref{sub:Macro for pure reverse mode transformation})
on a simple yet expressive language with functions on arrays (\ref{sub:sourcelang}), 
combined with standard functional optimizations (\ref{fig:optim}).  
We introduced a novel intermediate representation, unary form (\ref{sec:unf}) 
to decompose our transformation into simpler transformations.
We gave denotational semantics to our languages and our transformations (\ref{sec:correctness}) 
and showed correctness of the reverse-mode transformation (\ref{sub:Correctness theorem}).
We showed (\ref{sec:generalization}) how to lift some of the restrictions that
we introduced on arrays and how to extend our approach to a richer language with more primitive operations and conditionals.
\clearpage