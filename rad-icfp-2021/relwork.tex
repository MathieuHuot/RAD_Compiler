\section{Discussion and future work} % (fold)
\label{sec:discussion_and_future_work}

\noindent \textbf{Design space}
First, there is a tradeoff to reach between a general expressive language and a domain specific one. The latter usually has more static information and a specific representation that lends itself to better optimizations.
Then, many optimizations performed on AD implementations consist in hand-crafted derivatives for useful operations like matrix-matrix multiplication, dot-product, etc.
They don't seem to arise from theoretical justifications, are error prone, and can hardly fit with more general optimizations.
This makes these systems harder to prove correct. The problem is thus to ensure provable correctness and pureness, while not compromising on efficiency. In addition, we would like something easily provably efficient. 
A real-world implementation based on our work would of course optimize further, potentially using hand-crafted operations as well, but it would be based on solid grounds.

% \noindent \textbf{More dynamic language}
% JAX \cite{bradbury2020jax,frostig2018compiling} is a modern framework that provides state-of-the-art automatic differentiation. 
% One feature that makes it popular is that it uses JIT compilation, 
% which seems to be the best avenue to help best ML practitioners as it allows them 
% to quickly experiment but also benefit from the full power of static optimizations.
% We are interested in seeing how our work could sit in a JIT compiler. 

% \noindent \textbf{More optimizations.}
% As pointed in \ref{sub:Adding general arrays} it would be interesting to have a more principled representation 
% of tensors which could lead to more optimizations. 
% There is always a tension between dynamicity and the possibility for static optimizations 
% and we believe our language offers a good tradeoff, but it will be interesting to tailor it 
% further for specific applications.

% \noindent \textbf{Cost function and memory accesses}
% Our cost model \ref{sub:costModel} is simple but has limits. 
% In particular, it does not capture well cost in term of
% parallel computing, and memory management (cache optimization is one of best way to optimize such linear algebra intensive computations).
% For instance, in the earlier days of automatic differentiation, 
% advanced checkpointing techniques were used to avoid storing all the intermediate values just to save memory 
% and try avoiding saturation of RAM. 
% These problems are harder to analyze and, eventually, it seems to come down to a lot of experimenting and benchmarking.

% \noindent \textbf{Parallelism and GPU}
% We tried to locally preserve parallelism. Having a pure transformation should help with found parallelism. 
% It would be interesting to implement on transformations and see how they perform on GPU.
% It is well-known that the addition of references provoked by the standard reverse-mode transformation
% restricts a lot of optimizations and it is not hard to find examples in C where the compiler does not detect simple optimizations 
% because of the references introduced by reverse-mode.

% \noindent \textbf{implementation, BLaS, benchmark}

\noindent \textbf{Higher-order functions}
Some recent work \cite{vakar2020reverse,sherman2021} present reverse-mode in a higher-order language.
\cite{vakar2020reverse} uses categorical semantics to show correctness of the reverse-mode transformation 
and \cite{sherman2021} uses sophisticated higher-order primitives such as root finding, max, argmax or integral. 
Their work focuses on computable reals which is hard to compare in terms of efficiency with our more standard approach of AD.
It is however quite difficult to prove a complexity result in the higher-order setting. 
In addition, standard techniques of defunctionalization struggle when higher-order is combined with recursion.
Our reverse-mode transformation does not support recursion, 
so it is currently always possible to partially evaluate a higher-order program to our Source language, seen as intermediate representation,
then perform our reverse-mode transformation.

\noindent \textbf{Array primitives}
We focused on giving reverse derivatives for a small set of array primitives. 
Other common primitives include filter, flatten, gather. 
These functions can easily be added to our Source language once we provide a reverse derivative for them.
This is reminiscent of hand-crafted derivatives present in large AD frameworks (usually hundreds), 
and these can already be added in added in our language. 
As shown in \ref{sub:lift_recipe}, one mostly only needs to make sure that the provided transpose Jacobian is correct.

% \noindent \textbf{Recursion}
% It would be an interesting extension to our work to see if one can define efficient pure reverse derivatives on general recursion.

% \noindent \textbf{Higher derivatives}
% There are recent approaches to computing higher derivatives using jets \cite{betancourt2018geometric} which generalise forward and reverse mode.
% A more standard approach is to iterate forward or reverse-mode. 
% Forward mode is standard and can easily be defined on our Target language. 
% As our optimization procedure removes lambdas, to iterate reverse-mode we would only need to compute reverse derivatives for the new primitives of Target, that is !scanl, scanr, shift1L, shift1R!.

% \noindent \textbf{Further applications}
% Reverse-mode differentiation is notoriously difficult to grasp, implement, and optimize.
% We hope this work could help develop a new perspective on pure reverse-mode automatic differentiation.
% Other applications include the use of fancier datatypes e.g. dependent types, 
% in probabilistic programming, e.g. for HMC or Variational Inference. 
% It is also of interest to see how to extend this work with other techniques which 
% have had renewed interest such as implicit differentiation \cite{blondel2021efficient,lorraine2020optimizing}.

% 	https://github.com/apple/swift/blob/master/docs/DifferentiableProgramming.md#approaches-to-automatic-differentiation

\section{Related Work} % (fold)
\label{sec:related_work}

\noindent \textbf{Correctness of AD in functional languages.}
Several recent work \cite{huot2020correctness,vakar2020reverse,vakar2020denotational,brunel2019backpropagation,barthe2020versatility,mazza2021automatic,lee2020correctness} have focused on correctness of AD in a purely functional setting, 
often leaving efficiency on the side, especially for reverse-mode differentiation. 
We see our work as a complement and a first bridge between these works 
and more practical considerations of efficiency and implementations, 
which often require a lot more care than is acknowledged in more theoretical works.

% An exception is \cite{abadi-plotkin2020}.
% This work is purely first-order, does not have arrays and uses a non-standard semantics. 
% Our work is more canonical in the sense that it can more directly be implemented in a standard call-by-value language.
% They do allow conditionals and recursion but their semantics suggest that they don't have a canonical transformation on these constructs. 
% Instead, they follow the principle of unrolling the recursive calls and evaluating the conditional before differentiating, 
% which requires to recompute the whole gradient when taken at new values that can change the value of the conditional. 
% This is a known limitation in TensorFlow \cite{abadi2016tensorflow}. %which TeaserFlow EagerMode \cite{agrawal2019tensorflow} tries to bypass. 

\noindent \textbf{Usage of iteration mechanics}
An immense effort in machine learning for the past decade has been in finding
good architectures, to limit computational costs, 
avoid vanishing and exploding gradients, 
and have better building blocks for large complicated systems than traditional layers of a neural network.
Different approaches 
(Dynamic neural networks \cite{jin2017manipulability,wu2016deep}, 
Recursive NN \cite{socher2011parsing,biancofiore2017recursive}, 
Reccurent NN \cite{bahdanau2014neural,luong2015effective}, 
Tree LSTM \cite{tai2015improved,chen2016enhanced}, 
Dynamic Recursive NN \cite{guo2019dynamic}, 
Top-down Tree LSTM \cite{zhang2015top}, 
Recursion in DNN \cite{jeong2018improving}) 
have found that recursive data structures such as trees are good candidates.
We have emphasized here on differentiating fold left on arrays for efficiency, 
but one should be able to adapt this on any algebraic data type. 
It will be interesting to see if and how we recover efficient backpropagation on the proposed architectures 
which is usually derived by hand and one main goal of these papers.

\noindent \textbf{Array Languages and AD}

Given the enormous computation needs for state-of-the-art large scale machine learning applications, 
which require extremely efficient tensor computations and automatic differentiation for backpropagation, 
combining array languages and automatic differentiation (tensor calculus) in the best-fitted 
intermediate representation for optimizations is of key interest and active research. 
See for instance the recent work by \cite{bernstein2020differentiating,laue2018computing,laue2020simple}.
Advanced array programming is considered an orthogonal problem to AD and we focused our work on the latter.

\noindent \textbf{Comparison to other recent papers.}

Table~\ref{tbl:relwork} does not reflect all the aspects of AD. For instance \cite{lee2020correctness} studies in more detail non differentiability, 
and \cite{sherman2021} the differentiability of highly-non trivial higher-order functions. 
Our work is somewhat close in spirit to the idea of \cite{elliott2018simple} of compiling to categories.
The idea of using closures as back-propagators is receiving recent attention, as is highlighted in \cite{vytiniotis2019differentiable},
these ideas are used in Julia Zygote \cite{innes2019zygote}, Swift AD \cite{wei2018first}, and recently in \cite{paszke2021getting}.
These seem closer to using control mechanims than having purely functional reverse-derivatives as far as we could tell.
Other aspects of AD still are discussed in recent surveys \cite{van2018automatic,baydin2017automatic}.

\section{Conclusion}
\label{sec:conclusion}

We have introduced a transformation on programs to compute provably efficient (\S\ref{sec:complexity}) 
gradients via reverse derivatives in a purely functional way (\S\ref{sub:Macro for pure reverse mode transformation})
on a simple yet expressive language with functions on arrays (\S\ref{sub:sourcelang}), 
combined with standard functional optimizations (\S\ref{fig:optim}).  
We introduced a novel intermediate representation, Unary Normal Form (\S\ref{sec:unf}) 
to decompose our transformation into simpler transformations.
We gave denotational semantics to our languages and our transformations (\S\ref{sec:correctness}), 
and showed correctness of the reverse-mode translation (\S\ref{sub:Correctness theorem}).
We showed (\S\ref{sec:generalization}) how to lift the restrictions that
we introduced on arrays and how to extend our approach with other constructs such as conditionals.
\clearpage