\section{Conclusion}
\label{sec:conclusion}

\subsection{Summary} % (fold)
\label{sub:summary}

We have introduced a transformation on programs to compute provably efficient (\ref{sec:complexity}) 
gradients via reverse derivatives in a purely functional way (\ref{sub:Macro for pure reverse mode transformation})
on a simple yet expressive language with functions on arrays (\ref{sub:sourcelang}), 
combined with standard functional optimisations (\ref{fig:optim}).  
We introduced a novel intermediate representation, unary form (\ref{sec:unf}) 
to decompose our transformation into simpler transformations.
We gave denotational semantics to our languages and our transformations (\ref{sec:correctness}) 
and showed correctness of the reverse-mode transformation (\ref{sub:Correctness theorem}).
We showed (\ref{sec:generalisation}) how to lift some of the restrictions that
we introduced on arrays and how to extend our approach to a richer language with more primitive operations and conditionals.

\subsection{Related Work} % (fold)
\label{sub:related_work}

Relatively recent work in AD (2008-2020):
\cite{mak2020differential,elliotthigher,vytiniotis2019differentiable,innes2018don,baydin2017automatic,huot2020correctness,gallagher-sdg,manzyuk2012confusion,wang2018demystifying,beck1994if,wang2018backpropagation,betancourt2018geometric,elliott2018simple,carpenter2015stan,paszke2017automatic,shaikhha2019efficient,innes2019zygote,griewank2008evaluating,kucukelbir2017automatic,brunel2019backpropagation,barthe2020versatility,abadi2019simple,cockett2019reverse,van2018automatic,hascoet2013tapenade,abadi2016tensorflow,pearlmutter2008reverse,bergstra2010theano,fong2019backprop,ehrhard2003differential,agrawal2019tensorflow,bettencourt2019taylor,cruttwell2017cartesian,manzyuk2012simply,laue2018computing,bernstein2020differentiating}

Some work in ML where some kind of iteration/recursion is used.
Dynamic neural networks: \cite{jin2017manipulability,wu2016deep}
Recursive NN: \cite{socher2011parsing,biancofiore2017recursive}
Reccurent NN: \cite{bahdanau2014neural,luong2015effective}
Tree LSTM: \cite{tai2015improved,chen2016enhanced}
Dynamic Recursive NN: \cite{guo2019dynamic}. It's recent with very few citations, but their point is close to ours. 
Top-down Tree LSTM: \cite{zhang2015top}
Recursion in DNN: \cite{jeong2018improving}

\noindent \textbf{Automatic Differentiation.} 

\noindent \textbf{Array Languages and Fusion.}

\noindent \textbf{Numerical DSLs.} 

\noindent \textbf{Correctness of AD in functional languages.}
Several recent work \cite{huot2020correctness,vakar2020reverse,vakar2020denotational,brunel2019backpropagation,barthe2020versatility,mazza2021automatic} have focused on correctness of AD in a purely functional setting, 
often leaving efficiency on the side, especially for reverse-mode differentiation. 
We see our work as a complement and a first bridge between these works 
and more practical considerations of efficiency and implementations, 
which often require a lot more care than is acknowledegd in more theoretical works.

An exception is \cite{abadi-plotkin2020} which gives a reverse-mode transformation that is proved correct and with the right complexity.
This work is purely first-order, does not have arrays and uses a non standard semantics. 
Our work is more canonical in the sense that it can more directly be implemented in a standard call-by-value language.
They do allow conditionals and recursion but their semantics suggest that they don't have a canonical transformation on these constructs. 
Instead, they follow the principle of unrolling for recursive calls and evaluating the conditional before differentiating, 
which requires to recompute the whole gradient when taken at new values that can change the value of the conditional. 
This is a well-known limit in TensorFlow which TeaserFlow EagerMode tries to bypass. 

\noindent \textbf{Comparison to other recent papers.}

\subsection{Discussion and future work} % (fold)
\label{sub:discussion_and_future_work}

\noindent \textbf{More dynamic language}
JAX \cite{bradbury2020jax,frostig2018compiling} is a modern framework that provides state-of-the-art automatic differentiation. 
One feature that makes it popular is that it uses JIT compilation, 
which seems to be the best avenue to help best ML practionners as it allows them 
to quickly experiment but also benefit from the full power of static optimisations.
We are interested in seeing how our work could sit in a JIT compiler. 

\noindent \textbf{More optimisations.}
As pointed in \ref{sub:Adding general arrays} it would be interesting to have a more principled representation 
of tensors which could lead to more optimisations. 
There is always a tension between dynamicity and the possibility for static optimisations 
and we believe our language offers a good tradeoff, but it will be interesting to tailor it 
further for specific applications.

\noindent \textbf{Limitations and future work}
\begin{itemize}
    \item memory access for cost function 
    \item parallelism
    \item dedicated tensors for ML, and BLaS considerations
    \item implementation and benchmark
    \item conditionals
    \item recursion
    \item general HO function
    \item more 2nd order functions, though can be derived by hand once and then it's ok
    \item more dynamic arrays
    \item more dynamic control flow
    \item all store vs all recompute reverse-mode, and advanced checkpointing
    \item numerical accuracy
    \item fancier types and datatypes (eg dependent, adt) 
    \item iterated ad and higher derivatives
    \item use in Probabilistic programming
    \item non smooth and partial functions, although by simply following the ref book it's as usual in AD
    \item sparsity and data manipulation
    \item other optimisations like implicit function theorem
\end{itemize}

\MH{useful links (Swift is close to our project, and the doc is very readable):}
\begin{verbatim}https://docs.google.com/document/d/1_BirmTqdotglwNTOcYAW-ib6mx_jl-gH9Dbg4WmHZh0/edit# \end{verbatim}
\begin{verbatim}
	https://github.com/apple/swift/blob/master/docs/DifferentiableProgramming.md#approaches-to-automatic-differentiation
\end{verbatim}

\MH{understand related work sentence from Diff. Curry paper: The idea of using closures as back-propagators is receiving recent attention. For example Julia Zy- gote [15] and Swift AD adopts this design. Other recent work follows similar ideas [27, 26] but is using meta-programming as an implementation technique.}

\clearpage