\section{Simple pure reverse-mode differentiation}
\label{sec:simplediff}

\subsection{Source Language}
\label{sub:sourcelang}

\label{sub:sourcelang}

We consider a standard call-by-value language. 
It consists of a first-order functional language with arrays and a few typical second-order array operations. 
The types !T1,T2! and terms !e1,e2! are given in Figure~\ref{fig:source_grammar}.
We have included a minimal set of array operations for the sake of illustration,  it is not hard to add more.
See Section~\ref{sec:generalisation}.

\input{figures/source_grammar}

The typing rules are given in Figure~\ref{fig:source_typesystem}. 
For scalar operations, we assume given a set of operations, including $+$ and $*$. 
!op1! and !op2! denote respectively a unary and a binary operation on reals. 
These operations represent smooth total functions, 
but again this can be easily generalised (\S\ref{sec:generalisation}).  
Typical examples include !cos, exp, +, *!. 
We use infix notation for binary operators.

!reduce! is a fold left operator for which the function is assumed to be associative, 
and the provided initial value should be a unit of the binary operation.
It is a well-known parallel friendly construct. For the sake of simplicity in the presentation, the bound function in !reduce! 
is restricted to having no free variables. For the same reason, we currently restrict to arrays of reals.
We show how to lift these restrictions in Section~\ref{sec:generalisation}.

\input{figures/source_type_system}

\subsection{Target Language}

The target language of our source-code transformation is an extension to the source language.
It is a higher-order language, as our purely functional reverse-mode introduces a continuation.
The set of scalar operations should also be closed under partial differentiation. 
In more detail, for every unary scalar operation !op1!, 
we assumed given an operator $\partial_1$!op1! whose semantics should be the derivative of !op1!.
For every binary operator !op2!, we assume given operators $\partial_1$!op2!, $\partial_2$!op2! 
representing respectively the first and second partial derivative of !op2!.

Similarly, the target language contains more array primitives which are used to define the reverse derivatives of array operations. 
Scan left !scanl! is similar to fold left but also stores all the intermediate results in a array, which it returns.
In the same vein, scan right !scanr! perform a fold left by reading the array from right to left and also storing 
the intermediate results in a array from right to left.

Finally, we add a new shift operator !shift1!. 
It takes an array of size $n$,  shifts all the element of the array by one, 
forgetting the last element, and puts a one in the first place. 
This somewhat ad-hoc operator naturally shows up when differentiating fold-like operators.

The types and terms are presented in Figure~\ref{fig:target_grammar}.
Our lambda abstractions take $n$ arguments as we are not interested here with partial applications. 
In fact, the lambda abstractions introduced by reverse-mode will be removed during partial evaluation 
and the notation with lambda abstractions having $n$ bound variables makes reading slightly easier.
We note that we don't actually need the full power of higher-order because we only use lambda abstractions over variables of ground types
and let expressions binding such lambda abstractions. So we only require the target language to be second-order.
The type system for the extended grammar of the target language is presented in Figure~\ref{fig:target_typesystem}. 

\input{figures/target_grammar}

\input{figures/target_type_system}

\subsection{Macro for pure reverse mode transformation} % (fold)
\label{sub:Macro for pure reverse mode transformation}

TODO:redo
In Figure~\ref{fig:direct_diff_macro} we present our direct transformation from the source language to the target language for pure reverse mode differentiation.
We explain in the next Section (\ref{sub:Partial evaluation and optimisation}) how to optimise it further and compute the gradient.
Admittedly, this transformation may be hard to read, and prove correct. 
In Section~\ref{sec:unf} we decompose this transformation into three simpler steps via a novel intermediate representation (UNF).

\begin{example}
    TODO
\end{example}

We introduce several notations which are useful when defining the transformation for reverse-mode.

\input{figures/notation_D}

\input{figures/direct_macro_diff}

\subsection{Partial evaluation and optimisation} % (fold)
\label{sub:Partial evaluation and optimisation}

Given a term $\Gamma\vdash e : \reals$, we can compute its gradient $\grad_\Gamma e$ from a particular instance of 
$\directD{\rho}{\Gamma}{Y}(e)$. First, $\rho, Y$ specifies if we want to compute the whole gradient regarding the variables from $\Gamma$ or a subset of it.
For a subset $\rho\subset \Gamma$, one chooses $Y$ to be the projection function sending a variable 
$x_i:G$ of $\Gamma$ to $x_i$ if it belongs to $\rho$ and to $0_G$ otherwise.
In particular, we take $Y=Id_\Gamma$ to compute the whole gradient.
Next, the gradient will be given by the second part of the pair $\directD{\rho}{\Gamma}{Y}(e)$, 
and we need to initialise the tangent variables. All of them are set to $0$ except the one corresponding to the output value of !e!, 
which we initialise at $1$ to run the backpropagation. 
All in all, we compute the gradient via $\pi_2\directD{\rho}{\Gamma}{Id_\Gamma}(e)(0_\Gamma,1)$.

\begin{example}
    TODO
\end{example}

Following the insight from Section~\ref{subsec:insights}, 
we don't want to keep all the lambda abstractions as this is costly. 
Fortunately, the transformation is designed in such a way that we only have applications 
and we can use partial evaluation to do beta redudction. 

By inspection in Figure~\ref{fig:direct_diff_macro}, 
we see that there is a linear usage of  each continuation $Y$ 
and that it is always applied to almost the identity. 
More precisely, we have applications of the form !fun (x1,...,xn) -> Y(e1,...,en)! 
where the !ei! are !xi! except for at most $k$ (independant of $n$) terms.
In fact we have $k=2$, except for the !map2! case which requires a more detailed analysis (see Section~\ref{sec:complexity}).

Using the invariant above and the inlining and forward substitution given in Figure~\ref{fig:optim}, this allows us to rewrite
!fun (x1,...,xn) -> (fun (y1,...,yn) -> (f1,...,fn))(e1,...,en)! 
$\transto$ 
!fun (x1,...,xn) -> let y1,...,yn = e1,...,en in (f1,...,fn)!
$\transto$
!fun (x1,...,xn) -> (f1[e1/y1],...,fn[en/yn])!

TODO: redo
Now, importantly, every !yi! is used at most once (except for !map2! again and !reduce! which require a more detailed analysis)
which garantees that this will not blow up the complexity.

TODO: finish explanation, not sure how to conclude at the moment

Though this transformation after the inlining above already has the right complexity, 
its purity allows us to take the most out of generic optimisations.  
A list of common optimisations that are useful for this language can be found in Figure~\ref{fig:optim}.

TODO: use the optimisations to be efficient on examples