\section{Simple pure reverse-mode differentiation}
\label{sec:simplediff}

\subsection{Source Language}
\label{sub:sourcelang}

\label{sub:sourcelang}

We consider a standard call-by-value language. 
It consists of a first-order functional language with arrays and a few typical second-order array operations. 
The types !T1,T2! and terms !e1,e2! are given in Figure~\ref{fig:source_grammar}.
We have included a minimal set of array operations for the sake of illustration,  it is not hard to add more.
See Section~\ref{sec:generalization}.

\input{figures/source_grammar}

The typing rules are given in Figure~\ref{fig:source_typesystem}. 
For scalar operations, we assume given a set of operations, including $+$ and $*$. 
!op1! and !op2! denote respectively a unary and a binary operation on reals. 
These operations represent smooth total functions, 
but again this can be easily generalized (\S\ref{sec:generalization}).  
Typical examples include !cos, exp, +, *!. 
We use infix notation for binary operators.

!reduce! is a fold left operator for which the function is assumed to be associative, 
and the provided initial value should be a unit of the binary operation.
It is a well-known parallel friendly construct. For the sake of simplicity in the presentation, the bound function in !reduce! 
is restricted to having no free variables. For the same reason, we currently restrict to arrays of reals.
We show how to lift these restrictions in Section~\ref{sec:generalization}.

\input{figures/source_type_system}

\subsection{Target Language}

The target language of our source-code transformation is an extension to the source language.
It is a higher-order language, as our purely functional reverse-mode introduces a continuation.
The set of scalar operations should also be closed under partial differentiation. 
In more detail, for every unary scalar operation !op1!, 
we assumed given an operator $\partial_1$!op1! whose semantics should be the derivative of !op1!.
For every binary operator !op2!, we assume given operators $\partial_1$!op2!, $\partial_2$!op2! 
representing respectively the first and second partial derivative of !op2!.

Similarly, the target language contains more array primitives which are used to define the reverse derivatives of array operations. 
Scan left !scanl! is similar to fold left but also stores all the intermediate results in a array, which it returns.
In the same vein, scan right !scanr! perform a fold left by reading the array from right to left and also storing 
the intermediate results in a array from right to left.

Finally, we add a new shift operator !shift1!. 
It takes an array of size $n$,  shifts all the element of the array by one, 
forgetting the last element, and puts a one in the first place. 
This somewhat ad-hoc operator naturally shows up when differentiating fold-like operators.

The types and terms are presented in Figure~\ref{fig:target_grammar}.
Our lambda abstractions take $n$ arguments as we are not interested here with partial applications. 
In fact, the lambda abstractions introduced by reverse-mode will be removed during partial evaluation 
and the notation with lambda abstractions having $n$ bound variables makes reading slightly easier.
We note that we don't actually need the full power of higher-order because we only use lambda abstractions over variables of ground types
and let expressions binding such lambda abstractions. So we only require the target language to be second-order.
The type system for the extended grammar of the target language is presented in Figure~\ref{fig:target_typesystem}. 

\input{figures/target_grammar}

\input{figures/target_type_system}

\subsection{Macro for pure reverse mode transformation} % (fold)
\label{sub:Macro for pure reverse mode transformation}

In Figure~\ref{fig:direct_diff_macro} we present our direct transformation from the source language to the target language for pure reverse mode differentiation.
We explain in the next Section (\ref{sub:Partial evaluation and optimization}) how to optimize it further and compute the gradient.
Admittedly, this transformation may be hard to read, and it is not straightforward to show its correctness directly. 
In Section~\ref{sec:unf} we decompose this transformation into three simpler steps via a novel intermediate representation (UNF).

\begin{example}
    The reverse-mode transformation of the terms from the introduction are given by

    \begin{tabular}{c l}
        &$\directD{\rho}{\Gamma}{Y}$(!let w$_1$ = x$_1$ * x$_2$ in let w$_2$ = w$_1$ * x$_1$ in w$_2$!) \\
        =& !let w$_1$,Y$_1$=!\\
        & \quad\quad !let y$_1$1,Y$_1$1= <x$_1$, fun (y$_1$,y$_2$,y$_3$,z) -> Y(y$_1$+z,y$_2$,y$_3$)> in! \\
        & \quad\quad !let y$_{12}$,Y$_{12}$= <x$_{2}$, fun (y$_{1}$,y$_{2}$,y$_{3}$,y$_{4}$,z) -> Y$_{11}$(y$_{1}$,y$_{2}$+z,y$_{3}$,y$_{4}$)> in! \\
        & \quad\quad !<y$_{11}$ * y$_{12}$, fun (y$_{1}$,y$_{2}$,y$_{3}$,z) -> Y$_{12}$(y$_{1}$,y$_{2}$,y$_{3}$,y$_{12}$*z,y$_{11}$*z) > in! \\
        & !let w$_{2}$,Y$_{2}$=!\\
        & \quad\quad !let y$_{21}$,Y$_{21}$= <w$_{1}$, fun (y$_{1}$,y$_{2}$,y$_{3}$,y$_{4}$,z) -> Y(y$_{1}$,y$_{2}$,y$_{3}$,y$_{4}$+z)> in! \\
        & \quad\quad !let y$_{22}$,Y$_{22}$= <x$_{1}$, fun (y$_{1}$,y$_{2}$,y$_{3}$,y$_{4}$,y$_{5}$,z) -> Y$_{11}$(y$_{1}$+z,y$_{2}$,y$_{3}$,y$_{4}$,y$_{5}$)> in! \\
        & \quad\quad !<y$_{21}$ * y$_{22}$, fun (y$_{1}$,y$_{2}$,y$_{3}$,y$_{4}$,z) -> Y$_{22}$(y$_{1}$,y$_{2}$,y$_{3}$,y$_{4}$,y$_{22}$*z,y$_{21}$*z) > in! \\
        & !let y,Y$_{3}$= <w, fun (y$_{1}$,y$_{2}$,y$_{3}$,y$_{4}$,z) -> Y$_{2}$(y$_{1}$,y$_{2}$,y$_{3}$,y$_{4}$+z)> in! \\
        & !<y, fun (y$_{1}$,y$_{2}$,y$_{3}$,z) -> Y$_{3}$(y$_{1}$,y$_{2}$,y$_{3}$,0,z) >!
    \end{tabular}
    \medskip

    \begin{tabular}{c l}
        &$\directD{\rho}{\Gamma}{Y}$(!prod(A)!) \\
        =& !let x,Y$_{1}$= <0, fun (X,z) -> Y(X)> in! \\
        & !let B,Y$_{2}$= <A, fun (X,x,Z) -> Y(X+Z,x)> in!\\
        & !let A$_{0}$= scanl * x B in!\\
        & !let A$_{1}$= map2 (a,b.b) A$_{0}$ B in!\\
        & !let A$_{2}$= shift1 (map2 (a,b.a) A$_{0}$ B) in!\\
        & !let A$_{3}$= scanr * 1 A$_{1}$ in!\\
        & !<prod(B), fun (X,z) -> Y$_{2}$(X,0,map2 (a,b. a*z*b) A$_{2}$ A$_{3}$)>! 
    \end{tabular}
\end{example}

The idea is that $\rho$ represents the return type of the derivative part, which should be !A$_{1} \times \ldots \times$ A$_n$! 
if we want the whole gradient of a term !e! in context !$\Gamma = \;$x$_{1}$:A$_{1}$,...,x$_n$:A$_n$!.
The subscript $\Gamma$ denotes the current context, 
which is locally augmented for instance when differentiating a !let! rule.
For non-unary operations, we differentiate the arguments from left to right and add their derivatives to the current stack, 
which is modeled by the continuation $Y$. 
Importantly for performance, each continuation variable $Y$ is only used once.

We now introduce several notations which are useful when defining the transformation for reverse-mode.

\input{figures/notation_D}

We have the following typing lemma for $\directD{\rho}{\Gamma}{Y}$.
\begin{lemma}[Typing $\directD{\rho}{\Gamma}{Y}$]
    If $\Gamma \vdash$ !e: A!, then $\Gamma$!,Y: !$\Gamma\to \rho \vdash \directD{\rho}{\Gamma}{Y}$!(e): !$\directD{\rho}{\Gamma}{Y}$!(A)!.
\end{lemma}

\input{figures/direct_macro_diff}

\subsection{Partial evaluation and optimization} % (fold)
\label{sub:Partial evaluation and optimization}

TODO: redo
Given a term $\Gamma\vdash e : \reals$, we can compute its gradient $\grad_\Gamma e$ from a particular instance of 
$\directD{\rho}{\Gamma}{Y}(e)$. First, $\rho, Y$ specifies if we want to compute the whole gradient regarding the variables from $\Gamma$ or a subset of it.
For a subset $\rho\subset \Gamma$, one chooses $Y$ to be the projection function sending a variable 
$x_i:G$ of $\Gamma$ to $x_i$ if it belongs to $\rho$ and to $0_G$ otherwise.
In particular, we take $Y=Id_\Gamma$ to compute the whole gradient.
Next, the gradient will be given by the second part of the pair $\directD{\rho}{\Gamma}{Y}(e)$, 
and we need to initialize the tangent variables. All of them are set to $0$ except the one corresponding to the output value of !e!, 
which we initialize at $1$ to run the backpropagation. 
All in all, we compute the gradient via $\pi_2\directD{\rho}{\Gamma}{Id_\Gamma}(e)(0_\Gamma,1)$.

Following the insight from Section~\ref{subsec:insights}, 
we don't want to keep all the lambda abstractions as this is costly. 
Fortunately, the transformation is designed in such a way that we only have applications 
and we can use partial evaluation to do beta-reduction. 

By inspection in Figure~\ref{fig:direct_diff_macro}, 
we see that there is a linear usage of  each continuation $Y$ 
and that it is always applied to almost the identity. 
More precisely, we have applications of the form !fun (x1,...,xn) -> Y(e1,...,en)! 
where the !ei! are !xi! except for at most $k$ (independant of $n$) terms.
In fact we have $k=2$, except for the !map2! case which requires a more detailed analysis (see Section~\ref{sec:complexity}).

Using the invariant above and the inlining and forward substitution given in Figure~\ref{fig:optim}, this allows us to rewrite
!fun (x$_{1}$,...,x$_n$) -> (fun (y$_{1}$,...,yn) -> (f$_{1}$,...,f$_n$))(e$_{1}$,...,e$_n$)! 
$\transto$ 
!fun (x$_{1}$,...,x$_n$) -> let y$_{1}$,...,y$_n$ = e$_1$,...,e$_n$ in (f$_{1}$,...,f$_n$)!
$\transto$
!fun (x$_{1}$,...,x$_n$) -> (f$_{1}$[e$_{1}$/y$_{1}$],...,f$_n$[e$_n$/y$_n$])!

TODO: finish explanation

\begin{example}
    The gradient of the terms from the introduction reduces to

    \begin{tabular}{c l}
        & $\grad_\Gamma$(!let w$_{1}$ = x$_{1}$ * x$_{2}$ in let w$_{2}$ = w$_{1}$ * x$_{1}$ in w$_{2}$!) \\
        =& TODO \\
    \end{tabular}
    \medskip

    \begin{tabular}{c l}
        & $\grad_\Gamma$(!prod(A)!) \\
        =& TODO \\
    \end{tabular}
\end{example}

Though this transformation after the inlining above already has the right complexity, 
its purity allows us to make the most out of generic optimizations.  
A list of common optimizations that are useful for this language can be found in Figure~\ref{fig:optim}.

\begin{example}
As shown in appendix \ref{sub:gradintro}, the optimizations from Figure~\ref{fig:optim} 
are sufficient to show that the gradients of the terms of the introduction reduce to the following.

\begin{tabular}{{r c l}}
    $\nabla_A$!prod(A)! &=& !map2 * (scanr * 1 A) (shift1(scanl * 1 A))!\\
    $\nabla_A$!sum(A)! &=& !map (x -> 1) A!\\
    $\nabla_A$!dot(A,B)! &=& !B! 
\end{tabular}
\end{example}