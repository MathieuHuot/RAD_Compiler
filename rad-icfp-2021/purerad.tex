\section{Simple pure reverse-mode differentiation}
\label{sec:simplediff}

\subsection{Source Language}
\label{sub:sourcelang}

\label{sub:sourcelang}

We consider a standard call-by-value language. 
It consists of a first-order functional language with arrays and a few typical second-order array operations. 
The types !T1,T2! and terms !e1,e2! are given in Figure~\ref{fig:source_grammar}.
We have included a minimal set of array operations for the sake of illustration,  it is not hard to add more.
See Section~\ref{sec:generalisation}.

\input{figures/source_grammar}

The typing rules are given in Figure~\ref{fig:source_typesystem}. 
For scalar operations, we assume given a set of operations, including $+$ and $*$. 
!op1! and !op2! denote respectively a unary and a binary operation on reals. 
These operations represent smooth total functions, 
but again this can be easily generalised (\S\ref{sec:generalisation}).  
Typical examples include !cos, exp, +, *!. 
We use infix notation for binary operators.

For the sake of simplicity in the presentation, the bound function in !reduce! 
is restricted to having no free variables. For the same reason, we currently restrict to arrays of reals.
We show how to lift these restrictions in Section~\ref{sec:generalisation}.

\input{figures/source_type_system}

\subsection{Target Language}

The target language of our source-code transformation is an extension to the source language.
It is a higher-order language, as our purely functional reverse-mode introduces a continuation.
The set of scalar operations should also be closed under partial differentiation. 
In more detail, for every unary scalar operation !op1!, 
we assumed given an operator $\partial_1$!op1! whose semantics should be the derivative of !op1!.
For every binary operator !op2!, we assume given operators $\partial_1$!op2!, $\partial_2$!op2! 
representing respectively the first and second partial derivative of !op2!.

TODO:redo
Similarly, the target language contains more array primitives which are used to define the reverse derivatives of array operations. 
!scanl! is a !foldl! that stores all intermediate results. 
!scanr! is similar, but reads the array from right to left, and also stores from right to left.
!reduce! is a !foldl! for which the function is associative. 
This means in practice that it can be computed faster.
Finally, we add a somewhat strange operator which we call !shift1!. 
It takes an array of size $n$,  shifts all the element of the array by one, 
forgetting the last element, and puts a one in the first place. 
This operator naturally shows up when differentiating !reduce!.

The types and terms are presented in Figure~\ref{fig:target_grammar}.
We are not interested here with partial applications and our language is pure, so functions take $n$ arguments.
Lambda abstractions and applications will be removed during partial evaluation and this notation makes reading slightly easier.

\input{figures/target_grammar}

The type system for the extended grammar of the target language is presented in Figure~\ref{fig:target_typesystem}. 

\input{figures/target_type_system}

\subsection{Macro for pure reverse mode transformation} % (fold)
\label{sub:Macro for pure reverse mode transformation}

TODO:redo
In Figure~\ref{fig:direct_diff_macro} we present the direct transformation for pure reverse mode differentiation.
We explain in the next Section (\ref{sub:Partial evaluation and optimisation}) how to optimise it further and compute the gradient.
This transformation may be hard to read and prove correct. 
In Section~\ref{sec:unf} we decompose this transformation into several easier 
and more intuitive steps via a new intermediate representation which we call UNF.

TODO: on the examples

We introduce several notations which are useful when defining the transformation for reverse-mode.

\input{figures/notation_D}

\input{figures/direct_macro_diff}

\subsection{Partial evaluation and optimisation} % (fold)
\label{sub:Partial evaluation and optimisation}

Given a term $\Gamma\vdash e : \reals$, we can compute its gradient $\grad_\Gamma e$ from a particular instance of 
$\directD{\rho}{\Gamma}{Y}(e)$. First, $\rho, Y$ specifies if we want to compute the whole gradient regarding the variables from $\Gamma$ or a subset of it.
For a subset $\rho\subset \Gamma$, one chooses $Y$ to be the projection function sending a variable 
$x_i:G$ of $\Gamma$ to $x_i$ if it belongs to $\rho$ and to $0_G$ otherwise.
In particular, we take $Y=Id_\Gamma$ to compute the whole gradient.
Next, the gradient will be given by the second part of the pair $\directD{\rho}{\Gamma}{Y}(e)$, 
and we need to initialise the tangent variables. All of them are set to $0$ except the one corresponding to the output value of !e!, 
which we initialise at $1$ to run the backpropagation. 
All in all, we compute the gradient via $\pi_2\directD{\rho}{\Gamma}{Id_\Gamma}(e)(0_\Gamma,1)$.

TODO: on the examples

Following the insight from Section~\ref{subsec:insights}, 
we don't want to keep all the lambda abstractions as this is costly. 
Fortunately, the transformation is designed in such a way that we only have applications 
and we can use partial evaluation to do beta redudction. 

By inspection in Figure~\ref{fig:direct_diff_macro}, 
we see that there is a linear usage of  each continuation $Y$ 
and that it is always applied to almost the identity. 
More precisely, we have applications of the form !fun (x1,...,xn) -> Y(e1,...,en)! 
where the !ei! are !xi! except for at most $k$ (independant of $n$) terms.
In fact we have $k=2$, except for the !map2! case which requires a more detailed analysis (see Section~\ref{sec:complexity}).

Using the invariant above and the inlining and forward substitution given in Figure~\ref{fig:optim}, this allows us to rewrite
!fun (x1,...,xn) -> (fun (y1,...,yn) -> (f1,...,fn))(e1,...,en)! 
$\transto$ 
!fun (x1,...,xn) -> let y1,...,yn = e1,...,en in (f1,...,fn)!
$\transto$
!fun (x1,...,xn) -> (f1[e1/y1],...,fn[en/yn])!
Now, importantly, every !yi! is used at most once (except for !map2! again and !reduce! which require a more detailed analysis)
which garantees that this will not blow up the complexity.

TODO: finish explanation, not sure how to conclude at the moment

Though this transformation after the inlining above already has the right complexity, 
its purity allows us to take the most out of generic optimisations.  
A list of common optimisations that are useful for this language can be found in Figure~\ref{fig:optim}.

TODO: use the optimisations to be efficient on examples