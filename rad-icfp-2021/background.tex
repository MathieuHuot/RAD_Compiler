\section{Reverse-mode in Automatic Differentiation}
\label{sec:background}

% \subsection{Rudiments of differentiation and dual numbers.}
% Recall that the derivative of a function $f:\RR\to \RR$, if it exists, is a function
% $\nabla f:\RR\to \RR$ such that $\nabla f(x_0)=\frac {\dif f(x)}{\dif x}(x_0)$ is the gradient of $f$ at $x_0$. 

% To find $\nabla f$ in a compositional way, two generalizations are reasonable:
% \begin{itemize}
% \item We need both $f$ and $\nabla f$ when calculating $\nabla (f;g)$
% of a composition $f;g$, using the chain rule, so we are really interested in the pair $(f,\nabla f):\RR\to \RR\times \RR$;
% \item In building $f$ we will need to consider functions of multiple arguments, such as $+:\RR^2\to \RR$, and these functions should propagate derivatives.
% \end{itemize}
% Thus we are more generally interested in transforming a function $g:\RR^n\to \RR$ into a function
% $h:(\RR\times \RR)^n\to \RR\times \RR$ in such a way that for any
% $f_1\dots f_n:\RR\to\RR$, 
% \begin{equation}
%   \label{eqn:dualnumber}
%   (f_1,\nabla f_1,\dots, f_n,\nabla f_n);h
%   =
%   ((f_1,\dots, f_n);g,\nabla ((f_1, \dots, f_n);g))\text.
% \end{equation}

% An intuition for $h$ is often given in terms of dual numbers.
% The transformed function operates on pairs of numbers, $(x,x')$, and it is common
% to think of such a pair as $x+x'\epsilon$ for an `infinitesimal' $\epsilon$.
% But while this is a helpful intuition, the formalization of infinitesimals can be intricate, 
% and the development in this paper is focussed on the elementary formulation in~\eqref{eqn:dualnumber}. 

% The reader may also notice that $h$ encodes all the partial derivatives of
% $g$. For example, 
% if $g \colon \RR^2\to \RR$, then with $f_1(x)\defeq x$ and $f_2(x)\defeq x_2$, by applying \eqref{eqn:dualnumber} to $x_1$ we obtain
% $h(x_1,1,x_2,0)=(g(x_1,x_2), \frac {\partial g(x,x_2)}{\partial x}(x_1))$
% and similarly 
% $h(x_1,0,x_2,1)=(g(x_1,x_2), \frac {\partial g(x_1,x)}{\partial x}(x_2))$.
% And conversely, if $g$ is differentiable in each argument, then
% a unique $h$ satisfying \eqref{eqn:dualnumber} can be found by taking linear
% combinations of partial derivatives:
% \[\textstyle h(x_1,x_1',x_2,x_2')=(g(x_1,x_2),x_1' \cdot\frac {\partial g(x,x_2)}{\partial x}(x_1)+x_2'\cdot \frac {\partial g(x_1,x)}{\partial x}(x_2))\text.\]

% In summary, the idea of differentiation with dual numbers is 
% to transform a differentiable function
% $g:\RR^n\to \RR$ to a function $h:\RR^{2n}\to \RR^2$ which captures~$g$ and all its partial derivatives. We packaged this up in~\eqref{eqn:dualnumber} as a sort-of invariant which is useful for building derivatives of compound functions $\RR\to\RR$ in a compositional way.
% The idea of automatic differentiation is to perform this transformation at the source code level. 

% The two main ways in which it is performed in practice is by operator overloading \cite{} or by source code transformation. We focus our work on the latter.

% \subsection{Reverse-mode AD}

% AD comes in two main flavors: forward mode and reverse mode. 
% Forward mode computes a directional derivative of the Jacobian while reverse-mode computes a directional derivative of the transpose Jaobian. 
% This has a significant impact in practice as this means reverse mode can compute the whole gradient of a function $f:\RR^n\to \RR$ in one pass, whereas forward mode needs $n$ passes. 
% Assuming that both have about the same complexity, this can have a huge impact in practice. 
% Such functions are expremely commonly seen as loss functions in the context of machine learning for instance.

% Reverse-mode computes the derivatives backward, using the symmetry of the chain rule.
% This means the compute flow is reverted after the first pass to compute the value of the function and its intermediate values. 
% These values appear in the chain rule so should be stored or recomputed.
% These things make reverse-mode harder to implement efficiently, and especially harder to analyze and optimize. 
% Traditional implementations use references.
% For instance, let 
%  \begin{align*}
%      \trm\defeq &\letin{w_1}{\var_1 * \var_2}{\\&\letin{w_2}{w_1 * \var_1}{\\&w_2}}
%  \end{align*}
%  The imperative version of the reverse differentiation of $\trm$ is
% 	\begin{align*}
% 		&w_1=\var_1 * \var_2 \\
% 		&w_2=w_1 * \var_1 \\
%         &\var_1':=0~;~\var_2':=0 \\
%         &w_1':=0 \\
% 		&w_2':=1 \\
% 		&w_1'+= \var_1 * w_2'; \var_1'+= w_1*w_2' \\
% 		&\var_1'+= \var_2 * w_1'; \var_2'+=\var_1 * w_1'\\
% 		&\text{return }(\var_1',\var_2')  
% 	\end{align*}
% The tangent part for every variable from the context $\var_i'$ and every intermediate variable $w_i'$ is initialized at $0$, 
% except for the return variable $w_2$ for which  $w_2':=1$. 

% As noted in \cite{pearlmutter2008reverse}, in a functional language, one can have a purely functional reverse-mode using continuations. 
% This representation is notoriously inefficient, and the existing implementations of reverse mode in a functional setting have followed the lines of the imperative version above \cite{pearlmutter2008reverse, wang2018demystifying, baydin2016diffsharp}.

% In the next section, we first introduce a very simple first-order language. 
% Then we define reverse-mode on this language as a program transformation. 
% This first transformation is quite inefficient and we how we can use standard optimizations on functional languages to obtain a provably efficient pure program.
% The key idea behind that will be generalized and explored in detail in the rest of the paper is that of turning every operator into a unary one.     
% The remaining of the paper will generalize this idea and explore some consequences of this new approach. 

\subsection{Rudiments of forward-mode AD and dual numbers}

Recall that the derivative of a function $f:\RR\to \RR$, if it exists, is a function
$\nabla f:\RR\to \RR$ such that $\nabla f(x_0)=\frac {\dif f(x)}{\dif x}(x_0)$ is the gradient of $f$ at $x_0$. 

To find $\nabla f$ in a compositional way, two generalizations are reasonable:
\begin{itemize}
\item We need both $f$ and $\nabla f$ when calculating $\nabla (f;g)$
of a composition $f;g$, using the chain rule, so we are really interested in the pair $(f,\nabla f):\RR\to \RR\times \RR$;
\item In building $f$ we will need to consider functions of multiple arguments, such as $+:\RR^2\to \RR$, and these functions should propagate derivatives.
\end{itemize}
Thus we are more generally interested in transforming a function $g:\RR^n\to \RR$ into a function
$h:(\RR\times \RR)^n\to \RR\times \RR$ in such a way that for any
$f_1\dots f_n:\RR\to\RR$, 
\begin{equation}
  \label{eqn:dualnumber}
  (f_1,\nabla f_1,\dots, f_n,\nabla f_n);h
  =
  ((f_1,\dots, f_n);g,\nabla ((f_1, \dots, f_n);g))\text.
\end{equation}

An intuition for $h$ is often given in terms of dual numbers.
The transformed function operates on pairs of numbers, $(x,x')$, and it is common
to think of such a pair as $x+x'\epsilon$ for an `infinitesimal' $\epsilon$.
But while this is a helpful intuition, the formalization of infinitesimals can be intricate, 
and the development in this paper is focussed on the elementary formulation in~\eqref{eqn:dualnumber}. 

The reader may also notice that $h$ encodes all the partial derivatives of
$g$. For example, 
if $g \colon \RR^2\to \RR$, then with $f_1(x)\defeq x$ and $f_2(x)\defeq x_2$, by applying \eqref{eqn:dualnumber} to $x_1$ we obtain
$h(x_1,1,x_2,0)=(g(x_1,x_2), \frac {\partial g(x,x_2)}{\partial x}(x_1))$
and similarly 
$h(x_1,0,x_2,1)=(g(x_1,x_2), \frac {\partial g(x_1,x)}{\partial x}(x_2))$.
And conversely, if $g$ is differentiable in each argument, then
a unique $h$ satisfying \eqref{eqn:dualnumber} can be found by taking linear
combinations of partial derivatives:
\[\textstyle h(x_1,x_1',x_2,x_2')=(g(x_1,x_2),x_1' \cdot\frac {\partial g(x,x_2)}{\partial x}(x_1)+x_2'\cdot \frac {\partial g(x_1,x)}{\partial x}(x_2))\text.\]

In summary, the idea of differentiation with dual numbers is 
to transform a differentiable function
$g:\RR^n\to \RR$ to a function $h:\RR^{2n}\to \RR^2$ which captures~$g$ and all its partial derivatives. We packaged this up in~\eqref{eqn:dualnumber} as a sort-of invariant which is useful for building derivatives of compound functions $\RR\to\RR$ in a compositional way.
The idea of automatic differentiation is to perform this transformation at the source code level. 

The main two ways in which it is performed in practice is by operator overloading or by source code transformation (see e.g. \cite{griewank2008evaluating} Chapter 6). 
Our approach focuses on source code transformation which is better fitted for compilation and optimizations.

\subsection{Reverse-mode differentiation}

The problem with the previous approach shows up when one wants to compute the full gradient of a function $\RR^n\to\RR$, for a large $n$. 
Forward-mode only computes one directional derivative, for instance one partial derivative. This implies $n$ passes must be performed through the forward derivative to compute the whole gradient.
By using the symmetry in the chain rule, there is a way to compute the whole gradient faster, and this method is reverse-mode differentiation.
Suppose given a function $f=f_n\circ...\circ f_1 :\RR^n\to\RR$. 
The simplest way to see this mathematically, is first to say that forward mode essentially computes $(Jf)v=Jf_n(Jf_{n-1}(...(Jf_1v))...)$ for a direction $v\in\RR^n$. 
Reverse-mode, on the other hand, computes $(Jf)^Tv =J^Tf_1(J^Tf_{2}(...(J^Tf_nv))...)$ for a vector $v\in\RR$.
In particular, taking $v=1$ computes the gradient of $f$.

Because the computation flow of the function is reversed, the actual implementation of reverse-mode is way more tricky. 
(TODO: paragraph below from Matthijs' paper, need to rewrite.)
Reverse-mode AD is only well-understood as a source-code transformation 
(also called define-then-run style AD) on limited programming languages. 
Typically, its implementations on more expressive languages that have features such as higher-order functions and conditionals
make use of define-by-run approaches. 
These approaches first build a computation graph during runtime, effectively evaluating the program until a straight-line
first-order program is left, and then they evaluate this new program \cite{carpenter2015stan,paszke2017automatic}. 
Such approaches have the severe downside that the differentiated code cannot benefit from existing optimizing compiler architectures. 
As such, these AD libraries need to be implemented using carefully, manually optimized code, that for example does not contain any common subexpressions. 
This implementation process is precarious and labour intensive. 
Further, some whole-program optimizations that a compiler would detect go entirely unused in such systems.

\subsection{Purely functional inefficient reverse-mode}

Following \cite{pearlmutter2008reverse}, there is a simple way to define an inefficient yet purely functional reverse-mode transformation for first-order programs.
We review a slight modification of the transformation we presented, which is also better explained on an example. 

Let the term we want to differentiate be $x1:\RR,...,xn:\RR\vdash exp(cos(xi))$.
Following the chain rule, we need the jacobian matrices of $cos$ at $xi$ and of $exp$ at $cos(xi)$. 
Instead of considering these as operations from $\RR\to\RR$, we consider them as functions from the whole context. So $cos$ and $exp$ are seen as functions $\RR^n\to\RR$.
However, by simply doing this we lose compositionality. 
So $cos$ will also return its context and is now seen as a function $\sem{cos}:\RR^n\to\RR^{n+1}$.
Similarly for $exp$, but also taking the return value of $\sem{cos}$ as an extra argument, the one it will actually use and not simply return. 
We thus obtain $\sem{exp}:\RR^{n+1}\to\RR^{n+2}$. Now their jacobians matrices $J\sem{cos} \in Mat_{n,n+1}$, $J\sem{exp} \in Mat_{n+1,n+2}$ also compose nicely.
The same can be done for binary (or more) operators and let bindings (assuming no nested lets). 
This transforms a first-order program to a function $f:\RR^n\to\RR^{n+m}$ of the form $f_m\circ...\circ f_1$. 
If the original program was of type $\RR$, then the return value of the original program is the last component of $f$.
Following the mathematical presentation of reverse-mode above, the gradient of the the original program is then obtained as 
\[\nabla f= J^Tf(0,...,0,1)=J^Tf_1(J^Tf_{2}(...(J^Tf_m(0,...,0,1)))...)\]

To actually reverse the order of computation needed for this transpose of jacobians, we use a sort of simple continuation.
$f_i$ is turned into $\Dsynrevsymbol{f_i}:=<f_i, \lambda Y. Y\circ J^Tf_i>$ where $Y:\RR^{n+i-1}\to \RR^n$. 
We recover compositionality by noting that $<f_{i+1}(f_i), (\lambda Y. Y\circ J^Tf_{i+1}>)(\lambda Y. Y\circ J^Tf_i)>$ reduces to
$<f_{i+1}\circ f_i, \lambda Y. Y\circ J^Tf_i \circ J^Tf_{i+1}>$, and thus by induction we can obtain $<f, \lambda Y. YJ^Tf>$.
By applying the identity continuation $\RR^n\to\RR^n$ on the second component then the result to $(0,...,0,1)$, 
we have obtained a purely functional to compute $\nabla f$. 

This method is quite inefficient still. First, we have to carry a continuation and $\beta$-reduce a lot of higher-order functions.
Second, each $J^Tf_{i+1}$ is a potentially huge matrix if $n$ or $m$ is big.

The imperative version of reverse mode for a binary operator $op(x,y)$ is something like $x'+= \partial_1op(x,y);y'+= \partial_2op(x,y)$. 
It is for these cases that mutation is usually key. 
If we see our term as a directed graph, reverse mode needs to backpropagate from the end of the graph to the starting nodes via every path.
It's hard to keep track of all these information in parallel in a functional way.
One of the key simple ideas that we used was to transform every operator into a unary one. 
This essentially trivialises the computation flow to a line. 
Even if the starting program was a straight line program, 
having non-unary operators was a source of inefficiency which justified mutation in the first place.
By returning every variable every time, the problem of using a variable several times does not need to be dealt with via mutation. 
This simple idea of transforming a program into essentially a straight line is what our new intermediate representation unary-form does. 

\subsection{Insights for efficient reverse-mode}\
\label{subsec:insights}

If we look at $J^Tf_{i+1}$, we notice that this function is almost the identity, except at the last row. 
Even on the last row, if the original term was a unary or binary operator like $cos, exp, +, *$, 
the row is zero except for at most two indices (one for unary operators).
This means we can use a more compact representation $J^T\sem{op(xi,xj)}:=\lambda (y1,...,y(n+i)).(y1,...,y(n+i))+[i]\partial_1op(xi,xj)+[j]\partial_2op(xi,xj)$, 
where $[k]$ means that the element is added at the $k$-th index of the tuple.

We know in advance that such jacobian functions are going to be applied one to another and we can use partial evaluation to $\beta$-reduce all of these $\lambda$s.
Because each function is almost the identity, we obtain a lot of substitutions of the form $[x/y]$ where both $x$ and $y$ are variables. 
This allows us to drastically reduce the size of $J^Tf$. In fact, for simple programs, this is basically enough to get an efficient purely functional reverse derivative transformation.
We develop this idea further for a richer language.
