\section{Reverse-mode in Automatic Differentiation}
\label{sec:background}

\subsection{Rudiments of forward-mode AD and dual numbers}

To find the gradient $\nabla f$ of a function $f:\RR\to \RR$ in a compositional way, need both $f$ and $\nabla f$ when calculating $\nabla (f;g)$.
We are more thus generally interested in transforming a function $f:\RR^n\to \RR$ into a function
$g:(\RR\times \RR)^n\to \RR\times \RR$ in such a way that for any
$f_1\dots f_n:\RR\to\RR$, 
\begin{center}
    $(f_1,\nabla f_1,\dots, f_n,\nabla f_n);g = ((f_1,\dots, f_n);f,\nabla ((f_1, \dots, f_n);f))$.
\end{center}

The idea of AD is to systematically transform a differentiable function $f:\RR^n\to \RR$ to a function $g:\RR^{2n}\to \RR^2$ which captures~$f$ and all its partial derivatives.
An intuition for $g$ is often given in terms of dual numbers. The transformed function operates on pairs of numbers, $(x,x')$, and it is common
to think of such a pair as $x+x'\epsilon$ for an `infinitesimal' $\epsilon$. The main two ways in which AD is performed in practice is by operator overloading or by source code transformation (see e.g. \cite{griewank2008evaluating} Chapter 6). 
Our approach focuses on source code transformation which is better fitted for compilation and optimizations.

\subsection{Reverse-mode Automatic Differentiation}

The problem with the previous approach shows up when one wants to compute the full gradient of a function $\RR^n\to\RR$, for a large $n$. 
Forward-mode only computes one directional derivative, for instance one partial derivative. 
This implies $n$ passes must be performed through the forward derivative to compute the whole gradient.
By using the symmetry in the chain rule, there is a way to compute the whole gradient faster, and this method is reverse-mode differentiation.
Suppose given a function $f=f_n\circ\ldots\circ f_1 :\RR^n\to\RR$. 
Mathematically, forward mode essentially computes $(\J f)v=\J f_n(\J f_{n-1}(\ldots(\J f_1v))\ldots)$ for a direction $v\in\RR^n$. 
Reverse-mode, on the other hand, computes $(\J f)^Tv = \J^Tf_1(\J^T f_{2}(\ldots(\J^T f_nv))\ldots)$ for a vector $v\in\RR$.
In particular, taking $v=1$ computes the gradient of $f$.

Because the computation flow of the function is reversed, the actual implementation of reverse-mode is way quite tricky. 
Reverse-mode AD is only well-understood as a source-code transformation on limited programming languages. 
Typically, its implementations on more expressive languages that have features such as higher-order functions and conditionals
make use of define-by-run approaches. These approaches first build a computation graph during runtime, effectively evaluating the program until a straight-line
first-order program is left, and then they evaluate this new program \cite{carpenter2015stan,paszke2017automatic}. 
Such approaches have the severe drawback that the obtained code cannot benefit from existing optimizing compilers.
As such, the implementation process is tedious and labor-intensive as these AD libraries need to be implemented using carefully, manually optimized code.
In addition, some whole-program optimizations that a compiler would detect are completely missed.

\subsection{Inefficiency of purely functional reverse-mode AD}

Following \cite{pearlmutter2008reverse}, there is a simple way to define an inefficient yet purely functional reverse-mode transformation for first-order programs.
We review a slight modification of their transformation, which is also better explained through an example. 

Let us consider the term $x_1:\RR,\ldots,x_n:\RR\vdash \exp(\cos(x_i))$.
To compute its gradient, following the chain rule, we need the jacobian matrices of $\cos$ at $x_i$ and of $\exp$ at $\cos(x_i)$. 
Instead of considering these as operations from $\RR\to\RR$, we consider them as functions from the whole context. So $\cos$ and $\exp$ are seen as functions $\RR^n\to\RR$.
However, by simply doing this we lose compositionality. 
So we modify $\cos$ to also return its context. It is now seen as a function $\sem{\cos}:\RR^n\to\RR^{n+1}$.
Similarly, $\exp$ is transformed. It also needs to take the return value of $\sem{\cos}$ as an extra argument, the one it will actually use and not simply return. 
We thus obtain $\sem{\exp}:\RR^{n+1}\to\RR^{n+2}$. Now the jacobians matrices $\J\sem{\cos} \in Mat_{n,n+1}$, $\J\sem{\exp} \in Mat_{n+1,n+2}$ compose nicely.
The same can be done for binary operators and let bindings.
This transforms a first-order program to a function $f:\RR^n\to\RR^{n+m}$ of the form $f_m\circ\ldots\circ f_1$. 
If the original program was of type $\RR$, then the return value of the original program is the last component of $f$.
Following the mathematical presentation of reverse-mode above, the gradient of the the original program is then obtained as 
\begin{center}
    \begin{tabular}{r c l}
        $\nabla f$ &=& $\J^Tf(0,\ldots,0,1)= \J^Tf_1(J^Tf_{2}(\ldots(\J^Tf_m(0,\ldots,0,1))\ldots)$
    \end{tabular}
\end{center}

To actually reverse the order of computation needed for this transpose of jacobians, we use a simple continuation;
$f_i$ is turned into $\Dsynrevsymbol{f_i}:=<f_i, \lambda Y. Y\circ J^Tf_i>$ where $Y:\RR^{n+i-1}\to \RR^n$. 
We recover compositionality by noting that $<f_{i+1}(f_i), (\lambda Y. Y\circ J^Tf_{i+1})(\lambda Y. Y\circ J^Tf_i)>$ reduces to
$<f_{i+1}\circ f_i, \lambda Y. Y\circ J^Tf_i \circ J^Tf_{i+1}>$, and thus by induction we can obtain $<f, \lambda Y. YJ^Tf>$.
By applying the identity continuation $\RR^n\to\RR^n$ on the second component and then the result to $(0,\ldots,0,1)$, 
we have obtained a purely functional way to compute $\nabla f$. 

This purely functional implementation has the following issues in terms of efficiency: 

\smartpara{Issue 1}
If we see the term as a directed graph, reverse mode needs to backpropagate from the end of the graph to the starting nodes via every path.
However, it is hard to keep track of all these information in parallel in a functional way.
Mutation is usually key for these cases; 
the imperative version of reverse mode for a binary operator $op(x,y)$ adds $x'+= \partial_1op(x,y);y'+= \partial_2op(x,y)$, 
where $\partial_iop$ are the partial derivatives of $op$.

\smartpara{Issue 2} 
Each $J^Tf_{i+1}$ is a potentially huge matrix if $n$ or $m$ is big.

\smartpara{Issue 3}
We have to carry a continuation and $\beta$-reduce a lot of higher-order functions.

\subsection{Insights for efficient purely functional reverse-mode AD}
\label{subsec:insights}
Overall we use the following three insights to solve the inefficiency associated with the purely functional implementations of reverse-mode AD.

\smartpara{Insight 1}
One of the key simple ideas that we used was to transform every operator into a unary one. 
This essentially trivializes the computation flow to a line. 
Even if the starting program was a straight-line program, 
having non-unary operators was a source of inefficiency and justified the use of mutation in the first place.
By returning every variable every time, the problem of using a variable several times does not need to be dealt with via mutation. 
This simple idea of transforming a program into essentially a straight line is what our new intermediate representation UNF allows.

\smartpara{Insight 2} 
If we look at $J^Tf_{i+1}$, we notice that this function is almost the identity, except at the last row. 
Even on the last row, if the original term was a unary or binary operator like $cos, exp, +, *$, 
the row is zero except for at most two indices (one for unary operators).
This means we can use a more compact representation $J^T\sem{op(x_i,x_j)}:=\lambda (y_1,\ldots,y_{n+i}).(y_1,\ldots,y_{n+i})+[i]\partial_1op(x_i,x_j)+[j]\partial_2op(x_i,x_j)$, 
where $[k]$ means that the element is added at the $k$-th index of the tuple.

\smartpara{Insight 3}
We know in advance that the jacobian functions are going to be applied one to another and we can use partial evaluation to $\beta$-reduce all of these $\lambda$s.
Because each function is almost the identity, we obtain a lot of substitutions of the form $[x/y]$ where both $x$ and $y$ are variables. 
This allows us to drastically reduce the size of $\J^Tf$. In fact, for simple programs, this is basically enough to get an efficient purely functional reverse derivative transformation.
We develop this idea further for a richer language.

\pagebreak