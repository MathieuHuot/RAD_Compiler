\section{Appendix}

 \subsection{Operational semantics}

In Figure~\ref{fig:op_semantics_target} a small step call-by-value operational semantics for the language. 
The evaluation contexts aregiven in Figure~\ref{fig:ev_contexts}. 

\input{figures/ev_contexts}

\input{figures/op_semantics_target}

 \subsection{Reverse derivative of array operations}

We now prove that the reverse mode transformation is correct on array operations. 
The proofs consist in showing that unrolling the computation before differentiation 
and then differentiating gives the same as differentiation and then unrolling. 

 \begin{proposition}
    The reverse derivative of !map2! is correct.
\end{proposition}

\begin{proof}
    TODO
\end{proof}

\begin{lemma}
    if !op2! is an associative binary operation with unit $\Gamma \vdash$ !v:! $\reals$, then 
    $\frac{\partial op2(v,e)}{\partial y_1}\times\frac{\partial v}{\partial x_i}=0$ 
    and $\frac{\partial op2(e,v)}{\partial y_2}\times\frac{\partial v}{\partial x_i}=0$ for all $x_i$.
\end{lemma}

\begin{proof}
    For any $\Gamma \vdash$ !e:! $\reals$, we have !v op2 e!=!e!.
    Differentiating and using the chain rule we get 
    $$\frac{\partial op2(v,e)}{\partial y_1}\times\frac{\partial v}{\partial x_i}
    +\frac{\partial op2(v,e)}{\partial y_2}\times\frac{\partial e}{\partial x_i}
    = \frac{\partial e}{\partial x_i}$$
As $\frac{\partial e}{\partial x_i}$ is arbitrary, 
this shows that $\frac{\partial op2(v,e)}{\partial y_2}=1$ and $\frac{\partial op2(v,e)}{\partial y_1}\times\frac{\partial v}{\partial x_i}=0$.
Similarly for the other case.
\end{proof}

\begin{proposition}
    The reverse derivative of !reduce! is correct.
\end{proposition}

\begin{proof}
    TODO
\end{proof}

\subsection{gradient from the introduction}
\label{sub:gradintro}

We show that the gradients from Section \ref{sec:intro} are obtained as instances of our general construction. 
The proofs consist in instantiating the general derivatives to these cases 
and showing that each rewrite step is a simple known optimization.

Similarly to numpy, we use the notation OnesLike(A) to mean map (x -> 1) A 
and ZerosLike(A) to mean map (x -> 0) A. 
Finding these constant arrays is key to a lot of optimizations that leverage the ring algebraic structure of the reals to arrays.

 \begin{lemma}
    !$\nabla_A$prod(A)= map2 * (scanr * 1 (shift1L A)) (shift1R (scanl * 1 A))!
 \end{lemma}

 \begin{proof}
The gradient of  derivative $\nabla_A$!prod(A)! is given by

\begin{tabular}{c l r}
    & $\nabla_A$ !prod(A)! & \\
    =&  $\nabla_A$(!reduce * 1 A!) & \\
    $\defeq$ & \Big(!fun z ->! & \\ 
    & !let A$_0$ = shift1R (scanl * 1 A) in! & \\
    & !let A$_1$ = shift1L (map2 (x,y -> y) A$_0$ A) in! & \\ 
    & !let A$_2$ = map2 (x,y -> x) A$_0$ A in! & \\
    & !let A$_3$ = scanr * 1 A$_1$ in! &\\
    & !map2 (a,b -> a*b*z) A$_3$ A$_2$! & \\
    & \Big)(1)\\
     $\stackrel{\beta-reduction}{=}$  & !let A$_0$ = shift1R (scanl * 1 A) in! & \\
    & !let A$_1$ = shift1L (map2 (x,y -> y) A$_0$ A) in! & !A$_1$=shift1L A! \\
    & !let A$_2$ = map2 (x,y -> x) A$_0$ A in! & !A$_2$= A$_0$!\\
    & !let A$_3$ = scanr * 1 A$_1$ in! & \\
    & !map2 (a,b -> a*b*1) A$_3$ A$_2$! & !map2 * A$_3$ A$_2$! \\
    = & !let A$_0$ = shift1R (scanl * 1 A) in! & \\
    & !let A$_1$=shift1L A! &\\
    & !let A$_3$ = scanr * 1 A$_1$! & \\
    & !map2 * A$_3$ A$_0$! & forward substitution !A$_2$! \\
    $\stackrel{\eta-reduction}{=}$ & !map2 * (scanr * 1 shift1L A)! &\\
    & \quad\quad\quad\quad !(shift1R (scanl * 1 A))! &
\end{tabular}
 \end{proof}

 \begin{lemma}
     $\nabla_A$!sum(A)! = !map (x -> 1) A!
 \end{lemma}

 \begin{proof}
The gradient of !sum(A)! is given by

    \begin{tabular}{c l r}
    & $\nabla_A$ !sum(A)! & \\
    = & $\nabla_A$ !reduce + 0 A! & \\
    $\defeq$ & \Big(!fun z ->! & \\ 
    & !let A$_0$ = shift1R (scanl + 0 A) in! & \\
    & !let A$_1$ = shift1L (map2 (x,y -> 1) A$_0$ A) in! & \\ 
    & !let A$_2$ = map2 (x,y -> 1) A$_0$ A in! & \\
    & !let A$_3$ = scanr * 1 A$_1$ in! &\\
    & !map2 (a,b -> a*b*z) A$_3$ A$_2$! & \\
    & \Big)(1)\\
    $\stackrel{\beta-reduction}{=}$  & !let A$_0$ = shift1R (scanl + 0 A) in! & \\
    & !let A$_1$ = shift1L (map2 (x,y -> 1) A$_0$ A) in! & !A$_1$=OnesLike(shift1L(A))! \\
    & !let A$_2$ = map2 (x,y -> 1) A$_0$ A in! & !A$_2$=OnesLike(A)!\\
    & !let A$_3$ = scanr * 1 A$_1$ in! & \\
    & !map2 (a,b -> a*b*1) A$_3$ A$_2$! & !map2 * A$_3$ A$_2$! \\
    = & !let A$_0$ = shift1R (scanl + 0 A) in! & \\
    & !let A$_1$ = OnesLike(shift1L(A))! & forward substitution \\
    & !let A$_2$ = OnesLike(A)! & forward substitution \\
    & !let A$_3$ = scanr * 1 A$_1$ in! & \\
    & !map2 * A$_3$ A$_2$! & \\
    = & !let A$_3$ = scanr * 1 OnesLike(shift1L(A)) in! & !A$_3$=OnesLike(A)!\\
    & !map2 * A$_3$ OnesLike(A)! & forward substitution \\
    = & !map2 * OnesLike(A) OnesLike(A)! & \\
    = & !OnesLike(A)!
    \end{tabular}
\end{proof}

 \begin{lemma}
     $\nabla_A$!dot(A,B)! = !B! 
 \end{lemma}

 \begin{proof}
The reverse derivative of !map2 * A B! is given by 

\begin{tabular}{c l}
    & !fun (Z) ->! \\
    & !let C$_1$ = map2 (a,b -> b) A B in! \\
    & !let C$_2$ = map2 (a,b -> a) A B in! \\
    & !(map2 * C$_1$ Z, map2 * C$_2$ Z)!
\end{tabular}

Let's call this term !Y!.
For convenience, let us also rewrite !map2 * B (map (x -> 1) A)! 
as !let C = map (x -> 1) A in map2 * B C!.

Then the gradient of !dot(A,B)! is given by

\begin{tabular}{c l r}
    & $\nabla$ !dot(A,B)! & \\
    = & $\nabla$ !let C = map (x -> 1) A in map2 * B C! & \\
    $\defeq$ & \Big(!fun z ->! & \\ 
    & !let A$_0$ = shift1R (scanl + 0 A) in!  \hspace{4cm}\rdelim\}{5}{3mm}[\parbox{40mm}{same reduction\\as previously}]\\
    & !let A$_1$ = shift1L (map2 (x,y -> 1) A$_0$ A) in! & \\ 
    & !let A$_2$ = map2 (x,y -> 1) A$_0$ A in! & \\
    & !let A$_3$ = scanr * 1 A$_1$ in! &\\
    & !Y(map2 (a,b -> a*b*z) A$_3$ A$_2$)! & \\
    & \Big)(ZerosLike(A),1)\\
    = & !Y(OnesLike(A))! & \\
    = & !let C$_1$ = map2 (a,b -> b) A B in! & !C$_1$ = B! \\
    & !let C$_2$ = map2 (a,b -> a) A B in! & !C$_2$ = A! \\
    & !(map2 * C$_1$ OnesLike(A), map2 * C$_2$ OnesLike(A))!  &\\
    = & !let C$_1$=B in! & forward substitution \\
    & !let C$_2$=A in! & forward substitution \\
    & !(map2 * C$_1$ OnesLike(A),! & !C$_1$! \\
    & !map2 * C$_2$ OnesLike(A))! & !C$_2$! \\
    = & !(map2 * B OnesLike(A), map2 * A OnesLike(A))! \\
    = & !(B, A)!
\end{tabular}

If we are only interested in the gradient w.r.t. !A!, this indeed gives !B!.
\end{proof}