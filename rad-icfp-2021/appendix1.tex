\section{Appendix}

 \subsection{Operational semantics}

In Figure~\ref{fig:op_semantics_target} a small step call-by-value operational semantics for the language. 
The evaluation contexts aregiven in Figure~\ref{fig:ev_contexts}. 

\input{figures/ev_contexts}

Some operators are less conventional which we now explain.
!shift1! takes an array of reals of size $n$ and adds a $1$ as first component and forgets about the last component of the array, returning an array of size !n!.
!scanr! is similar to !scanl! except that the folding starts from the end of the array. 
It is equivalent to first reversing the input array, performing !scanl!, and reversing the resulting array.

\MH{need to explain that the !scanl!, !scanr! coming from !reduce! are also parallel ones}

\MH{might need to extend with the operators from the generalized section}

\input{figures/op_semantics_target}

 \subsection{Reverse derivative of array operations}

We now prove that the reverse mode transformation is correct on array operations. 
The proofs consist in showing that unrolling the computation before differentiation 
and then differentiating gives the same as differentiation and then unrolling. 

 \begin{proposition}
     The reverse derivative of !map! is correct.
 \end{proposition}

\begin{proof}
    
\end{proof}

 \begin{proposition}
    The reverse derivative of !map2! is correct.
\end{proposition}

\begin{proof}
    
\end{proof}

\begin{lemma}
    if !op2! is an associative binary operation with unit $\Gamma \vdash$ !v:! $\reals$, then 
    $\frac{\partial op2(v,e)}{\partial y_1}\times\frac{\partial v}{\partial x_i}=0$ 
    and $\frac{\partial op2(e,v)}{\partial y_2}\times\frac{\partial v}{\partial x_i}=0$ for all $x_i$.
\end{lemma}

\begin{proof}
    For any $\Gamma \vdash$ !e:! $\reals$, we have !v op2 e!=!e!.
    Differentiating and using the chain rule we get 
    $$\frac{\partial op2(v,e)}{\partial y_1}\times\frac{\partial v}{\partial x_i}
    +\frac{\partial op2(v,e)}{\partial y_2}\times\frac{\partial e}{\partial x_i}
    = \frac{\partial e}{\partial x_i}$$
As $\frac{\partial e}{\partial x_i}$ is arbitrary, 
this shows that $\frac{\partial op2(v,e)}{\partial y_2}=1$ and $\frac{\partial op2(v,e)}{\partial y_1}\times\frac{\partial v}{\partial x_i}=0$.
Similarly for the other case.
\end{proof}

\begin{proposition}
    The reverse derivative of !reduce! is correct.
\end{proposition}

\begin{proof}
    
\end{proof}

\subsection{gradient from the introduction}
\label{sub:gradintro}

We show that the gradients from Section \ref{sec:intro} are obtained as instances of our general construction. 
The proofs consist in instantiating the general derivatives to these cases 
and showing that each rewrite step is a simple known optimization.

Similarly to numpy, we use the notation OnesLike(A) to mean map (x -> 1) A 
and ZerosLike(A) to mean map (x -> 0) A. 
Finding these constant arrays is key to a lot of optimisations that leverage the ring algebraic structure of the reals to arrays.

\MH{be more explicit about optimisations. 0 should be ZerosLike. A bit outdated compared to current D transformation.
Also, I am freely referring to the variables from the context or bound in the original term, should be clarified.}

 \begin{lemma}
     $\nabla_A$!prod(A)! = !map2 * (scanr x 1 A) (shift1(scanl * 1 A))!
 \end{lemma}

 \begin{proof}
The gradient of  derivative $\nabla_A$!prod(A)! is given by

\begin{tabular}{c l r}
    & $\nabla_A$ !prod(A)! & \\
    =&  $\nabla_A$(!reduce * 1 A!) & \\
    $\defeq$ & \Big($\lambda$ !A',z. let A1 = scanl * 1 A in! & \\
    & !let A2 = map2 (x,y -> y) A1 A in! & \\ 
    & !let A3 = shift1 (map2 (x,y -> x) A1 A) in! & \\
    & !let A4 = scanr * 1 A2 in! &\\
    & !map3 (a,b,c -> a+b*c*z) A' A4 A3! & \\
    & \Big)(0,1)\\
     $\stackrel{\beta-reduction}{=}$  & !let A1 = scanl * 1 A in! & \\
    & !let A2 = map2 (x,y -> y) A1 A in! & !A2=A! \\
    & !let A3 = shift1 (map2 (x,y -> x) A1 A) in! & !A3=shift1 A1!\\
    & !let A4 = scanr * 1 A2 in! & \\
    & !map3 (a,b,c -> a+b*c*1) 0 A4 A3! & !map2 * A4 A3! \\
    = & !let A1 = scanl * 1 A in! & \\
    & !let A3 = shift1 A1 in! &\\
    & !let A4 = scanr * 1 A in! & forward substitution !A2!\\
    & !map2 * A4 A3! & \\
    $\stackrel{\eta-reduction}{=}$ & !map2 * (scanr x 1 A) (shift1(scanl * 1 A))! &
\end{tabular}
 \end{proof}

 \begin{lemma}
     $\nabla_A$!sum(A)! = !map (x -> 1) A!
 \end{lemma}

 \begin{proof}
The gradient of  derivative $\nabla_A$!sum(A)! is given by

    \begin{tabular}{c l r}
    & $\nabla_A$ !sum(A)! & \\
    = & $\nabla_A$ !reduce + 0 A! & \\
    $\defeq$ & \Big($\lambda$ !A',z. let A1 = scanl * 1 A in! \\
    & !let A2 = map2 (x,y -> 1) A1 A in! & \\
    & !let A3 = shift1 (map2 (x,y -> 1) A1 A) in! & \\
    & !let A4 = scanr * 1 A2 in! & \\
    & !map3 (a,b,c -> a+b*c*z) A' A4 A3! &
    &  \Big)(0,1) & \\
    $\stackrel{\beta-reduction}{=}$  & !let A1 = scanl * 1 A in! & \\
    & !let A2 = map2 (x,y -> y) A1 A in! & !A2=OnesLike(A)! \\
    & !let A3 = shift1 (map2 (x,y -> x) A1 A) in! & !A3=shift1 OnesLike(A)!\\
    & !let A4 = scanr * 1 A2 in! & \\
    & !map3 (a,b,c -> a+b*c*1) 0 A4 A3! & !map2 * A4 A3! & \\
    = & !let A1 = scanl * 1 A in! & \\
    & !let A2= OnesLike(A) in! &  \\
    & !let A3 = shift1 OnesLike(A) in! & !A3=OnesLike(A)! \\
    & !let A4 = scanr * 1 A2 in! & !A4=OnesLike(A2)! \\
    & !map2 * A4 A3! & \\
    = & !map2 * OnesLike(A) OnesLike(A)! & \\
    = & !OnesLike(A)!
    \end{tabular}
\end{align*}
 \end{proof}

 \begin{lemma}
     $\nabla_A$!dot(A,B)! = !B! 
 \end{lemma}

 \begin{proof}
The reverse derivative of !map2 * A B! is given by 

\begin{tabular}{c l}
    & $\lambda$ !A',B',C'.! \\
    & !let C1 =map2 (a,b -> b) A B in! \\
    & !let C2 =map2 (a,b -> a) A B in! \\
    & !(map3 (a,b,c -> a+b*c) A' C1 C', map3 (a,b,c -> a+b*c) A' C2 C')!
\end{tabular}

Let's call this term Y.
For convenience, let us also rewrite !map2 * B (map (x -> 1) A)! 
as !let C = map (x -> 1) A in map2 * B C!.

Then the gradient of !dot(A,B)! is given by

\begin{tabular}{c l r}
    & $\nabla$ !dot(A,B)! & \\
    = & $\nabla$ !let C = map (x -> 1) A in map2 * B C! & \\
    $\defeq$ & $\Big(\lambda$ A',B',C',z. & \\
    & !let A1 = scanl * 1 A in!  \hspace{4cm}\rdelim\}{5}{3mm}[\parbox{40mm}{same reduction\\as previously}]\\
    & !let A1 = scanl * 1 A in! & \\
    & !let A2 = map2 (x,y -> 1) A1 A in! & \\
    & !let A3 = shift1 (map2 (x,y -> 1) A1 A) in! &\\
    & !let A4 = scanr * 1 A2 in! &\\
    & !Y(A', B', map3 (a,b,c -> a+b*c*z) A' A4 A3)!\Big) &  \\
    & \quad !(0,0,0,1)! & \\
    = & !Y(0,0,OnesLike(C))! & \\
    $\defeq$ & !let C1=map2 (a,b -> b) A B in! & !C1 = B! \\
    & !let C2=map2 (a,b -> a) A B in! & !C2 = A! \\
    & !(map3 (a,b,c -> a+b*c) 0 C1 OnesLike(C)!, & !map2 C1 OnesLike(C)! \\
    & !map3 (a,b,c -> a+b*c) 0 C2 OnesLike(C))!  & !map2 C2 OnesLike(C)! \\
    = & !let C1=B! & forward substitution \\
    & !let C2=A! & forward substitution \\
    & !map2 C1 OnesLike(C)! & !C1! \\
    & !map2 C2 OnesLike(C)! & !C2! \\
    = & !(B, A)!
\end{tabular}

So if we are only interested in the gradient w.r.t. A, this indeed gives us B.
\end{proof}