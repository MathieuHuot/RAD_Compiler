\section{Appendix}

 \subsection{Operational semantics}

In Figure~\ref{fig:op_semantics_target} a small step call-by-value operational semantics for the language. 
The evaluation contexts aregiven in Figure~\ref{fig:ev_contexts}. 

\input{figures/ev_contexts}

\input{figures/op_semantics_target}

 \subsection{Reverse derivative of array operations}

 \label{sub:Reverse derivative of array operations}

We now prove that the reverse mode transformation is correct on array operations. 
% The proofs consist in showing that unrolling the computation before differentiation 
% and then differentiating gives the same as differentiation and then unrolling. 

 \begin{proposition}
    The reverse derivative of !map2! is correct.
\end{proposition}

\begin{proof}
Let us denote by !t! the term !map2 (x,y.e) A B!. Without loss of generality, assume !A!=$[a_1,\ldots,a_n]$ and !B!=$[b_1,\ldots,b_n]$.
By chosing !Z! to a hot vector at i, call it !Z$_i$!, we're back to showing the result for the term $\Gamma \vdash$!e[a$_i$/x,b$_i$/y]: $\reals$!.
In !$\directD{\rho}{\Gamma}{Y}$(t)(Z$_i$)!, the term !G = (map2 * Z (map2 (a,b.($\grad_\Gamma$e$_{1}$)[a/x.b/y]) A B))! 
reduces to $\frac{\partial e}{\partial x_i}$[a$_i$/x,b$_i$/y]. As !a$_i$,b$_i$! are independant of $x_i$, 
this term is equal to $\frac{\partial e[a_i/x,b_i/y]}{\partial x_i}$, as expected.
Similarly, the term !map2 * (map2 (a,b.(!$\grad_{\{x\}}$!e$_{1}$)[a/x,b/y]) A B) Z! reduces to $\frac{\partial e}{\partial x}$[a$_i$/x,b$_i$/y].
As !a$_i$! is a variable, and independant of !b$_i$!, this is equal to $\frac{\partial e[a_i/x,b_i/y]}{\partial a_i}$.
\end{proof}

\begin{lemma}
    if !op2! is an associative binary operation with unit $\Gamma \vdash$ !v:! $\reals$, then 
    $\frac{\partial op2(v,e)}{\partial y_1}\times\frac{\partial v}{\partial x_i}=0$ 
    and $\frac{\partial op2(e,v)}{\partial y_2}\times\frac{\partial v}{\partial x_i}=0$ for all $x_i$.
\end{lemma}

\begin{proof}
    For any $\Gamma \vdash$ !e:! $\reals$, we have !v op2 e!=!e!.
    Differentiating and using the chain rule we get 
    $$\frac{\partial op2(v,e)}{\partial y_1}\times\frac{\partial v}{\partial x_i}
    +\frac{\partial op2(v,e)}{\partial y_2}\times\frac{\partial e}{\partial x_i}
    = \frac{\partial e}{\partial x_i}$$
As $\frac{\partial e}{\partial x_i}$ is arbitrary, 
this shows that $\frac{\partial op2(v,e)}{\partial y_2}=1$ and $\frac{\partial op2(v,e)}{\partial y_1}\times\frac{\partial v}{\partial x_i}=0$.
Similarly for the other case.
\end{proof}

\begin{proposition}
    The reverse derivative of !reduce! is correct.
\end{proposition}

\begin{proof}
    The notation for the general case are cumbersome and non-insightful. 
    We will exemplify the proof on the case of an array of size 3. We use infix notation for the binary operation $e$.
    The output term unrolls to $v_3\defeq ((v ~e ~a_1)~ e~ a_2) ~e~ a_3$, where $v$ is the unit of $e$.
    By the lemma above we know that the derivative w.r.t $v$ is 0, and focus on the partial derivatives w.r.t $a_i$.
    Write $v_1\defeq(v~ e~ a_1)$, $v_2 \defeq v_1 ~e ~a_2$.
    By inspection we have
    \begin{itemize}
        \item $\frac{\partial v_3}{\partial a_3}=\frac{\partial e}{\partial x_2}(v_2,a_3)$
        \item $\frac{\partial v_3}{\partial a_2}=\frac{\partial v_3}{\partial x_1}\times\frac{\partial v_2}{\partial x_2}=\frac{\partial e}{\partial x_1}(v_2,a_3)\times \frac{\partial e}{\partial x_2}(v_1,a_2)$
        \item $\frac{\partial v_3}{\partial a_1}=\frac{\partial v_3}{\partial x_1}\times\frac{\partial v_2}{\partial x_2}\times\frac{\partial v_1}{\partial x_2}=\frac{\partial e}{\partial x_1}(v_2,a_3)\times \frac{\partial e}{\partial x_1}(v_1,a_2)\times\frac{\partial e}{\partial x_2}(v,a_1)$
    \end{itemize}
    We thus need the following intermediate results
    \begin{itemize}
        \item $A_0=[v,v_1,v_2]$=!shift1R(scanl v e A)!
        \item $A_1 =[\grad_{\{x_1\}}e(v_1,a_2),\grad_{\{x_1\}}e(v_2,a_3)]=$ !shift1L (map2 (a,b.$\grad_{\{x_1\}}$e(a/x,b/y)) $A_0$ A)!
        \item $A_2 =[\grad_{\{x_2\}}e(v,a_1),\grad_{\{x_2\}}e(v_1,a_2),\grad_{\{x_2\}}e(v_2,a_3)]=$ !map2 (a,b.$\grad_{\{x_2\}}$e(a/x,b/y)) $A_0$ A!
        \item $A_3 =[\frac{\partial e}{\partial x_1}(v_2,a_3)\times \frac{\partial e}{\partial x_1}(v_1,a_2),\frac{\partial e}{\partial x_1}(v_2,a_3),1]=$ !scanr 1 * $A_1$!
    \end{itemize}
    And we return $A_4=$ !map2 $A_2$ $A_3$!.
\end{proof}

\subsection{Adding more array operators}
\label{sub:lift_more_arr_op}

There are two main differences with fold left !foldl! compared to !reduce!. 
First, in !foldl (x,y.e) v A! the starting accumulation element !v! is not a unit for !(x,y.e)!,
so it will have non-trivial derivatives in general and we need to account for that.
Second, we will need a more general !scanl! which allows !(x,y.e)! as a function argument. 
In other words, we need the general scan left computing the intermediate values of a fold left.
This should be a different primitive but we will still call it !scanl! in this section.

Finally, the former point implies we need a few more array manipulations.
These can be elegantly dealt with by changing the semantics of !scanl! and !scanr!. 
Let's assume that they now return a pair of an array of size $n$ 
of the intermediate computations and the final result.
The reverse derivative of !foldl! is then as shown in Figure~\ref{fig:foldl_map_trans}.
In this figure, we have shown the translation of the !map! operator as well, which is very similar to !map2!.

\begin{figure}
\begin{center}
\begin{tabular}{|r c l|}
\hline
    $\directD{\rho}{\Gamma}{Y}$(!foldl (x,y.e$_1$) e$_2$ e$_3$!) &=&
            !let v,Y$_1$ = $\directD{\rho}{\Gamma}{Y}$(e$_2$) in! \\
            && !let A,Y$_2$ = $\directD{\rho}{\Gamma,v}{Y_1}$(e$_3$) in! \\
            && !let A$_0$,r$_1$ = (scanl (x,y.e$_1$) v A) in! \\
            && !let A$_1$ = map2 (a,b.(!$\grad_{\{x\}}$!e$_1$)[a/x,b/y]) A$_0$ A! \\
            && !let A$_2$ = map2 (a,b.(!$\grad_{\{y\}}$!)e$_1$[a/x,b/y]) A$_0$ A! \\
            && !let r$_2$, A$_3$ = scanr * 1 A$_1$! \\
            && !<r$_1$, fun (x$_1$,$\ldots$,x$_n$,z) ->! \\
            && !let y$_1$,B = r$_2$*z ,map2 (x,y. x*y*z) A$_2$ A$_3$ in! \\
            && !Y$_2$(x$_1$,$\ldots$,x$_n$,y$_1$,B)>! \\
    $\directD{\rho}{\Gamma}{Y}$(!map (x.e$_{1}$) e$_{2}$!) &=&  
            !let A,Y$_{1}$ = $\directD{\rho}{\Gamma}{Y}$(e$_{2}$) in! \\
            && !let G=$\grad_{\Gamma}$ e$_{1}$ in!\\
            && !<map (x.e$_{1}$) A, fun (x$_{1}$,$\ldots$,x$_n$,Z) -> !\\
            && !Y$_{1}$( (x$_{1}$,$\ldots$,x$_n$)$\widehat{+}$(G*(reduce + 0 Z)),!\\
            && \quad\quad !map2 (a,b.($\grad_{\{x\}}$e$_{1}$)[a/x]*b) A Z )>!\\\hline
\end{tabular}
\end{center}
\caption{The reverse-mode AD transformation of \codekw{foldl} and \codekw{map} operators.}
\label{fig:foldl_map_trans}
\end{figure}

\subsection{Adding non-smooth scalar operators} % (fold)
\label{sub:lift_non_smooth}

We assumed the unary and binary operators were denoted by smooth functions $\RR^n\to\RR$. 
There is no additional difficulty in considering operators which are partial functions 
like division or operators which are not smooth at a point like square root.

These functions are then given intentional derivatives which provide valid derivatives 
on the domain of definition and differentiability of the operator. 
These functions are well known to be the bete noire of AD \cite{griewank2008evaluating} 
and we do not provide novel solutions to these.  
Several recent work have shown how to give semantics to such operators in the context of AD \cite{vakar2020denotational,mazza2021automatic,sherman2021,lee2020correctness}.

\subsection{General arrays} % (fold)
\label{sub:lift_gen_arr}

We now show how to generalize our reverse-mode transformation to be defined on arrays over any ground type !G!.
That is, we need to adapt the reverse derivatives of !map2! and !reduce! when they have more general function arguments.

A ground type !G! is interpreted as an Euclidean space $A$. 
It is in particular a real vector space.
Similarly, a ground context !$\Gamma$=x$_1$:G$_1$,$\ldots$,x$_n$:G$_n$! is interpreted as $\bigoplus_{1\leq i\leq n}A_i$, where $A_i\defeq\seml$(!G$_i$!)$\semr$.
The denotation of the gradient of a term !$\Gamma \vdash$ e: G! at a point is then a matrix, more precisely an element of $(\bigoplus_{1\leq i\leq n}A_i)\otimes A$
where $\otimes$ is the tensor product of real vector spaces. This space is isomorphic to $\bigoplus_{1\leq i\leq n}A_i\otimes A$.

We can define $\otimes$ on the types of our language inductively by

\begin{tabular}{r c l}
    $\reals \otimes A$ & $\defeq$ & $A$ \\
    $(A_1 \times \ldots \times A_n)\otimes A$ & $\defeq$ & $(A_1\otimes A) \times \ldots \times (A_n \otimes A)$ \\
    $\Array{A_1}{n} \otimes A$ & $\defeq$ & $\Array{A_1\otimes A}{n}$
\end{tabular}

With this definition, we recover that the gradient of $\Gamma \vdash$!e: !$\reals$ is a tuple of type $A_1\times\ldots\times A_n$ as expected.
For !map2!, we need a generalization of $*:\reals\to\reals$. 
If !e$_1$: A!, then $\grad_{\{x\}}$!e$_1$: $A\otimes A$! and we need a new primitive $\widehat{*}:(A\otimes A) \times A \to A$.
If we represent $A\otimes A$ as a matrix, then $\widehat{*}$ is matrix-vector multiplication.
Similarly, for !A$_{3}$! in !reduce! we need a new primitive $\widetilde{*}:(A\otimes A)\times(A\otimes A) \to (A\otimes A)$.
$\widetilde{*}$ corresponds to matrix-matrix multiplication.
Then, using this notation, there are only minimal changes to the reverse derivatives of !map2! and !reduce!, as can be seen in Figure~\ref{fig:lift_gen_arr}.

\begin{figure}
\begin{center}
\begin{tabular}{|r c l|}
\hline
    $\directD{\rho}{\Gamma}{Y}$(!map2 (x,y.e$_1$: G) e$_2$ e$_3$!) &=&  
    !let A,Y$_1$ = !$\directD{\rho}{\Gamma}{Y}$!(e2) in! \\
    && !let B,Y$_2$ = !$\directD{\rho}{\Gamma,A}{Y1}$!(e$_3$) in! \\
    && !let C=!$\grad_{\Gamma}$!e$_1$ in!\\
    && !<map2 (x,y.e$_1$) A B, fun (x$_1$,$\ldots$,x$_n$,Z) -> !\\
    && !Y$_2$( (x$_1$,$\ldots$,x$_n$)!$\widehat{+}$!(C!$\widehat{*}$!(reduce + 0 Z)),!\\
    && \quad\quad! map2 (a,b.(!$\grad_{\{x\}}$!e$_1$)[a/x]!$\widehat{*}$!b) A Z,!\\
    && \quad\quad! map2 (a,b.(!$\grad_{\{y\}}$!e$_1$)[a/x]!$\widehat{*}$!b) B Z )>!\\
    $\directD{\rho}{\Gamma}{Y}$(!reduce (x,y.e$_{1}$) e$_{2}$ e$_{3}$!) &=&
    !let y$_{1}$,Y$_{1}$ = !$\directD{\rho}{\Gamma}{Y}$!(e$_{2}$) in! \\
    && !let A,Y$_{2}$ = !$\directD{\rho}{\Gamma,y_1}{Y_1}$!(e$_{3}$) in! \\
    && !let A$_{0}$ = shift1R (scanl (x,y.e$_{1}$) y$_{1}$ A) in! \\
    && !let A$_{1}$ = shift1L (map2! \\ 
    && !      (a,b.(!$\grad_{\{x\}}$!e$_{1}$)[a/x,b/y]) A$_{0}$ A) in! \\
    && !let A$_{2}$ = map2! \\
    && !      (a,b.(!$\grad_{\{y\}}$!)e$_{1}$[a/x,b/y]) A$_{0}$ A in! \\
    && !let A$_{3}$ = scanr $\widetilde{*}$ 1 A$_{1}$ in! \\
    && !<reduce (x,y.e$_{1}$) y$_{1}$ A, fun (x$_{1}$,$\ldots$,x$_n$,z) ->! \\
    && !Y$_{2}$(x$_{1}$,$\ldots$,x$_n$, map2 (x,y. x$\widehat{*}$(y$\widehat{*}$z)) A$_{2}$ A$_{3}$>! \\ \hline
\end{tabular}
\end{center}
\caption{The reverse-mode AD transformation of \codekw{map2} and \codekw{reduce} for general arrays.}
\label{fig:lift_gen_arr}
\end{figure}

Evidently, one can combine all the generalizations from the previous subsections.
Even though this transformation has the correct complexity, it is open for future research to find
even better representations to allow for more optimizations. 
In particular, representations looking like Einsum \cite{van2011numpy} could be of interest 
and has been recently studied in the context of AD \cite{laue2018computing,laue2020simple}.
More generally, there is growing interest in tensor calculus \cite{liao2019differentiable,bernstein2020differentiating}.

\subsection{Gradient from the introduction}
\label{sub:gradintro}

We show that the gradients from Section \ref{sec:intro} are obtained as instances of our general construction. 
The proofs consist in instantiating the general derivatives to these cases 
and showing that each rewrite step is a simple known optimization.

Similarly to numpy, we use the notation OnesLike(A) to mean map (x -> 1) A 
and ZerosLike(A) to mean map (x -> 0) A. 
Finding these constant arrays is key to a lot of optimizations that leverage the ring algebraic structure of the reals to arrays.

 \begin{lemma}
    !$\nabla_A$prod(A)= map2 * (scanr * 1 (shift1L A)) (shift1R (scanl * 1 A))!
 \end{lemma}

 \begin{proof}
The gradient of  derivative $\nabla_A$!prod(A)! is given by

\begin{tabular}{c l r}
    & $\nabla_A$ !prod(A)! & \\
    =&  $\nabla_A$(!reduce * 1 A!) & \\
    $\defeq$ & \Big(!fun z ->! & \\ 
    & !let A$_0$ = shift1R (scanl * 1 A) in! & \\
    & !let A$_1$ = shift1L (map2 (x,y -> y) A$_0$ A) in! & \\ 
    & !let A$_2$ = map2 (x,y -> x) A$_0$ A in! & \\
    & !let A$_3$ = scanr * 1 A$_1$ in! &\\
    & !map2 (a,b -> a*b*z) A$_3$ A$_2$! & \\
    & \Big)(1)\\
     $\stackrel{\beta-reduction}{=}$  & !let A$_0$ = shift1R (scanl * 1 A) in! & \\
    & !let A$_1$ = shift1L (map2 (x,y -> y) A$_0$ A) in! & !A$_1$=shift1L A! \\
    & !let A$_2$ = map2 (x,y -> x) A$_0$ A in! & !A$_2$= A$_0$!\\
    & !let A$_3$ = scanr * 1 A$_1$ in! & \\
    & !map2 (a,b -> a*b*1) A$_3$ A$_2$! & !map2 * A$_3$ A$_2$! \\
    = & !let A$_0$ = shift1R (scanl * 1 A) in! & \\
    & !let A$_1$=shift1L A! &\\
    & !let A$_3$ = scanr * 1 A$_1$! & \\
    & !map2 * A$_3$ A$_0$! & forward substitution !A$_2$! \\
    $\stackrel{\eta-reduction}{=}$ & !map2 * (scanr * 1 shift1L A)! &\\
    & \quad\quad\quad\quad !(shift1R (scanl * 1 A))! &
\end{tabular}
 \end{proof}

 \begin{lemma}
     $\nabla_A$!sum(A)! = !map (x -> 1) A!
 \end{lemma}

 \begin{proof}
The gradient of !sum(A)! is given by

    \begin{tabular}{c l r}
    & $\nabla_A$ !sum(A)! & \\
    = & $\nabla_A$ !reduce + 0 A! & \\
    $\defeq$ & \Big(!fun z ->! & \\ 
    & !let A$_0$ = shift1R (scanl + 0 A) in! & \\
    & !let A$_1$ = shift1L (map2 (x,y -> 1) A$_0$ A) in! & \\ 
    & !let A$_2$ = map2 (x,y -> 1) A$_0$ A in! & \\
    & !let A$_3$ = scanr * 1 A$_1$ in! &\\
    & !map2 (a,b -> a*b*z) A$_3$ A$_2$! & \\
    & \Big)(1)\\
    $\stackrel{\beta-reduction}{=}$  & !let A$_0$ = shift1R (scanl + 0 A) in! & \\
    & !let A$_1$ = shift1L (map2 (x,y -> 1) A$_0$ A) in! & !A$_1$=OnesLike(shift1L(A))! \\
    & !let A$_2$ = map2 (x,y -> 1) A$_0$ A in! & !A$_2$=OnesLike(A)!\\
    & !let A$_3$ = scanr * 1 A$_1$ in! & \\
    & !map2 (a,b -> a*b*1) A$_3$ A$_2$! & !map2 * A$_3$ A$_2$! \\
    = & !let A$_0$ = shift1R (scanl + 0 A) in! & \\
    & !let A$_1$ = OnesLike(shift1L(A))! & forward substitution \\
    & !let A$_2$ = OnesLike(A)! & forward substitution \\
    & !let A$_3$ = scanr * 1 A$_1$ in! & \\
    & !map2 * A$_3$ A$_2$! & \\
    = & !let A$_3$ = scanr * 1 OnesLike(shift1L(A)) in! & !A$_3$=OnesLike(A)!\\
    & !map2 * A$_3$ OnesLike(A)! & forward substitution \\
    = & !map2 * OnesLike(A) OnesLike(A)! & \\
    = & !OnesLike(A)!
    \end{tabular}
\end{proof}

 \begin{lemma}
     $\nabla_A$!dot(A,B)! = !B! 
 \end{lemma}

 \begin{proof}
The reverse derivative of !map2 * A B! is given by 

\begin{tabular}{c l}
    & !fun (Z) ->! \\
    & !let C$_1$ = map2 (a,b -> b) A B in! \\
    & !let C$_2$ = map2 (a,b -> a) A B in! \\
    & !(map2 * C$_1$ Z, map2 * C$_2$ Z)!
\end{tabular}

Let's call this term !Y!.
For convenience, let us also rewrite !map2 * B (map (x -> 1) A)! 
as !let C = map (x -> 1) A in map2 * B C!.

Then the gradient of !dot(A,B)! is given by

\begin{tabular}{c l r}
    & $\nabla$ !dot(A,B)! & \\
    = & $\nabla$ !let C = map (x -> 1) A in map2 * B C! & \\
    $\defeq$ & \Big(!fun z ->! & \\ 
    & !let A$_0$ = shift1R (scanl + 0 A) in!  \hspace{4cm}\rdelim\}{5}{3mm}[\parbox{40mm}{same reduction\\as previously}]\\
    & !let A$_1$ = shift1L (map2 (x,y -> 1) A$_0$ A) in! & \\ 
    & !let A$_2$ = map2 (x,y -> 1) A$_0$ A in! & \\
    & !let A$_3$ = scanr * 1 A$_1$ in! &\\
    & !Y(map2 (a,b -> a*b*z) A$_3$ A$_2$)! & \\
    & \Big)(ZerosLike(A),1)\\
    = & !Y(OnesLike(A))! & \\
    = & !let C$_1$ = map2 (a,b -> b) A B in! & !C$_1$ = B! \\
    & !let C$_2$ = map2 (a,b -> a) A B in! & !C$_2$ = A! \\
    & !(map2 * C$_1$ OnesLike(A), map2 * C$_2$ OnesLike(A))!  &\\
    = & !let C$_1$=B in! & forward substitution \\
    & !let C$_2$=A in! & forward substitution \\
    & !(map2 * C$_1$ OnesLike(A),! & !C$_1$! \\
    & !map2 * C$_2$ OnesLike(A))! & !C$_2$! \\
    = & !(map2 * B OnesLike(A), map2 * A OnesLike(A))! \\
    = & !(B, A)!
\end{tabular}

If we are only interested in the gradient w.r.t. !A!, this indeed gives !B!.
\end{proof}